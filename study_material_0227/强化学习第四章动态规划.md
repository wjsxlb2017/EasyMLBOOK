[TOC]

# 引言

![image-20190131184013315](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131184013315.png)



《强化学习》第四章的主要内容是强化学习中的动态规划算法，这一章的内容是理解之后RL方法的基础，涉及到Policy Evaluation, Policy Iteration, Value Iteration等重要概念，学习这一章的时候可以参考DeepMind AlphaGo和AlphaStar项目的领头人David Silver在UCL教授的RL课程第三讲[https://www.youtube.com/watch?v=Nd1-UUMVfz4](http://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DNd1-UUMVfz4)，内容和《强化学习》一书第四章基本相同，而且会详细的讲解书中例子的计算过程，对于理解这一章内容十分有帮助。

# **动态规划基本概念**

动态规划**Dynamic Programming**, 简称DP，其中Dynamic代表了处理的问题有一些时序性质，Programming这里指的是优化策略，即寻找optimal policy。

动态规划适用的问题通常满足以下两个条件：

1. 具有最优的子结构Optimal Substructure。这代表最优解可以分而治之，先解决较简单的子问题，再得到原问题的解。
2. 子问题有重合性 Overlapping Subproblem，即子问题可能会重复出现，因此可以cache这些子问题的解进行重复利用。

恰好MDP问题（可回顾[川陀学者：有限马尔可夫决策过程——强化学习第三章](https://zhuanlan.zhihu.com/p/55079492)）符合上述两个条件：Bellman Equation给出来递归的子问题形式，而value function就是用来存储和反复利用子问题的解的。

动态规划适用于MDP的planning问题，即假设我们知道关于环境的模型，通过计算该模型给出的结果来不断改善应采取的policy。强化学习中两个重要的问题是**Prediction**和**Control**。Prediction指的是当知道了policy的时候如何计算相应的value function，这一过程又叫做**policy evaluation**。而control就是如何找到更优的policy的问题。动态规划对于解决这两个问题都适用。

# **Policy Evaluation**

那么，假如给定了某种policy $$\pi$$，我们如何计算state-value function$$v_{\pi}$$呢，前一章我们得到了其递归形式Bellman Equation，或者叫Bellman Expectation Equation:

![image-20190131184535614](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131184535614.png)

这一问题实际上是求解有$$|S|$$个未知量的一组线性方程组，当然直接求解析解计算量较大，通常可以用迭代的方法来求解，即通过将上述式子不断的迭代到一系列value function的近似解中$$v_1\rightarrow v_2 \rightarrow ... \rightarrow v_{\pi}​$$ 直到收敛到其真实值，其更新法则为：

![image-20190131184614068](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131184614068.png)

这一过程实际上就是对于如下的backup tree不断的对所有子节点即下一个state的value function求期望并更新父节点的过程：

![image-20190131184636322](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131184636322.png)

# **Policy Iteration**

上面我们总结了给定policy时evaluate value function的方法，但是我们的最终目的是找到更好的policy，那么怎样利用动态规划来解决这一问题呢？

假设当前的policy是 $$\pi$$ ，我们如何找到新的policy $$\pi$$使得对于所有的state， $$v_{\pi'}(s)\geq v_{\pi}(s)$$呢？一种直观的方法是采取greedy policy,即

![image-20190131184702532](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131184702532.png)

其依据是policy improvement theorem，即假设对于所有$$s \in S​$$，如果有 $$q_{\pi}(s,\pi'(s))\geq v_{\pi}(s)​$$，则 $$v_{\pi'}(s)\geq v_{\pi}(s)​$$ ，该定理简要推导过程如下：

![image-20190131184740232](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131184740232.png)

如果经过这一过程，新的policy不再improve，即$$v_{\pi'}(s)= v_{\pi}(s)$$，则

![image-20190131184830868](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131184830868.png)

这时满足了Bellman Optimal Equation，说明 $$v_{\pi}(s)= v_{*}(s)]​$$ ，我们已经得到了optimal policy。

当我们得到了优于policy $$\pi​$$的policy ![\pi'](http://www.zhihu.com/equation?tex=%5Cpi%27) ，我们可以再对其进行policy evaluation得到符合新的policy的value function ![v_{\pi'}](http://www.zhihu.com/equation?tex=v_%7B%5Cpi%27%7D) ，然后再根据新的value function进行改善得到更优的policy ![\pi''](http://www.zhihu.com/equation?tex=%5Cpi%27%27) ，如此通过不断的交替进行policy evaluation和policy improvement过程，我们最终可以找到optimal policy，这一方法叫做**policy iteration**。这一不断的交替进行policy evaluation和利用greedy算法进行policy improvement的过程如下图所示：

![image-20190131184922623](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131184922623.png)

这一过程可以看做既是竞争又是合作的关系。竞争是指evaluation和improvement似乎是在向着相反的方向前进：greedy policy通常会使得value function相对于新的policy不再准确，而通过evaluation使value function和新的policy一致又会使该policy不再是greedy policy。但是，长期来讲，这两个过程相互作用将会使我们得到optimal value function和optimal policy，所以可以说又是合作的关系。

# **Value Iteration**

policy iteration的一个缺点是当进行policy evaluation是我们需要扫过所有的状态若干次直到value function收敛，这需要较久的计算，我们能不能缩短这一过程呢？

实际上，我们可以缩短这一过程仍能保持最终结果的收敛，其中一个方法就是每扫过所有态一次就停止policy evaluation的过程而进行policy improvement，其更新公式为：

![image-20190131185023615](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131185023615.png)

可以看出，这就是将Bellman Optimality Equation转化为了更新公式。从另一个角度，我们也可以将其理解为通过Bellman Optimality Equation不断的迭代直到得到optimal value function $$v_1\rightarrow v_2 \rightarrow ... \rightarrow v_{*}$$，并且我们并不需要显性的定义中间的policy，这一方法称为v**alue iteration**。value iteration可以看做是对于如下的backup tree进行反复迭代的过程

![image-20190131185045753](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131185045753.png)

得到了optimal value function $$v_*​$$后，optimal policy的求解就容易了: $$\pi_*(s)=argmax_a\sum_{s',r}p(s',r|s,a)[r+\gamma v_*(s')]​$$

# **总结及延伸**

关于**policy evaluation**, **policy iteration**, **value iteration**和**Bellman Equations**之间的关系，David Silver的讲义里总结的很好：

![image-20190131185120856](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131185120856.png)

另外以上算法我们都是讨论的每次iterate我们扫过全部state的情况，这一方法称为同步动态规划Synchronous DP，但是实际问题中可能有上千万的state，扫过全部state不太现实，通常我们会用**异步动态规划Asynchronous DP**，即我们并不需要保持所有state更新的同步，有的state可以先更新，更新的时候利用其他state的当前值，有些state可能更新若干次后其他state的value才更新一次，这样可以更快的达到收敛。关于asynchronous DP第八章中会进一步详细讨论。

# **参考资料**

1.Sutton和Barto合著强化学习[Reinforcement Learning: An Introduction](http://link.zhihu.com/?target=http%3A//incompleteideas.net/book/the-book-2nd.html)第四章

2.David Silver的[RL course](http://link.zhihu.com/?target=http%3A//www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html)第三讲