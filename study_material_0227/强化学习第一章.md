[TOC]

# [引言](https://zhuanlan.zhihu.com/p/52541089)

![image-20190131190755078](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131190755078.png)

《强化学习》[Reinforcement Learning: An Introduction](http://link.zhihu.com/?target=http%3A//incompleteideas.net/book/the-book-2nd.html)是Sutton和Barto编著的关于强化学习方面的经典教材，在1998年就出版了第一版，有助于了解强化学习的核心思想与算法，而第二版于今年2018年刚刚出版，在次基础上又显著扩充了很多内容，包含了更丰富的内容与最新进展。

# **核心思想**

当我们思索学习的本质的时候，一个自然的过程是通过与环境的互动从而获得某种行动所对应的结果的因果关系的信息，进而决定之后采取什么样的行动以实现目标。**强化学习Reinforcement Learning**就是这样的一种机器学习方法，即将当前场景(situation)映射到适宜的行动(action)以实现奖励回报(reward)的最大化，当然，采取何种行动不是先验就知道的，而是需要进行不断的探索尝试才知道哪些行动会产出更大的奖励，而且这些行动**不仅仅**是造成即时的反馈，也可能**对后续的奖励产生影响**。通过试错来寻找合适的行动(**trial-and-error search**)以及延时奖励(**delayed reward**)是强化学习有别于其他问题的两大特征。

强化学习是**监督学习(supervised learning)**和**无监督学习(unsupervised learning)**之外机器学习另一重大领域。关于监督学习和无监督学习的详细内容可以参考[深度学习花书读书笔记目录](https://zhuanlan.zhihu.com/p/38431213)。

![image-20190131190909990](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131190909990.png)

监督学习是由一个外部的监督者supervisor通过对有标记的训练集数据学习来进行的，这里每一个样本可以看做是对某一situation所对应的正确的action的一种描述，其目的是通过对这些样本的学习系统可以将其推广到未在训练数据中出现过的未知样本上的预测。与之相比，强化学习问题通常需要一个代理**learning agent**通过对环境的感知互动来学习，而**事先不知道**哪些action是正确的，只有通过和环境的互动以及反馈才能学习到其中的规律。

而无监督学习通常是对于**没有标记**的数据集寻找一些**隐藏的结构**，与之相比，虽然强化学习也是不依赖于标记的样本，但其目的是**最大化奖励信号**，而不是寻找隐藏结构。

强化学习的一大挑战是如何平衡**探索(exploration)**和**利用(exploitation)**。想要最大化奖励，强化学习代理必须偏向选择那些之前尝试过的且被验证过有效的行动，但另一方面为了发现这些行动，它需要取尝试之前没有进行过的行动。代理需要exploit它已有的经验来获得奖励，但同时为了使得将来能有更好的行动选择它必须进行explore，而且对于一个随机的任务，每个行动还需要多次尝试来有效地估计奖励的期望，这些问题也是在监督学习或无监督学习中没有遇到的。

强化学习本身并不是一个新鲜的概念，它更像是机器学习与其他领域如心理学、神经科学、运筹学和控制论的结合。它与各学科的关系如下图所示：

![image-20190131191238004](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131191238004.png)

# **实例**

目前强化学习的主要应用还是在**机器控制以及游戏领域**，这些领域都涉及到进行决策的代理与其环境的互动，即使环境存在未知与偶然性，但是代理也要尽力实现它的目标比如获得游戏胜利或者完成某些操作任务。一些实例如下图所示：

![image-20190131191315623](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131191315623.png)



# **基本要素**

强化学习系统有如下四个要素：

- **policy策略**定义了agent在某一时刻执行何种行动，通常是基于已感知的环境的状态来决定下一步该进行的行动，某些场景下该行动可能是一个简单的函数或者是lookup table，其他场景下可能是更复杂的搜索过程。通常，policy是有一定随机性(stochastic)的，即是对每一个行动的概率分布而不是确然发生的。
- **reward signal奖励信号**定义了强化学习问题的目标。在每一个时间点，环境会向agent反馈一个数值信息reward，而agent的唯一目的就是使得整个过程中所积累的全部奖励最大。**reward是改变policy的主要动力**，如果policy选择的action带来很低的reward，之后policy可能就会被改变以选取可以获得更大reward的action。
- **value function值函数**。reward是**即时**的反应某种action**是好是坏**的信号，而value function是描述当前state**更长远的是好是坏**的信号。value function的定义是从当前状态出发，将来agent可积累的reward的总和。例如，某个state可能即刻带来的reward非常低，但之后它导致的状态可能产出很高的reward，那么它的value function仍然很高。比如学习过程可能是痛苦的，不如社交网络或者娱乐能够即时带来快乐，但长远来讲它可以给你带来更多的帮助，其即时的reward较小，但value function很大。而强化学习算法的**目的**也是**追求value function最大的状态**，而不是reward最大的状态，当然由于reward是由环境即时反馈的可以直接得到，而value通常需要追踪很长时间才能得到较精确的评估，实际上许多强化学习的算法都是在解决如何有效的评估value function这一问题。
- **model of environment**(optional)环境的模型是指对于环境的一种模拟，这样我们就可以推断采取某种行动时环境会给出何种反馈。Model经常用来做**planning规划**任务，即在行动真正实施之前就考虑未来可能发生的情况来采取一系列适宜的行动。包含有model的强化学习方法称作**model-based methods**，而对于仅仅依靠trail-and-error而没有关于环境的model的强化学习方法称作**model-free methods**，之后会详细总结。

根据Value Function, Policy, 以及有无model我们可以将agent如下图所示分为几类，之后章节也会详细总结每类agent的算法：

![image-20190131191704790](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131191704790.png)

# **内容结构**

我们再来看看强化学习一书的总体结构，除了第一章前言对强化学习所要研究的问题以及与其他算法的不同之处做简要介绍外，该书可分为三大部分：

- 2到8章构成第一部分，会阐述强化学习领域几乎所有的核心思想，但是会局限于比较简单的形式：即假设状态(state)和行动(action)空间都比较小，使得value function可以用数组或者是表格形式表示，即tabular solution。对于这种情况，通常我们可以明确的找到最优的value function和最优的policy。
- 9到13章构成第二部分，会延伸第一部分方法的适用范围到更复杂的状态空间更大的领域。通常我们也无法直接找到最优的value function或policy，而是需要进行有效的近似。这里就会用到在机器学习领域里一些常用的函数近似方法。
- 14到17章构成第三部分，会结合心理学与神经科学的思想来思考强化学习，并且会讨论强化学习的一些实例以及前沿问题。


前言部分总结完毕，可以对强化学习所研究的问题以及基本概念有个大致了解，下一篇会总结Multi-armed bandits多臂老虎机问题, to be continued。


# 参考资料：


1.Sutton和Barto合著强化学习[Reinforcement Learning: An Introduction](http://link.zhihu.com/?target=http%3A//incompleteideas.net/book/the-book-2nd.html)第一章

2.David Silver的[RL course](http://link.zhihu.com/?target=http%3A//www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html)第一讲

3.伯克利深度强化学习[CS 294-112 Deep Reinforcement Learning](http://link.zhihu.com/?target=http%3A//rail.eecs.berkeley.edu/deeprlcourse/)第一讲