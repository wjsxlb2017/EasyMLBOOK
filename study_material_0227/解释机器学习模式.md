
[TOC]
#  引言

无论您的数据科学解决方案的最终目标是什么，最终用户总是倾向于可解释和可理解的解决方案。 此外，作为一名数据科学家，您将总是受益于您的模型的可解释性，以验证和改进您的工作。 在这篇博文中，我试图解释机器学习中解释性的重要性，并讨论一些简单的方案和框架，你可以自己做实验。



![img](https://cdn-images-1.medium.com/max/1600/0*Pnypj3flKzC2dR8F.png)


# 为什么可解释性在机器学习中很重要？

In traditional statistics, we construct and verify hypotheses by investigating the data at large. We build models to construct rules that we can incorporate into our mental models of processes. A marketing firm for example can build a model that correlates marketing campaign data to finance data in order to determine what constitutes an effective marketing campaign.This is a top-down approach to data science, and interpretability is key as it is a cornerstone of the rules and processes that are defined. As correlation often does not equal causality, a solid model understanding is needed when it comes to making decisions and explaining them.

在传统的统计学中，我们通过调查大量的数据来构建和验证假设。 我们建立模型来构建规则，这些规则可以纳入我们的心智模型中。 例如，一家营销公司可以建立一个模型，将营销活动的数据与资金数据联系起来，以确定什么是有效的营销活动。这是一种自上而下的数据科学方法，解释性是关键，因为它是所定义的规则和流程的基石。 由于相关性往往不等于因果关系，所以在做决策和解释决策时，需要一个扎实的模型理解。

In a bottom-up approach to data science, we delegate parts of the business process to machine learning models. In addition, completely new business ideas are enabled by machine learning . Bottom-up data science typically corresponds to the automation of manual and laborious tasks. A manufacturing firm can for example put sensors on their machines and perform predictive maintenance. As a result, maintenance engineers can work more efficiently and don’t need to perform expensive periodic checks. Model interpretability is necessary to verify the that what the model is doing is in line with what you expect and it allows to create trust with the users and ease the transition from manual to automated processes.

在自底向上的数据科学方法中，我们将部分业务流程委托给机器学习模型。 此外，全新的商业理念通过机器学习得以实现。 自底向上的数据科学通常对应于手工和艰苦任务的自动化。 例如，制造企业可以在机器上安装传感器并进行预测性维护。 因此，维护工程师可以更有效地工作，不需要执行昂贵的定期检查。 模型的可解释性对于验证模型所做的事情是否符合您的期望是必要的，它允许与用户建立信任，并使从手动过渡到自动化过程变得容易。



![img](https://cdn-images-1.medium.com/max/1600/0*YsXC_ks40MuiFBbQ.png)

In a top-down process, you iteratively construct and validate a set of hypotheses. In a bottom-up approach, you attempt to automate a process by solving a problem from the bottom-up. 在自顶向下的过程中，您迭代地构造和验证一组假设。 在自底向上的方法中，您试图通过自底向上解决问题来实现流程的自动化

As a data scientist you are often concerned with fine-tuning models to obtain optimal performance. Data science is often framed as: ‘given data X with labels y, find the model with minimal error’. While the ability to train performant models is a critical skill for a data scientist, it is important to be able to look at the bigger picture. Interpretability of data and machine learning models is one of those aspects that is critical in the practical ‘usefulness’ of a data science pipeline and it ensures that the model is aligned with the problem you want to solve. Although it is easy to lose yourself in experimenting with state-of-the-art techniques when building models, being able to properly interpret your findings is an essential part of the data science process.

作为一名数据科学家，您经常关心如何对模型进行微调以获得最佳性能。 数据科学通常被定义为: 给定数据 x，标记为 y，找到误差最小的模型。 虽然训练性能模型的能力对于数据科学家来说是一项关键技能，但是能够着眼于更大的图景也很重要。 数据和机器学习模型的可解释性是数据科学管道实用性的关键方面之一，它可以确保模型与你想要解决的问题保持一致。 尽管在构建模型时，很容易迷失在用最先进的技术进行实验的过程中，但能够正确地解释您的发现是数据科学过程的重要组成部分。



![img](https://cdn-images-1.medium.com/max/1600/0*QkAukFdMphHgaJTG.jpg)

Interpreting models is necessary to verify the usefulness of the model predictions. 解释模型是必要的，以验证模型预测的有用性


# 为什么对您的模型进行深入分析是必要的呢？

There are several reasons to focus on model interpretability as a data scientist. Although there is overlap between these, they capture the different motivations for interpretability:

作为一个数据科学家，有几个理由关注模型的可解释性。 尽管这两者之间有重叠，但它们捕捉到了解释性的不同动机:

> Identify and mitigate bias. 识别并消除偏差

Bias is potentially present in any dataset and it is up to the data scientist to identify and attempt to fix it. Datasets can be limited in size and they might not be representable for the full population, or the data capturing process might have not accounted for potential biases. Biases often only become apparent after thorough data analysis or when the relation between model predictions and the model input is analysed. If you want to learn more about the different types of types of bias exist, I highly recommend the video below. Note that there is no single solution to resolving bias, but a critical step towards interpretability being aware of potential bias.

任何数据集中都可能存在偏差，这要靠数据科学家来识别并修正它。 数据集的大小是有限的，它们不能代表整体分布，或者说，数据获取过程可能没有考虑到潜在的偏差。 偏差往往只有在彻底的数据分析之后，或者在分析模型预测和模型输入之间的关系之后才变得明显。 如果你想了解更多不同类型偏差，我强烈推荐下面的视频。 请注意，解决偏差问题没有单一的解决方案，但可解释性是关键一步，能检测到潜在的偏差。



<iframe data-width="854" data-height="480" width="700" height="393" data-src="/media/b7e2a24f5613a3d48b9b6828eb504122?postId=70c30694a05f" data-media-id="b7e2a24f5613a3d48b9b6828eb504122" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2F59bMh59JQDo%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="https://towardsdatascience.com/media/b7e2a24f5613a3d48b9b6828eb504122?postId=70c30694a05f" style="display: block; position: absolute; margin: auto; max-width: 100%; box-sizing: border-box; transform: translateZ(0px); top: 0px; left: 0px; width: 700px; height: 393.391px;"></iframe>

Other examples of bias are the following:

其他偏差的例子如下:

e.g. word2vec vectors contain [gender biases](http://wordbias.umiacs.umd.edu/) due to the inherent biases that are present in the corpora they have been trained on. When you would train a model with these word embeddings, a recruiter searching for "technical profiles" will leave female resumes at the bottom of the pile.

Word2vec 向量包含性别偏见，这是由于他们所学习的语料库中存在固有的偏差。 当你使用这些单词"嵌入"时训练一个模型时，招聘人员通过搜索"技术档案"，得到的结果是，女性简历在最下面。

e.g. when you train an object detection model on a small, manually created dataset, it is often the case that the breadth of images is too limited. A wide variety of images of the objects in different environments, different lightning conditions and different angles is required in order to avoid an model that only fits to noisy and unimportant elements in the data.

例如，当你在一个手工创建的小型数据集上训练一个目标检测模型时，通常会发现图像的宽度太有限了。 为了避免模型只适用于数据中有噪声和不重要的元素，需要在不同的环境、不同的闪电条件和不同的角度下拍摄各种各样的物体图像。

> Accounting for the context of the problem. 会计问题的背景

In most problems, you are working with a dataset that is only a rough representation of the problem you are trying to solve and a machine learning model can typically not capture the full complexity of the real-life task. An interpretable model helps you to understand and account for the factors that are (not) included in the model and account for the context of the problem when taking actions based on model predictions.

在大多数问题中，您使用的数据集只是您试图解决的问题的粗略表示，而机器学习模型通常不能捕捉到真实任务的全部复杂性。 一个可解释的模型可以帮助您理解和解释模型中没有包含的因素，并在根据模型预测采取行动时考虑到问题的背景。

> Improving generalisation and performance. 改进概括性和性能

A high interpretability typically leads to a model that generalises better. Interpretability is not about understanding every single detail of the model for all of the data points. The combination of solid data, model and problem understanding is necessary to have a solution that performs better.

高度的可解释性通常会导致一个更好概括的模型。 可解释性不是理解模型中所有数据点的每一个细节。 实体数据、模型和问题理解的结合对于提供一个性能更好的解决方案是必要的。

> Ethical and legal reasons. 道德和法律原因

In industries like finance and healthcare it is essential to audit the decision process and ensure it is e.g. not discriminatory or violating any laws. With the rise of data and privacy protection regulation like GDPR, interpretability becomes even more essential. In addition, in medical applications or self-driving cars, a single incorrect prediction can have a significant impact and being able to ‘verify’ the model is critical. Therefore the system should be able to explain how it reached a given recommendation.

在金融和医疗保健等行业，必须对决策过程进行审计，并确保决策过程没有歧视性或违反任何法律。 随着像 GDPR 这样的数据和隐私保护法规的兴起，解释性变得更加重要。 此外，在医疗应用或自动驾驶汽车，一个单一的错误预测可以产生重大影响，能够"验证"模型是至关重要的。 因此，系统应该能够解释它是如何达到给定的建议的。


# 解读你的模型

A common quote on model interpretability is that with an increase in model complexity, model interpretability goes down at least as fast. Feature importance is a basic (and often free) approach to interpreting your model. Even for black-box models such as deep learning, techniques exist to improve interpretability. Finally, the LIME framework will be discussed, which serves as a toolbox for model analysis.

关于模型可解释性的通常认识是：随着模型复杂性的增加，模型可解释性下降得快得多。 特性重要性是解释模型的一个基本方法(通常不需要额外的训练开销)。 即使对于像深度学习这样的黑箱模型，也存在一些提高可解释性的工具。最后，将讨论 LIME 框架--一个模型分析的工具箱。

## **特征重要性**

###  广义线性模型

Generalised Linear Models ([GLM’s](https://en.wikipedia.org/wiki/Generalized_linear_model)) are all based on the following principle: 
if you take a linear combination of your features *x* with the model weights *w*, and feed the result through a squash function *f*, you can use it to predict a wide variety of response variables. Most common applications for GLM’s are regression (linear regression), classification (logistic regression) or modelling Poisson processes (Poisson regression). The weights that are obtained after training are a direct proxy of feature importance and they provide very concrete interpretation of the model internals.

广义线性模型(GLM)都是基于以下原则: 如果你用你的特征 x 的线性组合和模型权重 w，并通过一个压缩函数 f 输入结果，你可以用它来预测各种各样的响应变量。 最常见的 GLM 的应用是回归(线性回归) ，分类(Logit模型)或建模泊松过程(泊松回归)。 训练后得到的权重是特征重要性的直接代理，它们为模型内部提供了非常具体的解释。

e.g. when building a text classifier you can plot the most important features and verify whether the model is overfitting on noise. If the most important words do not correspond to your intuition (e.g. names or stopwords), it probably means that the model is fitting to noise in the dataset and it won’t perform well on new data.

例如，当构建一个文本分类器时，你可以绘制出最重要的特征，并验证模型是否与噪声过度匹配。 如果最重要的单词不符合你的直觉(例如名字或停顿词) ，这可能意味着模型适合数据集中的噪音，它不会在新数据上表现良好。



![img](https://cdn-images-1.medium.com/max/1600/0*g5TGSTlR1MQbfnoz.png)

An example of a neat visualisation for text interpretability purposes from 一个简洁的可视化的例子，用于文本的可解释性目的[TidyTextMining](https://www.tidytextmining.com/02-sentiment-analysis_files/figure-html/pipetoplot-1.png).

### 随机森林与支持向量机

Even non-linear models such as tree based models (e.g. Random Forest) also allow to obtain information on the feature importance. In Random Forest, feature importance comes for free when training a model, so it is a great way to verify initial hypotheses and identify ‘what’ the model is learning. The weights in kernel based approaches such as SVM’s are often not a very good proxy of feature importance. The advantage of kernel methods is that you are able to capture non-linear relations between variables by projecting the features into kernel space. On the other hand, just looking at the weights as feature importance does not do justice to the feature interaction.

甚至非线性模型，例如基于树的模型(例如随机森林)也允许获得关于特征重要性的信息。 在随机森林中，当训练一个模型时，特征的重要性是顺便获取的，所以这是一个很好的方法来验证初始假设和识别什么是模型学习。 在基于核的方法中，比如支持向量机的权值往往不能很好地代表特征的重要性。 核函数方法的优点是，通过将特征投影到核空间，可以捕获变量之间的非线性关系。 另一方面，仅仅把权重看作是特性的重要性并不能公平地对待特性之间的交互。



![img](https://cdn-images-1.medium.com/max/1600/0*ZTWkq69gOH8-LtB5.png)

By looking at the feature importance, you can identify what the model is learning. As a lot of importance in this model is put into time of the day, it might be worthwhile to incorporate additional time-based features. 通过查看特性的重要性，您可以确定模型正在学习什么。 由于在这个模型中很重要的一点是在一天中的某个时间，因此可能值得加入一些额外的基于时间的特性。

##  **深度学习**

深度学习模型由于大量的参数和复杂的特征提取/组合方法而变得难以解释。在许多任务上， 这些模型能获得最先进的性能，因此许多研究都集中在如何将模型预测结果与输入联系起来。



![img](https://cdn-images-1.medium.com/max/1600/1*aELhps_fMJLxTxQHbb8v4w.png)

The amount of research on interpretable machine learning is growing rapidly ( 可解释性机器学习的研究正在迅速发展[MIT 麻省理工学院](http://people.csail.mit.edu/beenkim/papers/BeenK_FinaleDV_ICML2017_tutorial.pdf)).

Especially when moving towards even more complex systems that process text and image data, it becomes hard to interpret what the model is actually learning. The main focus in research is currently primarily on linking and correlating outputs or predictions back to the input data. While this is fairly easy in the context of linear model, it is still an unsolved problem for deep learning networks. The two main approaches are either gradient-based or attention-based.

特别是当向处理文本和图像数据的更加复杂的系统移动时，很难理解模型实际上在学习什么。 目前研究的主要重点主要是将产出或预测与输入数据联系起来并相互关联。 虽然这在线性模型的背景下是相当容易的，但它仍然是深度学习网络尚未解决的问题。 两种主要的方法要么基于梯度，要么基于注意力。

在基于梯度的方法中，利用在后向通道中计算出的目标概念的梯度生成一个地图，高亮显示预测目标概念对应的输入图像中的重要区域。 这在计算机视觉上广泛应用。



![img](https://cdn-images-1.medium.com/max/1600/1*dpoq3YrKcXM_30xEWADyQQ.png)



[Grad-CAM](https://arxiv.org/pdf/1610.02391.pdf),在图像字幕生成时使用一种基于梯度的方法。 基于输出标题，该方法确定输入图像中哪些区域是重要的

- 基于注意力的方法通常用于序列数据(例如文本)。 除了网络的正常权重外，还要训练注意力权重，它的功能是"输入门"。 这些注意力权重决定了最终网络输出中每个不同元素的数量。 除了可解释性之外，在上下文中加入了注意力机制的系统如基于文本的问答系统中，也会带来更好的结果，因为网络能够"集中"注意力。

![img](https://cdn-images-1.medium.com/max/1600/0*KE2487HWDJlqO7hl.png)

带注意力机制的问答系统中，可以指出文本中哪些词最重要，以确定问题的答案

## **LIME**

**LIME**是一个更加通用的框架，旨在使任意机器学习模型的预测结果更具可解释性。

为了保持与模型无关，LIME 通过修改模型的输入来工作。 因此，与其试图理解整个模型，不如修改特定的输入实例，并监视对预测的影响。 在文本分类的上下文中，这意味着一些单词被替换，以确定输入的哪些元素影响预测。

