[TOC]

# 多臂老虎机问题——强化学习第二章

继续《强化学习》一书的笔记总结，这一章聚焦的是多臂老虎机问题 Multi-armed bandit 或者叫k-armed bandit。k-armed bandit相对来说是比较简单的评估系统反馈的机器学习问题，但是通过引入解决这一问题的基本方法，我们可以将其拓展到更复杂的强化学习问题。

# **问题定义**

什么是**k-armed bandit**问题呢？就是你面临了k种不同的选择，当每做一次选择后，根据你的行动系统会反馈一个从**平稳分布(stationary probability distribution)**取样的reward，你的目标就是使一定时间后所获取的总的reward最大化。为什么叫k-armed bandit呢？这是因为老虎机又被称作one-armed bandit（其中one-arm 用来指旁边的一根杠杆，bandit原意是强盗，暗指老虎机使玩家变穷），而延伸到k种选择就是k-armed，这一选择的过程就像是玩家选择拉下老虎机其中的一个杠杆，而奖励就是达到某种特定的图形组合赢得奖金池。通过多次行动，玩家需要尽量选择赢奖几率最大的杠杆来最大化获胜的概率，当然事先玩家是不知道哪个杠杆胜率最高的。

对于k-armed bandit问题，每种行动都会有一个奖励期望，我们把这个叫做该行动的**value**，我们用$$A_t$$表示t时刻的行动，相应的奖励用$$R_t$$表示，则其value $$q_*(a)$$等于做出行动a的奖励期望： $$q_*(a) \doteq E[R_t|A_t=a]$$
如果我们知道每个行动对应的value，则k-armed bandit问题就是每次简单的选择具有最高value的行动，但是通常我们并不确切的知道这一信息，而是知道个大概的估计，我们将这一对value的估计值写作 $$Q_t(a)$$ ，我们希望 $$Q_t(a)$$尽量接近$$q_*(a)$$。

我们有了对value的估计值，则在每一次选择的时候都至少有一种行动的value的估计值最大，我们将这些行动称作**greedy action**。当我们选择了greedy action，则我们在**exploit**尽力利用我们当前已获得的信息，而如果我们选择了nongreedy action，则我们在**explore**即探索新的行动使得我们对于nongreedy action的估计值更准确。Exploitation对于某一步可能带来的即时的奖励最高，但是exploration可能长期来讲有助于产生更大的reward，这是因为我们对于nongreedy action的估计可能存在较大的不确定性，其中可能存在比greedy action更好的行动，但仅仅是因为探索不够造成对其估计偏低。在探索过程中，短期内的reward可能降低，但当你探索到了更好的行动后你可以多次exploit它们获取更高的总奖励。如何平衡exploration和exploitation是强化学习的核心问题之一。

# **平衡exploration和exploitation的方法**

我们来看看平衡exploration和exploitation有哪些基本方法。

为此，我们需要先定义value的估计值 $$Q_t(a)$$，其中一种比较简单直观的方法是对于当前已有的样本所求的均值：$$Q_t(a) \doteq \frac{\text{在t时刻前做出行动a带来的奖励之和}}{\text{在t时刻前做出行动a的次数}}=\frac{\sum_{i=1}^{t-1}R_i\cdot I_{A_i=a} }{\sum_{i=1}^{t-1}I_{A_i=a}}$$

其中$$I_{A_i=a}$$ 代表如果第i次行动选择了a则值为1，否则为0。当我们的样本量足够多时，则 $$Q_t(a)$$趋近于 $$q_*(a)$$。

最简单的行动规则就是每次都选择估计值最大的action，即 $$A_t \doteq \underset{a}{\arg\max}Q_t(a)$$，这就是上面提到的greedy action。这种规则每次都最大化即时的奖励而不去探索当前可能奖励较小的选项。一个简单的可以进行explore的方法是虽然我们大多数时候选择greedy action，但定期比如以小概率 $$\epsilon$$随机选取一个行动，这种方法称作 $$\epsilon-greedy$$ 方法。这一方法的优点可以保证随着取样量的增加，每一种行动都会被取样到，使得对行动的value估计值更接近于真实值。当然， $$\epsilon-greedy$$ 与greedy方法的优劣取决于所面对的问题，理论上，如果奖励的方差较大，则我们需要进行较多的探索才能找到合适的行动， $$\epsilon-greedy$$ 要优于greedy方法，但如果奖励方差较小甚至为零，则直接用greedy方法可能能尽早的exploit最优解。但是实际中，即使奖励方差较小， $$\epsilon-greedy$$ 也可能有其优势，因为我们之前都是假设奖励分布是平稳分布，但实际问题中奖励可能是随时间变化的，这个时候我们也需要 $$\epsilon-greedy$$  因为之前的nongreedy action可能随着时间进行变得优于当前的greedy action了。

我们定义了$$Q_t(a)$$为我们的采样平均值，我们能不能简化它的计算过程呢？我们用$$R_j$$来表示第j次选取该行动的reward，$$Q_n$$来表示选取该行动n-1次后对其value的估计：

$$Q_n \doteq \frac{R_1+R_2+...+R_{n-1}}{n-1}​$$

最直观的方式是我们记录所有的reward，而每次都利用这个公式进行计算，但是这种方法的缺点是随着采样次数的增加，我们需要越来越多的存储量记住所有的值而且要进行很多次重复计算。实际上，我们可以用dynammic programming的方法来简化这一过程

$$Q_{n+1}=\frac{1}{n}\sum_{i=1}^nR_i  \\=\frac{1}{n}(R_n+\sum_{i=1}^{n-1}R_i) \\=\frac{1}{n}(R_n+(n-1)\frac{1}{n-1}\sum_{i=1}^{n-1}R_i) \\=\frac{1}{n}(R_n+(n-1)Q_n) \\=Q_n+\frac{1}{n}[R_n-Q_n]​$$

这一形式可以看做是强化学习里常用的更新公式

$$新的估计值\leftarrow 旧估计值 + 步进值* [目标值-旧估计值]$$

的一种特殊形式，其中[目标值-旧估计值]代表我们估计的误差，当我们向目标值方向步进时该误差减小。这里，我们选取了随时间变化的步进值$$\frac{1}{n}​$$.
我们假设用函数bandit(a)来代表选取行动a系统返回的奖励值，则$$\epsilon-greedy​$$ 方法可用如下pseudocode表示：

![image-20190131193352321](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131193352321.png)

以上方法适用于平稳分布的老虎机问题，即奖励概率不随时间变化的问题，假如奖励随时间可能发生变化，则一个合理的想法是对于最近的奖励给予较高的权重而对于较久远的奖励给予较低的权重，一个常用的做法是将步进值选为一个常数，即

$$Q_{n+1} =Q_n+\alpha[R_n-Q_n]$$，其中 $$\alpha \in (0,1]$$ 是一个常数。

由该更新公式我们可以展开得到 $$Q_{n+1}=(1-\alpha)^nQ_1+\sum_{i=1}^n\alpha (1-\alpha)^{n-i}R_i $$

观察其形式，可知 $$R_i $$的权重 $$\alpha (1-\alpha)^{n-i}$$依赖于多久之前得到这一奖励， $$1-\alpha$$小于1，所以越早的奖励其权重衰减的越多，所以这一形式也被称作**exponential recency-weighted average**。

以上方法都在某种程度上依赖于 $$Q_1$$这一初始值，当然对于采样平均方法，随着采样量的增加所有行动都被选择过后，这一bias消失，对于步进值为常数的方法这一bias一直存在，但会随着时间减小。其缺点是我们相当于在算法中引入了一系列的变量需要我们去初始化，但同时也是优点，即我们可以将某些先验的经验融入到这些初始值中。这些初始值还可以用来鼓励exploration，如我们将所有行动的Q都初始化为很大的值，不论我们选取那种行动，其奖励都会小于这个初始值，这就鼓励我们去尝试其他的未被探索过的奖励估计值仍为这个较大的初始值的行动，直到所有的行动选择都被多次尝试后其value的估计值才开始收敛，所以虽然我们每次选取的是greedy action，但是过程中我们对所有行动都进行了有效的exploration，这一方法被称作**optimistic initial values**。



$$\epsilon-greedy$$ 方法进行explore的选择十分随机且概率平均分配，但实际上可能有的行动已被取样多次其不确定性较小，而有的行动不确定性较大需要更多的explore，一个合理的采样方式是同时考虑其奖励值以及这一估计值的不确定性即其获得更高奖励的潜力。一个有效的方法是：

$$A_t \doteq \underset{a}{\arg\max}[Q_t(a)+c\sqrt{\frac{ln\ t }{N_t(a)}}]$$

其中 $$ln\ t$$代表t的自然对数， $$N_t(a)$$代表t时刻前a被选择的次数，$$c$$ 用来控制进行exploration的程度。

这一方法被称作**upper confidence bound**，简称**UCB**，其平方根项代表了对于a的value的估值的不确定性，当a行动每次被选后$$N_t(a)]$$增加，其不确定性降低，如果未被选择分子项增加分母不变不确定性升高。这一形式代表了所有的行动都会被采样，但具有较低的value估计值或者已经被多次采样的行动其被选择的频率会随时间降低。UCB对于简单的问题比较有效，但是对于非平稳分布或者状态空间较大的问题，UCB方法通常不太实际。

# **梯度老虎机算法**

以上方法我们都是直接估计action value来决定选择哪种action，我们可以转换思路不直接估计action value，而是对每个action引入一个preference偏好函数，用 $$H_t(a)$$ 表示，其值越大，则该行动选取概率越大。注意偏好函数与奖励无直接关系，只有两个action之间的相对的偏好才有意义，例如如果对所有偏好值加上1000不应影响选取任一行动的概率，其概率可用softmax形式表示：

$$Pr\{A_t=a\}\doteq \frac{e^{H_t(a)}}{\sum_{b=1}^ke^{H_t(b)}} \doteq \pi _t(a)$$

其中 $$\pi _t(a)​$$定义为t时刻选取a行动的概率。

利用**随机梯度上升(stochastic gradient ascent)**方法,我们可以得到对偏好函数的如下更新公式（详细推导可见书中2.8节）：

![image-20190131193421959](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131193421959.png)

其中 $$\bar{R_t}$$代表了直到t时刻的奖励的平均值，如果选择的行动 $$A_t$$带来的奖励高于这一平均值，则之后采取行动 $$A_t$$的概率增加，否则降低。对于没有选择的行动则向相反方向更新。

这一方法被称作**gradient bandit algorithm**梯度老虎机算法，在以后的章节中我们会知道这一方法是gradient-based reinforcement learning算法中的一个特殊形式，之后会发展为actor-critic及policy-gradient算法。

# **总结**

这一章通过简单的k-arm bandit 问题引入了强化学习的一些基本概念，尤其是对于exploration和exploitation的平衡问题，讲解了以下几种处理方法：$$\epsilon-greedy$$ ，optimistic initial value以及UCB。同时引入了步进更新以及梯度算法等重要思想。下一章重点讨论马尔科夫决策过程，to be continued。



# 参考资料

Sutton和Barto合著强化学习[Reinforcement Learning: An Introduction](http://link.zhihu.com/?target=http%3A//incompleteideas.net/book/the-book-2nd.html)第二章