关于可解释机器学习的几点思考

 

Mix-and-match approaches for visualizing data and interpreting machine learning models and results.

可视化数据和解释机器学习模型和结果的混合匹配(Mix-and-match )方法。

 

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image005.png)

 

Inputs activating diﬀerent neurons in a neural network.

输入激活神经网络中的不同神经元。

 

You’ve probably heard by now that machine learning algorithms can use big data to predict whether a donor will give to a charity, whether an infant in a NICU will develop sepsis, whether a customer will respond to an ad, and on and on. Machine learning can even [drive cars ](http://www.nytimes.com/2016/12/21/technology/san-francisco-california-uber-driverless-car-.html)and [predict elections](http://www.nytimes.com/2016/11/10/technology/the-data-said-clinton-would-win-why-you-shouldnt-have-believed-it.html). ... Err, wait. Can it? I believe it can, but these recent high-profile hiccups should leave everyone who works with data (big or not) and machine learning algorithms asking themselves some very hard questions: do I understand my data? Do I understand the model and answers my machine learning algorithm is giving me? And do I trust these answers? Unfortunately, the complexity that bestows the extraordinary predictive abilities on machine learning algorithms also makes the answers the algorithms produce hard to understand, and maybe even hard to trust.

你可能已经听说过，机器学习算法可以使用大数据来预测捐赠者是否会给慈善机构捐款，NICU的婴儿是否会发展成败血症，客户是否会对广告做出反应等等。机器学习甚至可以。呃，等等。可以吗？我相信可以，但最近这些引人注目的小插曲应该会让每一个从事数据(大或不大)和机器学习算法的人问自己一些非常难以回答的问题:我理解自己的数据吗？我理解机器学习算法给我的模型和答案了吗？我相信这些答案吗？不幸的是，赋予机器学习算法非凡预测能力的复杂性也是如此,使得算法给出的答案难以理解，甚至难以信任。

 

Although it is possible to enforce monotonicity constraints (a relationship that only changes in one direction) between independent variables and a machine-learned response function, machine learning algorithms tend to create nonlinear, non-monotonic, non-polynomial, and even non-continuous functions that approximate the relationship between independent and dependent variables in a data set. (This relationship might also be referred to as the conditional distribution of the dependent variables, given the values of the independent variables.) These functions can then make very specific predictions about the values of dependent variables for new data— whether a donor will give to a charity, an infant in a NICU will develop sepsis, a customer will respond to an ad, etc. Conversely, traditional linear models tend to create linear, monotonic, and continuous functions to approximate the very same relationships. Even though they’re not always the most accurate predictors, the elegant simplicity of linear models makes the results they generate easy to interpret.

虽然可以在自变量和机器学习响应函数之间实施单调约束(一种只在一个方向上改变的关系)，但机器学习算法倾向于创建非线性、非单调、非多项式、甚至非连续的函数，这些函数近似于一个数据集中的独立变量和相关变量之间的关系。(这种关系也可以称为因变量的条件分布，给定自变量的值。)然后，这些功能可以对新数据的因变量值做出非常具体的预测ーー捐赠者是否会捐赠给慈善机构、NICU中的婴儿是否会患上败血症、客户是否会对广告做出反应等等。相反，传统的线性模型倾向于创建线性、单调和连续函数来近似相同的关系。尽管它们并不总是最准确的预测器，但线性模型优雅的简单性使得它们生成的结果易于解释。

 

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image007.png)

 

 

While understanding and trusting models and results is a general requirement for good (data) science, model interpretability is a serious legal mandate in the regulated verticals of banking, insurance, and other industries. Business analysts, doctors, and industry researchers simply must understand and trust their models and modeling results. For this reason, linear models were the go-to applied predictive modeling tool for decades, even though it usually meant giving up a couple points on the accuracy scale. Today, many organizations and individuals are embracing machine learning algorithms for predictive modeling tasks, but diﬃculties in interpretation still present a barrier for the widespread, practical use of machine learning algorithms.

尽管理解和信任模型和结果是良好(数据)科学的一般要求，但在银行、保险和其他行业的受监管垂直领域，模型可解释性是一项严肃的法律授权。业务分析师、医生和行业研究人员必须理解并信任他们的模型和建模结果。由于这个原因，线性模型是数十年来首选的应用预测建模工具，即使它通常意味着放弃几个点的准确度规模。今天，许多组织和个人都在使用机器学习算法来完成预测建模任务，但是解释困难仍然是机器学习算法广泛、实际应用的障碍。

 

In this article, I present several approaches beyond the usual error measures and assessment plots for visualizing data and interpreting machine learning models and results. Users are encouraged to mix and match these techniques to best fit their own needs.

在本文中，我提出了几种方法，超出了通常的错误措施和评估图可视化数据和解释机器学习模型和结果。鼓励用户混合和匹配这些技术，以最适合自己的需要。

 

Wherever possible, “interpretability” of each technique in this article is deconstructed into more basic components—complexity, scope, understanding, and trust—which I will first outline below.

只要有可能，本文中每种技术的"可解释性"都被解构为更基本的组成部分ー复杂性、范围、理解和信任ー我将在下面首先概述。

 

 

Complexity of response function to be explained

响应函数的复杂性待解释

 

 

Linear, monotonic functions: Functions created by linear regression algorithms are probably the most interpretable class of models. These models will be referred to here as “linear and monotonic,” meaning that for a change in any given independent variable (or sometimes combination or function of an independent variable), the response function changes at a defined rate, in only one direction, and at a magnitude represented by a readily available coeﬃcient. Monotonicity also enables intuitive and even automatic reasoning about predictions. For instance, if a lender rejects your credit card application, they can tell you why because their probability-of-default model often assumes your credit score, your account balances, and the length of your credit history are monotonically related to your ability to pay your credit card bill. When these explanations are created automatically, they are typically called “reason codes.” Of course, linear and monotonic response functions enable the calculation of relative variable importance measures, too. Linear and monotonic functions have several uses in machine learning interpretability. Part 1 and Part 2 below discuss the many ways linear, monotonic functions can be used to make machine learning interpretable.

线性、单调函数:线性回归算法创建的函数可能是最具解释力的模型。这些模型在这里称为"线性和单调"，这意味着对于任何给定的自变量(或有时是自变量的组合或函数)的变化，响应函数按确定的比率变化，只朝一个方向变化，其幅度由一个现成的系数表示。单调性还能够直观、甚至自动地推理预测。例如，如果贷款人拒绝你的信用卡申请，他们可以告诉你原因，因为他们的违约概率模型通常假设你的信用评分、你的账户余额和你的信用历史长度与你支付信用卡账单的能力单调相关。当这些解释被自动创建时，它们通常被称为"原因代码"当然，线性和单调响应函数也可以用来计算相对变量的重要性度量。线性函数和单调函数在机器学习的可解释性方面有几种用途。下面的第1部分和第2部分讨论了线性、单调函数可以用来使机器学习具有可解释性的许多方法。

 

Nonlinear, monotonic functions: Although most machine learned response functions are nonlinear, some can be constrained to be monotonic with respect to any given 

independent variable. While there is no single coeﬃcient that represents the change in the response function induced by a change in a single independent variable, nonlinear and monotonic functions do always change in one direction as a single input variable changes. Nonlinear, monotonic response functions usually allow for the generation of both reason codes and relative variable importance measures. Nonlinear, monotonic [response functions are highly interpretable and often suitable for use in regulated](https://www.youtube.com/watch?v=SitMy5oeN_A) [applications.](Of course, there are linear, non-monotonic machine-learned response functions that can, for instance, be created by the [multi-variate adaptive regression splines ](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines)approach. These functions are not highlighted here because they tend to be less accurate predictors than purely nonlinear, non-monotonic functions, while also lacking the high interpretability of their monotonic counterparts.)

非线性，单调函数:虽然大多数机器学习的响应函数是非线性的，一些可以约束为单调相对于任何给定自变量。虽然没有一个单一的系数代表响应函数的变化引起的变化在一个单一的独立变量，非线性和单调函数始终在一个方向上的变化作为一个单一的输入变量的变化。非线性，单调响应函数通常允许原因代码和相对变量重要性措施的生成。非线性，单调(当然，有线性的，非单调的机器学习的响应函数，例如，可以创建的方法。这里没有强调这些函数，因为它们往往不如纯粹的非线性、非单调函数准确，同时也缺乏单调函数的高度可解释性

Nonlinear, non-monotonic functions: Most machine learning algorithms create nonlinear, non-monotonic response functions. This class of functions is the most diﬃcult to interpret, as they can change in a positive and negative direction and at a varying rate for any change in an independent variable. Typically, the only standard interpretability measure these functions provide are relative variable importance measures. You may need to use a combination of additional techniques presented below to interpret these extremely complex models.

非线性，非单调函数:大多数机器学习算法创建非线性，非单调响应函数。这类函数是最难解释的，因为它们可以朝着正方向和负方向变化，而且对于一个自变量的任何变化，它们的变化率都是不同的。通常，这些函数提供的唯一标准解释性度量是相对可变的重要性度量。您可能需要结合使用下面提供的其他技术来解释这些极其复杂的模型。

Scope of interpretability

可解释范围



Global interpretability: Some of the presented techniques facilitate global interpretations of machine learning algorithms, their results, or the machine-learned relationship between the inputs and the dependent variable(s) (e.g., the model of the conditional distribution). Global interpretations help us understand the entire conditional distribution modeled by the trained response function, but global interpretations can be approximate or based on average values.

全局可解释性:一些提出的技术促进机器学习算法的全局解释，其结果，或机器学习的关系之间的输入和因变量(如，模型的条件分布)。全局解释可以帮助我们理解训练后的响应函数所建立的整个条件分布，但全局解释可以是近似的，也可以是基于平均值的。

 

Local interpretability: Local interpretations promote understanding of small regions of the conditional distribution, such as clusters of input records and their corresponding predictions, or deciles of predictions and their corresponding input rows. Because small sections of the conditional distribution are more likely to be linear, monotonic, or otherwise well-behaved, local explanations can be more accurate than global explanations.

地方可解释性:地方解释促进对条件分布的小区域的理解，例如输入记录的集群及其对应的预测，或预测的十分位数及其对应的输入行。因为条件分布的一小部分更可能是线性的，单调的，或者其他良好的行为，局部解释可以比全局解释更准确。

 

 

Understanding and trust

理解和信任

 

 

Machine learning algorithms and the functions they create during training are sophisticated, intricate, and opaque. Humans who would like to use these models have basic, emotional needs to understand and trust them because we rely on them for our livelihoods or because we need them to make important decisions for us. For some users, technical descriptions of algorithms in textbooks and journals provide enough insight to fully understand machine learning models. For these users, cross-validation, error measures, and assessment plots probably also provide enough information to trust a model. Unfortunately, for many applied practitioners, the usual definitions and assessments don’t often inspire full trust and understanding in machine learning models and their results. The techniques presented here go beyond the standard practices to engender greater understanding and trust. These techniques enhance understanding by providing specific insights into the mechanisms of the algorithms and the functions they create, or by providing detailed information about the answers they provide. The techniques below enhance trust by enabling users to observe or ensure the stability and dependability of machine learning algorithms, the functions they create, or the answers they generate.

机器学习算法和它们在训练中创造的功能是复杂的、复杂的和不透明的。想要使用这些模型的人有基本的情感需求去理解和信任它们，因为我们依赖它们来维持生计，或者因为我们需要它们来为我们做出重要的决定。对于一些用户来说，教科书和期刊中关于算法的技术描述为充分理解机器学习模型提供了足够的视角。对于这些用户来说，交叉验证、错误度量和评估图可能也提供了足够的信息来信任一个模型。不幸的是，对于许多应用实践者来说，通常的定义和评估并不能激发对机器学习模型及其结果的完全信任和理解。这里介绍的技术超越了标准实践，从而产生更大的理解和信任。这些技术通过提供关于算法的机制和它们创建的功能的具体见解，或者通过提供关于它们提供的答案的详细信息，来增强理解。下面的技术通过使用户能够观察或确保机器学习算法的稳定性和可靠性，他们创建的功能，或他们产生的答案来增强信任。

 

Another important way to classify model interpretability techniques is whether they are “model-agnostic,” meaning they can be applied to diﬀerent types of machine learning algorithms, or “model-specific,” meaning techniques that are only applicable

对模型可解释性技术进行分类的另一个重要方法是它们是否是"模型不可知的"，这意味着它们可以应用于不同类型的机器学习算法，或者"模型特定的"，意味着只适用的技术



for a single type or class of algorithm. The techniques herein are described in an approachable, high-level manner that is generally model-agnostic. Model-specific techniques will be called out for the reader when appropriate.

对于单一类型或类别的算法。此处所描述的技术是一种可接近的、高级的方式，通常与模型无关。特定于模型的技术将在适当的时候提供给读者。



The techniques are presented in three parts. Part 1 includes approaches for seeing and understanding your data in the context of training and interpreting machine learning algorithms, Part 2 introduces techniques for combining linear models and machine learning algorithms for situations where interpretability is of paramount importance, and Part 3 describes approaches for understanding and validating the most complex types of predictive models.

这些技术分为三个部分。第一部分包括在训练和解释机器学习算法的背景下查看和理解数据的方法，第二部分介绍了在可解释性至关重要的情况下结合线性模型和机器学习算法的技术，第三部分描述了理解和验证最复杂类型的预测模型的方法。

 

 

Part 1: Seeing your data

# 第一部分:查看数据



Most real data sets are hard to see because they have many variables and many rows. Like most sighted people, I rely on my visual sense quite heavily for understanding information. For me, seeing data is basically tantamount to understanding data. However, I can only understand two or three visual dimensions, preferably two. And, something called [change blindness ](https://en.wikipedia.org/wiki/Change_blindness)frustrates human attempts to reason analytically given information split across diﬀerent pages or screens. So, if a data set has more than two or three variables or more rows than can fit on a single page or screen, it’s realistically going to be hard to understand what’s going on in it without resulting to more advanced techniques than scrolling through countless rows of data.

大多数真实的数据集很难看到，因为它们有许多变量和行。像大多数视力正常的人一样，我非常依赖我的视觉来理解信息。对我来说，看到数据基本上等同于理解数据。然而，我只能理解两个或三个视觉维度，最好是两个。而且，有一种叫做阻挠的东西，它会使人们试图通过分析来推理分散在不同页面或屏幕上的信息的尝试失败。因此，如果一个数据集有超过两个或三个变量，或者超过一个页面或屏幕所能容纳的数据行，那么实际上很难理解其中发生了什么，除非使用比滚动数不清的数据行更高级的技术。

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image017.png)



Of course, there are many, many ways to visualize data sets. Most of the techniques highlighted below help illustrate **all** of a data set in just two dimensions, not just univariate or bivariate slices of a data set (meaning one or two variables at a time). This is important in machine learning because most machine learning algorithms automatically model high-degree interactions between variables (meaning the eﬀect of combining many—i.e., more than two or three—variables together). Traditional univariate and bivariate tables and plots are still important and you should use them, but they are more relevant in the context of traditional linear models and slightly less helpful in understanding nonlinear models that can pick up on arbitrarily high-degree interactions between independent variables.

当然，可视化数据集的方法很多很多。下面重点介绍的大多数技术可以帮助说明仅在两个维度中的所有数据集，而不仅仅是数据集的单变量或双变量切片(意味着一次只有一个或两个变量)。这在机器学习中很重要，因为大多数机器学习算法会自动建立变量之间高度交互的模型(意味着将多个变量组合在一起的效果)。传统的单变量和双变量表和图仍然很重要，你应该使用它们，但它们在传统线性模型的背景下更具相关性，在理解非线性模型方面稍微不那么有帮助，因为这些模型可以捕捉到独立变量之间任意高度的相互作用。

 

 

Glyphs

雕文

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image018.jpg)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 1. Glyphs representing operating systems and web browser (agent) types. Image courtesy of [Ivy Wang](https://twitter.com/ivy_wang) and the H2O.ai team.

图1。象形文字代表操作系统和web浏览器(代理)类型。图片由h2oai团队提供。

 

Glyphs are visual symbols used to represent data. The color, texture, or alignment of a glyph can be used to represent diﬀerent values or attributes of data. In Figure 1, colored circles are defined to represent diﬀerent types of operating systems and web browsers. When arranged in a certain way, these glyphs can be used to represent rows of a data set.

字形是用来表示数据的视觉符号。标志符号的颜色、纹理或对齐方式可用于表示数据的不同值或属性。在图1中，定义了彩色圆圈来表示不同类型的操作系统和web浏览器。当以某种方式排列时，这些象形文字可用于表示数据集的行。
































https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                                7/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 7 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image019.jpg)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 2. Glyphs arranged to represent many rows of a data set. Image courtesy of Ivy Wang and the H2O.ai team.

图2。排列成一个数据集的多行的象形文字。图片由IvyWang和h2oai团队提供。

 

Figure 2 gives an example of how glyphs can be used to represent rows of a data set. Each grouping of four glyphs can be either a row of data or an aggregated group of rows in a data set. The highlighted Windows/Internet Explorer combination is very common in the data set (represented by blue, teal, and green circles) and so is the OS X and Safari combination (represented by two grey circles). It’s quite possible these two combinations are two compact and disjoint clusters of data. We can also see that, in general, operating system versions tend to be older than browser versions and that using Windows and Safari is correlated with using newer operating system and browser versions, whereas Linux users and bots are correlated with older operating system and browser versions. The red dots that represent queries from bots standout visually (unless you are red-green colorblind). Using bright colors or unique alignments for events of interest or outliers is a good method for making important or unusual data attributes clear in a glyph representation.

图2给出了一个如何使用标志符号表示数据集的行的示例。四个字形的每个分组可以是一行数据，也可以是数据集中的聚合行组。突出显示的windows/internetexplorer组合在数据集中非常常见(由蓝色、蓝绿色和绿色圆圈表示)，OSx和Safari组合也是如此(由两个灰色圆圈表示)。这两种组合很有可能是两个紧凑而不相交的数据集群。我们还可以看到，一般来说，操作系统版本往往比浏览器版本更老，使用Windows和Safari与使用更新的操作系统和浏览器版本相关，而Linux用户和机器人则与更老的操作系统和浏览器版本相关。代表机器人查询的红点在视觉上很突出(除非你是红绿色盲)。对于感兴趣的事件或异常值，使用明亮的颜色或唯一的对齐方式是在字形表示中清晰显示重要或不寻常数据属性的好方法。

 

 

Correlation Graphs

相关图


































https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                                8/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 8 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image020.jpg)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 3. A correlation graph representing loans made by a large financial firm. Figure courtesy of Patrick Hall and the H2O.ai team.

图3。表示大型金融公司贷款的相关图。图片由PatrickHall和h2oai团队提供。

 

A correlation graph is a two-dimensional representation of the relationships (correlation) in a data set. While many details regarding the display of a correlation graph are optional and could be improved beyond those chosen for Figure 3, correlation graphs are a very powerful tool for seeing and understanding relationships (correlation) between variables in a data set. Even data sets with tens of thousands of variables can be displayed in two dimensions using this technique.

相关图是数据集中的关系(相关性)的二维表示。虽然关于显示相关图的许多细节是可选的，可以在图3选择的细节之外进行改进，但是相关图是查看和理解数据集中变量之间关系(相关性)的一个非常强大的工具。使用这种技术，即使是具有成千上万个变量的数据集也可以在二维空间中显示。

 

In Figure 3, the nodes of the graph are the variables in a loan data set and the edge weights (thickness) between the nodes are defined by the absolute values of their pairwise Pearson correlation. For visual simplicity, absolute weights below a certain

在图3中，图中的节点是贷款数据集中的变量，节点之间的边权(厚度)由它们两两相关的皮尔逊相关性的绝对值定义。为了视觉上的简单性，绝对权重低于某个






https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                                9/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 9 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

threshold are not displayed. The node size is determined by a node’s number of connections (node degree), node color is determined by a graph community calculation, and node position is defined by a graph force field algorithm. The correlation graph allows us to see groups of correlated variables, identify irrelevant variables, and discover or verify important relationships that machine learning models should incorporate, all in two dimensions.

阈值没有显示。节点大小由节点的连接数(节点度)决定，节点颜色由图社团计算决定，节点位置由图力场算法定义。相关图让我们可以看到一组相关的变量，识别不相关的变量，发现或验证机器学习模型应该包含的重要关系，所有这些都是在两个维度上。

 

In a supervised model built for the data represented in Figure 3, assuming one of the represented variables was an appropriate target, we would expect variable selection techniques to pick one or two variables from the light green, blue, and purple groups, we would expect variables with thick connections to the target to be important variables in the model, and we would expect a model to learn that unconnected

在为图3所示的数据建立的监督模型中，假设其中一个表示的变量是一个合适的目标，我们期望变量选择技术从浅绿色、蓝色和紫色的组中选择一个或两个变量，我们期望与目标有密切联系的变量在模型中是重要的变量，我们期望模型能够了解这些不相关的变量

 

variables like CHANNEL_R are not very important. Figure 3 also illustrates common sense relationships such as that between FIRST_TIME_HOMEBUYER_FLAG_N and ORIGINAL_INTEREST_RATE that should be reflected in a trustworthy model.

像CHANNELr这样的变量并不是很重要。图3还说明了常识的关系，如首次购房者flagn和原始利率之间的关系，应该反映在一个值得信赖的模型。

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image021.png)

 

 

2D projections

二维投影

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image022.jpg)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 4. Two-dimensional projections of the famous 784-dimensional MNIST data set using (left) Principal Components Analysis (PCA) and (right) a stacked denoising autoencoder. Image courtesy of Patrick Hall and the H2O.ai team.

图4。二维投影著名的784维MNIST数据集使用(左)主成分分析(PCA)和(右)堆叠去噪自动编码器。图片由PatrickHall和h2oai团队提供。

 

 

There are many techniques for projecting the rows of a data set from a usually high-dimensional original space into a more visually understandable lower-dimensional space, ideally two or three dimensions. Popular techniques include:

有许多技术可以将数据集的行从通常高维的原始空间投影到视觉上更容易理解的低维空间，最好是二维或三维空间。流行的技巧包括:

 

[Principal Component Analysis ](https://en.wikipedia.org/wiki/Principal_component_analysis)(PCA)

(PCA)

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image023.png)

 

[Multidimensional Scaling ](https://en.wikipedia.org/wiki/Multidimensional_scaling)(MDS)

(MDS)

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image023.png)![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image025.png)

 

[t-distributed Stochastic Neighbor Embedding ](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)(t-SNE)

(t-SNE)

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image023.png)






https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               10/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 10 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

[Autoencoder networks](https://en.wikipedia.org/wiki/Autoencoder)

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image023.png)![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image027.png)

 

Each of these techniques has strengths and weaknesses, but the key idea they all share is to represent the rows of a data set in a meaningful low-dimensional space. Data sets containing images, text, or even business data with many variables can be diﬃcult to visualize as a whole. These projection techniques enable high-dimensional data sets to be projected into a representative low-dimensional space and visualized using the trusty old scatter plot technique. A high-quality projection visualized in a scatter plot should exhibit key structural elements of a data set, such as clusters, hierarchy, sparsity, and outliers.

这些技术各有长处和短处，但它们都共享的关键思想是在有意义的低维空间中表示数据集的行。包含图像、文本、甚至包含许多变量的业务数据的数据集可能难以作为一个整体可视化。这些投影技术使得高维数据集能够投影到一个具有代表性的低维空间，并使用可信的老散点图技术进行可视化。在散点图中可视化的高质量投影应该展示数据集的关键结构元素，如集群、层次结构、稀疏性和异常值。

 

In Figure 4, the famous [MNIST data set ](http://yann.lecun.com/exdb/mnist/)is projected from its original 784 dimensions onto two dimensions using two diﬀerent techniques: PCA and autoencoder networks. The quick and dirty PCA projection can separate digits labeled as 0 from digits labeled as 1 very well. These two-digit classes are projected into fairly compact clusters, but the other digit classes are generally overlapping. In the more sophisticated (but also more computationally expensive) autoencoder projection, all the digit classes appear as clusters, with visually similar digits appearing close to one another in the reduced two-dimensional space. The autoencoder projection is capturing the presumed clustered structure of the original high-dimensional space and the relative locations of those clusters. Interestingly, both plots can pick up on a few outlying digits.

在图4中，使用两种不同的技术(PCA和自动编码器网络)将名人从原来的784个维度投影到两个维度。快速和脏PCA投影可以很好地将标记为0的数字与标记为1的数字分开。这些两位数字类被投射到相当紧凑的集群中，但其他数字类通常是重叠的。在更复杂(但也更昂贵)的自动编码器投影中，所有的数字类都以群的形式出现，视觉上相似的数字在缩小的二维空间中彼此接近。自动编码器投影捕捉原始高维空间的假定簇结构和簇的相对位置。有趣的是，这两个情节都可以在一些边远的数字上出现。



 

Projections can add an extra and specific degree of trust if they are used to confirm machine learning modeling results. For instance, if known hierarchies, classes, or clusters exist in training or test data sets and these structures are visible in 2D projections, it is possible to confirm that a machine learning model is labeling these structures correctly. A secondary check is to confirm that similar attributes of structures are projected relatively near one another and diﬀerent attributes of structures are projected relatively far from one another. Consider a model used to classify or cluster marketing segments. It is reasonable to expect a machine learning model to label older, richer customers diﬀerently than younger, less aﬄuent customers—and moreover, to expect that these diﬀerent groups should be relatively disjointed and compact in a projection, and relatively far from one another. Such results should also be stable under minor perturbations of the training or test data, and projections from perturbed versus non-perturbed samples can be used to check for stability or for potential patterns of change over time.

如果投影用于确认机器学习建模结果，它们可以增加额外的特定信任度。例如，如果在训练或测试数据集中存在已知的层次结构、类或集群，并且这些结构在二维投影中是可见的，那么就有可能确认机器学习模型正在正确地标记这些结构。二级检查是为了确认结构的相似属性相对靠近投影，不同属性的结构相对远离投影。考虑一个用于分类或集群营销部门的模型。我们有理由期望，机器学习模型将年长、富裕的消费者与年轻、不太富裕的消费者区分开来，而且期望这些不同的群体在预测中应该相对脱节、紧凑，彼此之间相对距离较远。这种结果也应该在训练或测试数据的微小扰动下保持稳定，并且可以利用扰动样本与未扰动样本的预测来检查稳定性或随时间变化的潜在模式。

 

 

 

Partial dependence plots

部分依赖图










https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               11/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 11 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image029.jpg)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 5. One-dimensional partial dependence plots from a gradient boosted tree ensemble model of the well-known California housing data set. Image courtesy Patrick Hall and the H2O.ai team.

图5。来自著名的加利福尼亚住房数据集的梯度增强树集合模型的一维部分相关图。图片由PatrickHall和h2oai团队提供。

 

Partial dependence plots show us the way machine-learned response functions change based on the values of one or two independent variables of interest, while averaging out the eﬀects of all other independent variables. Partial dependence plots with two independent variables are particularly useful for visualizing complex types of variable interactions between the independent variables of interest. Partial dependence plots can be used to verify monotonicity of response functions under monotonicity constraints, and they can be used to see the nonlinearity, non-monotonicity, and two-way interactions in very complex models. In fact, the way partial dependence plots enhance understanding is exactly by showing the nonlinearity, non-monotonicity, and two-way interactions between independent variables and a dependent variable in complex models. They can also enhance trust when displayed relationships conform to domain knowledge expectations, when the plots remain stable or change in expected ways over time, or when displayed relationships remain stable under minor perturbations of the input data.

部分依赖图向我们展示了机器学习的响应函数是如何根据感兴趣的一个或两个自变量的值变化的，同时平均了所有其他自变量的影响。具有两个独立变量的部分依赖图对于显示所感兴趣的独立变量之间变量相互作用的复杂类型特别有用。部分依赖图可以用来验证在单调约束下响应函数的单调性，也可以用来观察非线性、非单调性和双向相互作用的复杂模型。实际上，部分依赖图增强理解的方式正是通过表现复杂模型中自变量和因变量之间的非线性、非单调性和双向相互作用来实现的。当显示的关系符合领域知识期望时，当图形随时间保持稳定或预期方式发生变化时，或者当显示的关系在输入数据的微小扰动下保持稳定时，它们也可以增强信任。

 

 

 

Partial dependence plots are global in terms of the rows of a data set, but local in terms of the independent variables. They are used almost exclusively to show the relationship between one or two independent variables and the dependent variable [over the domain of the independent variable(s). Individual conditional expectation](https://arxiv.org/abs/1309.6392) [(ICE) plots, a newer and less well-known adaptation of partial dependence plots, ](https://arxiv.org/abs/1309.6392)can be used to create more localized explanations using the same ideas as partial

部分相关绘图是按照数据集的行进行的全局绘图，但是按照独立变量进行的局部绘图。它们几乎完全用于显示一个或两个独立变量之间的关系，因变量可用于创建更局部的解释使用相同的想法作为部分

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image030.png)








https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               12/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 12 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

dependence plots. ICE plots are particularly useful when there are strong relationships between many input variables.

依赖图。当许多输入变量之间有很强的关系时，ICE绘图特别有用。

 

 

Residual analysis

残余分析

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image032.jpg)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 6. Screenshot from an example residual analysis application. Image courtesy of [Micah Stubbs ](https://twitter.com/micahstubbs)and the H2O.ai team.

图6。一个残差分析应用实例的屏幕截图。图片由h2oai团队提供。

 

Residuals refer to the diﬀerence between the recorded value of a dependent variable and the predicted value of a dependent variable for every row in a data set. Generally, the residuals of a well-fit model should be randomly distributed because good models will account for most phenomena in a data set, except for random error. Plotting the residual values against the predicted values is a time-honored model assessment technique and a great way to see all your modeling results in two dimensions. If strong patterns are visible in plotted residuals, this is a dead giveaway that there are problems with your data, your model, or both. Vice versa, if models are producing randomly distributed residuals, this a strong indication of a well-fit, dependable, trustworthy model, especially if other fit statistics (i.e., R2, AUC, etc.) are in the appropriate ranges.

残差是指在一个数据集中，一个因变量的记录值和一个因变量的预测值之间的差值。一般来说，良好拟合模型的残差应该是随机分布的，因为好的模型可以解释数据集中的大多数现象，除了随机误差。根据预测值绘制剩余值是一种历史悠久的模型评估技术，也是查看所有二维建模结果的好方法。如果在绘制的残差中可以看到强模式，那么这就意味着您的数据、模型或两者都存在问题。反之亦然，如果模型产生随机分布的残差，这是一个良好的、可靠的、可信赖的模型的强烈迹象，特别是如果其他拟合的统计数据(例如，R2，AUC等)在适当的范围内。

 

 

 

In Figure 6, the callouts point to a strong linear pattern in the residuals. The plot shows the traditional residual plot and residuals plotted by certain independent variables. Breaking out the residual plot by independent variables can expose more granular information about residuals and assist in reasoning through the cause of non-random patterns. Figure 6 also points to outliers, which residual plots can help to identify. As many machine learning algorithms seek to minimize squared residuals, observations with high residual values will have a strong impact on most models, and

在图6中，标注指向残差中的强线性模式。该图显示了由某些自变量绘制的传统残差图和残差图。利用独立变量对残差图进行分割，可以揭示更多关于残差的细粒度信息，有助于通过非随机模式产生原因进行推理。图6还指出了异常值，这些剩余图可以帮助识别。由于许多机器学习算法寻求最小化平方残差，具有高残差值的观测值将对大多数模型产生强烈的影响，和






https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               13/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 13 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

human analysis of the validity of these outliers can have a big impact on model accuracy.

人类对这些异常值有效性的分析会对模型的准确性产生很大的影响。

 

Now that several visualization techniques have been presented, they can be tied back to the overarching concepts scope, complexity, understanding and trust by asking a few simple questions. These questions will be asked of techniques presented in later sections as well.

现在已经介绍了几种可视化技术，通过问一些简单的问题，可以将它们与总体概念的范围、复杂性、理解和信任联系起来。这些问题也将在后面的章节中介绍。

 

Do visualizations provide global or local interpretability?

可视化是否提供全局或局部解释能力？

 

 

Both. Most forms of visualizations can be used to see a courser view of the entire data set, or they can provide granular views of local portions of the data set. Ideally, advanced visualization tool kits enable users to pan, zoom, and drill-down easily. Otherwise, users can plot diﬀerent parts of the data set at diﬀerent scales themselves.

两者都有。大多数形式的可视化可用于查看整个数据集的courser视图，或者可以提供数据集的本地部分的细粒度视图。理想情况下，高级可视化工具包可以让用户轻松地平移、缩放和向下钻取。否则，用户可以以不同的尺度自己绘制数据集的不同部分。

 

What complexity of functions can visualizations help interpret?

可视化能帮助解释哪些功能的复杂性？

 

 

Visualizations can help explain functions of all complexities.

可视化可以帮助解释所有复杂的函数。

 

 

How do visualizations enhance understanding?

可视化如何增进理解？

 

 

For most people, visual representations of structures (clusters, hierarchy, sparsity, outliers) and relationships (correlation) in a data set are easier to understand than scrolling through plain rows of data and looking at each variable's values.

对于大多数人来说，数据集中的结构(集群、层次结构、稀疏性、离群值)和关系(相关性)的可视化表示要比在数据行中滚动查看每个变量的值更容易理解。

 

How do visualizations enhance trust?

可视化如何增强信任？

 

 

Seeing structures and relationships in a data set usually makes those structures and relationships easier to understand. An accurate machine learning model should create answers that are representative of the structures and relationships in a data set. Understanding the structures and relationships in a data set is a first step to knowing if a model’s answers are trustworthy.

在数据集中查看结构和关系通常使这些结构和关系更容易理解。一个精确的机器学习模型应该创建能够代表数据集中的结构和关系的答案。理解数据集中的结构和关系是了解模型的答案是否值得信赖的第一步。

 

In certain cases, visualizations can display the results of sensitivity analysis, which can also enhance trust in machine learning results. In general, visualizations themselves can sometimes be thought of as a type of sensitivity analysis when they are used to display data or models as they change over time, or as data are intentionally changed to test stability or important corner cases for your application.

在某些情况下，可视化可以显示敏感度分析的结果，这也可以增强对机器学习结果的信任。一般来说，可视化本身有时可以被认为是一种敏感度分析，当它们被用来显示随着时间的推移而变化的数据或模型时，或者当数据被有意地改变以测试稳定性或应用程序的重要角落情况时。

 

 

Part 2: Using machine learning in regulated industry

第二部分:机器学习在规范行业中的应用








https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               14/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 14 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

For analysts and data scientists working in regulated industries, the potential boost in predictive accuracy provided by machine learning algorithms may not outweigh their current realities of internal documentation needs and external regulatory responsibilities. For these practitioners, traditional linear modeling techniques may be the only option for predictive modeling. However, the forces of innovation and competition don’t stop because you work under a regulatory regime. Data scientists and analysts in the regulated verticals of banking, insurance, and other similar industries face a unique conundrum. They must find ways to make more and more accurate predictions, but keep their models and modeling processes transparent and interpretable.

对于在受监管行业工作的分析师和数据科学家来说，机器学习算法提供的预测准确性的潜在提高可能不会超过他们目前的内部文档需求和外部监管责任的现实。对于这些从业者来说，传统的线性建模技术可能是预测建模的唯一选择。然而，创新和竞争的力量不会因为你在监管体制下工作而停止。银行、保险和其他类似行业受监管垂直领域的数据科学家和分析师面临着一个独特的难题。他们必须找到方法做出越来越准确的预测，但是保持他们的模型和建模过程的透明性和可解释性。

 

 

 

The techniques presented in this section are newer types of linear models or models that use machine learning to augment traditional, linear modeling methods. Linear model interpretation techniques are highly sophisticated, typically model specific, and the inferential features and capabilities of linear models are rarely found in other classes of models. These techniques are meant for practitioners who just can’t use machine learning algorithms to build predictive models because of interpretability concerns. These models produce linear, monotonic response functions (or at least monotonic ones) with globally interpretable results like those of traditional linear models, but often with a boost in predictive accuracy provided by machine learning algorithms.

本节介绍的技术是使用机器学习来扩充传统的线性建模方法的较新类型的线性模型或模型。线性模型解释技术是高度复杂的，典型的特定模型，线性模型的推断特征和能力在其他类型的模型中很少发现。这些技术意味着从业人员谁只是不能使用机器学习算法来建立预测模型，因为解释性的考虑。这些模型产生的线性、单调响应函数(或至少是单调响应函数)与传统线性模型的结果一样具有全局可解释性，但机器学习算法往往提高了预测精度。

 

 

 

 

OLS regression alternatives

苏丹生命线行动回归方案

 

 

 

Penalized regression

被处罚的回归
























































https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               15/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 15 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image033.jpg)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 7. Shrunken feasible regions for L1/LASSO penalized regression parameters (left) and L2/ridge penalized regression parameters (right). Image courtesy Patrick Hall, Tomas Nykodym, and the H2O.ai team.

图7。L1/lasso惩罚回归参数(左)和l2/岭惩罚回归参数(右)的可行域缩小。图片由PatrickHall，TomasNykodym和h2oai团队提供。

 

[Ordinary least squares ](https://en.wikipedia.org/wiki/Least_squares)(OLS) regression is about 200 years old. Maybe it’s time to move on? As an alternative, penalized regression techniques can be a gentle introduction to machine learning. Contemporary penalized regression techniques usually combine [L1/LASSO ](https://en.wikipedia.org/wiki/Lasso_(statistics))penalties for variable selection purposes and [Tikhonov/L2/ridge ](https://en.wikipedia.org/wiki/Tikhonov_regularization)penalties for robustness in a technique known as [elastic net](https://en.wikipedia.org/wiki/Elastic_net_regularization). They also make fewer assumptions about data than OLS regression. Instead of solving the classic normal equation or using statistical tests for variable selection, penalized regression minimizes constrained objective functions to find the best set of regression parameters for a given data set. Typically, this is a set of parameters that model a linear relationship but also satisfy certain penalties for assigning correlated or meaningless variables to large regression coeﬃcients. You can learn all about penalized regression in **Elements of Statistical Learning**, but for our purposes here, it’s

回归大约有200年的历史。也许是时候向前看了？作为一种替代方法，惩罚回归技术可以温和地引入机器学习。现代惩罚回归技术通常将惩罚结合在一起，用于不同的选择目的，惩罚结合在一种被称为。与OLS回归相比，它们对数据的假设也更少。惩罚回归不是解决经典的正态方程，也不是使用统计检验来选择变量，而是使受约束的目标函数最小化，以便为给定的数据集找到最佳的回归参数集。通常情况下，这是一组参数，建模一个线性关系，但也满足一定的惩罚分配相关或无意义的变量大回归系数。你可以在年学习所有关于惩罚回归的知识，但是为了我们这里的目的，它是

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image034.png)![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image035.png)![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image036.png)![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image037.png)![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image038.png)

just important to know when you might want to try penalized regression.

重要的是要知道什么时候你可能会尝试缺陷回归。

 

 

Penalized regression has been applied widely across many research disciplines, but it is a great fit for business data with many columns, even data sets with more columns than rows, and for data sets with a lot of correlated variables. L1/LASSO penalties drive

惩罚回归已经在许多研究领域得到了广泛的应用，但是它非常适合于具有许多列的业务数据，甚至是列多于行的数据集，以及具有许多相关变量的数据集。L1/lasso点球得分






https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               16/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 16 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

unnecessary regression parameters to zero, selecting a small, representative subset of regression parameters for the regression model while avoiding potential multiple comparison problems that arise in forward, backward, and stepwise variable selection. Tikhonov/L2/ridge penalties help preserve parameter estimate stability, even when many correlated variables exist in a wide data set or important predictor variables are correlated. It’s also important to know penalized regression techniques don’t always create confidence intervals, t-statistics, or p-values for regression parameters. These types of measures are typically only available through iterative methods or bootstrapping that can require extra computing time.

为回归模型选择一个小的、有代表性的回归参数子集，同时避免前向、后向和逐步变量选择中可能出现的多重比较问题。Tikhonov/l2/岭惩罚有助于保持参数估计的稳定性，即使在广泛的数据集中存在许多相关变量或重要的预测变量是相关的。同样重要的是要知道惩罚回归技术并不总是为回归参数创建置信区间、t-统计量或p值。这些类型的度量值通常只能通过需要额外计算时间的迭代方法或自举方法获得。

 

 

Generalized Additive Models (GAMs)

广义可加模型(GAMs)

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image039.jpg)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 8. Spline functions for several variables created by a generalized additive model. Image courtesy Patrick Hall and the H2O.ai team.

图8。基于广义可加模型的多变量样条函数。图片由PatrickHall和h2oai团队提供。

 

[Generalized Additive Models ](https://en.wikipedia.org/wiki/Generalized_additive_model)(GAMs) enable you to hand-tune a tradeoﬀ between increased accuracy and decreased interpretability by fitting standard regression coeﬃcients to certain variables and nonlinear spline functions to other variables. Also, most implementations of GAMs generate convenient plots of the fitted splines. Depending on your regulatory or internal documentation requirements, you may be able to use the splines directly in predictive models for increased accuracy. If not, you may be able to eyeball the fitted spline and switch it out for a more interpretable polynomial, log, trigonometric or other simple function of the predictor variable that may also increase predictive accuracy. You can learn more about GAMs in **Elements of** **Statistical Learning**, too.

通过对特定变量拟合标准回归系数和对其他变量拟合非线性样条函数，可以在提高精度和降低可解释性之间进行权衡。此外，大多数GAMs的实现生成方便的拟合样条曲线图。根据您的法规或内部文档要求，您可以直接在预测模型中使用样条函数，以提高准确性。如果没有，你也许可以目测拟合的样条曲线，然后把它换成更能解释的多项式、对数、三角函数或其他预测变量的简单函数，这样也可以提高预测的准确性。你也可以在《统计学习元素》中学到更多关于GAMs的知识。

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image040.png)

 

 

Quantile regression

分量回归


























https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               17/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 17 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image041.jpg)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 9. An illustration of quantile regression in two dimensions. Figure courtesy of Patrick Hall and the H2O.ai team.

图9。分量回归的二维插图。图片由PatrickHall和h2oai团队提供。

 

[Quantile regression ](https://en.wikipedia.org/wiki/Quantile_regression)allows you to fit a traditional, interpretable, linear model to diﬀerent percentiles of your training data, allowing you to find diﬀerent sets of variables with diﬀerent parameters for modeling diﬀerent behaviors across a customer market or portfolio of accounts. It probably makes sense to model low-value customers with diﬀerent variables and diﬀerent parameter values from those of high-value customers, and quantile regression provides a statistical framework for doing so.

允许你适应传统的，可解释的，线性模型到你的培训数据的不同百分位数，允许你找到不同的变量集与不同的参数建模不同的行为跨一个客户市场或帐户组合。用不同的变量和参数值对低价值客户和高价值客户进行建模可能是有意义的，分量回归为此提供了一个统计框架。

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image042.png)

 

Do alternative regression techniques provide global or local interpretability?

替代回归技术是否提供全局或局部解释能力？

 

 

Alternative regression techniques often produce globally interpretable linear, monotonic functions that can be interpreted using coeﬃcient values or other traditional regression measures and statistics.

备选回归技术往往产生全局解释的线性，单调函数，可以解释使用系数值或其他传统的回归措施和统计。

 

What are the complexity of alternative regression functions?

替代回归函数的复杂性是什么？

 

 

Alternative regression functions are generally linear, monotonic functions. However, GAM approaches can create quite complex nonlinear functions.

备选回归函数一般是线性的，单调的函数。然而，GAM方法可以创建相当复杂的非线性函数。

 

How do alternative regression techniques enhance understanding?

替代回归技术如何增进理解？

 

 

It’s quite possible that the lessened assumption burden, the ability to select variables without potentially problematic multiple statistical significance tests, the ability to incorporate important but correlated predictors, the ability to fit nonlinear phenomena, or the ability to fit diﬀerent quantiles of the data's conditional distribution (and not just the mean of the conditional distribution) could lead to more accurate understanding of modeled phenomena.

假设负担的减轻、选择变量而不需要潜在的多重统计显著性检验的能力、合并重要但相关的预测因子的能力、拟合非线性现象的能力，或者对数据的条件分布的不同分位数的拟合能力(而不仅仅是条件分布的均值)，都可能导致对模型现象的更准确的理解。






https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               18/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 18 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

How do alternative regression techniques enhance trust?

替代回归技术如何增强信任？

 

 

Basically, these techniques are trusted linear models, but used in new and diﬀerent ways. Trust could be increased further if these techniques lead to more accurate results for your application.

基本上，这些技术是可信的线性模型，但用于新的和不同的方式。如果这些技术能够为您的应用程序带来更准确的结果，则可以进一步增加信任。

 

 

Build toward machine learning model benchmarks

建立机器学习模型基准

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image043.jpg)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 10. Assessment plots that compare linear models with interactions to machine learning algorithms.

图10。比较线性模型与交互作用和机器学习算法的评估图。

 

Figure courtesy of Patrick Hall and the H2O.ai team.

图片由PatrickHall和h2oai团队提供。

 

 

Two of the main diﬀerences between machine learning algorithms and traditional linear models are that machine learning algorithms incorporate many implicit, high-degree variable interactions into their predictions and that machine learning algorithms create nonlinear, non-polynomial, non-monotonic, and even non-continuous response functions.

机器学习算法与传统线性模型的两个主要区别是，机器学习算法将许多隐式的、高度变量的交互作用纳入其预测中，而机器学习算法创建非线性、非多项式、非单调、甚至非连续的响应函数。

 

If a machine learning algorithm is seriously outperforming a traditional linear model, fit a decision tree to your inputs and target and generate a plot of the tree. The variables that are under or over one another in each split typically have strong interactions. Try adding some of these interactions into the linear model, including high-degree interactions that occur over several levels of the tree. If a machine learning algorithm is vastly outperforming a traditional, linear model, also try breaking it into several piecewise linear models. GAMs or partial dependence plots are ways to see how machine-learned response functions treat a variable across its domain and can give insight into where and how piecewise models could be used. Multivariate adaptive regression splines is a statistical technique that can automatically discover and fit diﬀerent linear functions to diﬀerent parts of a complex, nonlinear conditional distribution. You can try multivariate adaptive regression splines to fit piecewise models directly.

如果一个机器学习算法比传统的线性模型表现更好，根据你的输入和目标拟合一个决策树并生成一个树图。在每个分裂中彼此低于或高于彼此的变量通常具有很强的相互作用。尝试将这些交互添加到线性模型中，包括在树的几个级别上发生的高度交互。如果一个机器学习算法大大优于一个传统的线性模型，也可以尝试把它分解成几个分段线性模型。Gams或部分依赖图是研究机器学习的响应函数如何处理跨领域的变量的方法，并且可以给出分段模型可以在哪里以及如何使用的洞察力。多元自适应回归样条是一种能够自动发现和拟合复杂非线性条件分布中不同部分的线性函数的统计技术。您可以尝试使用多元自适应回归样条直接拟合分段模型。












https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               19/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 19 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

Does building toward machine learning model benchmarks provide global or local interpretability?

建立机器学习模型基准是否提供全局或局部解释能力？

 

If linearity and monotonicity are maintained, this process will result in globally interpretable linear, monotonic functions. If piecewise functions are used, building toward machine learning model benchmarks could provide local interpretability, but potentially at the expense of global interpretability.

如果保持线性和单调性，这个过程将导致全局解释的线性，单调函数。如果使用分段函数，建立机器学习模型基准可以提供局部解释性，但可能以牺牲全局解释性为代价。

 

What complexity of function does building toward machine learning model benchmarks create?

建立机器学习模型基准会产生什么样的功能复杂性？

 

With caution, testing, and restraint, building toward machine learning benchmarks can preserve the linearity and monotonicity of traditional linear models. However, adding many interactions or piecewise components will result in extremely complex response functions.

通过谨慎、测试和克制，建立机器学习基准可以保持传统线性模型的线性和单调性。然而，添加许多交互或分段组件将导致极其复杂的响应函数。

 

How does building toward machine learning model benchmarks enhance understanding?

建立机器学习模型基准如何增进理解？

 

This process simply uses traditional, understandable models in a new way. Building toward machine learning model benchmarks could lead to greater understanding if more data exploration or techniques such as GAMs, partial dependence plots, or multivariate adaptive regression splines lead to deeper understanding of interactions and nonlinear phenomena in a data set.

这个过程只是以一种新的方式使用传统的、可理解的模型。如果更多的数据探索或技术，比如GAMs，部分相关图，或者多元自适应回归样条，能够更深入地理解数据集中的相互作用和非线性现象，那么建立机器学习模型基准可以导致更深入的理解。

 

How does building toward machine learning model benchmarks enhance trust?

建立机器学习模型基准如何增强信任？

 

 

This process simply uses traditional, trusted models in a new way. Building toward machine learning model benchmarks could lead to increased trust in models if additional data exploration or techniques such as GAMs, partial dependence plots, or multivariate adaptive regression splines create linear models that represent the phenomenon of interest in the data set more accurately.

这个过程只是以一种新的方式使用传统的、可信的模型。如果额外的数据探索或技术，如GAMs，部分依赖图，或多元自适应回归样条创建线性模型，更准确地表示数据集中感兴趣的现象，建立机器学习模型基准可以导致增加对模型的信任。

 

 

Machine learning in traditional analytics processes

传统分析过程中的机器学习






























https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               20/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 20 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image044.jpg)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 11. Diagrams of several potential uses for machine learning in traditional analytical processes. Figure courtesy of [Vinod Iyengar ](https://twitter.com/VinodIyengar)and the H2O.ai team.

图11。在传统分析过程中机器学习的几个潜在用途的图表。图片由h2oai团队提供。

 

Instead of using machine learning predictions directly for analytical decisions, traditional analytical lifecycle processes (such as data preparation and model deployment) can be augmented with machine learning techniques leading to potentially more accurate predictions from regulator-approved linear, monotonic models. Figure 11 outlines three possible scenarios in which analytical processes can be augmented with machine learning:

传统的分析生命周期过程(如数据准备和模型部署)不再直接使用机器学习预测进行分析决策，而是可以通过机器学习技术进行扩充，从而从监管机构认可的线性单调模型中获得更准确的预测。图11概述了三种可能的场景，其中分析过程可以通过机器学习得到增强:

 

Introduce complex predictors into traditional, linear models: Introducing interactions, polynomials, or simple functional transformations into linear models is a standard practice. Machine learning algorithms can be used to create diﬀerent types of nonlinear and non-polynomial predictors that can also represent high-degree interactions between independent variables. There are many options for creating these predictors. Examples include the nonlinear features extracted by autoencoder networks or the optimal bins represented by the terminal node labels of a decision tree.

在传统的线性模型中引入复杂的预测器:在线性模型中引入交互作用、多项式或简单的函数变换是一种标准的做法。机器学习算法可以用来创建不同类型的非线性和非多项式预测，也可以表示自变量之间的高度相互作用。有许多选项可以创建这些预测器。实例包括由自动编码网络提取的非线性特征或由决策树的终端节点标签表示的最优箱子。

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image023.png)

 

Use multiple gated linear models: Very often, segmenting data into smaller groups based on important data attributes or time and building linear models for each segment can lead to more accurate results. It is not uncommon for organizations to use several deployed linear models to handle diﬀerent market segments or diﬀerent times of year. Deciding how to manually fuse the predictions of these diﬀerent models can be a tedious task for analysts and data scientists. However, if data is collected about past model performance, this process can be automated by allowing

使用多个门控线性模型:通常情况下，根据重要的数据属性或时间将数据分成更小的组，并为每个组建立线性模型可以得到更准确的结果。组织使用多个部署的线性模型来处理不同的细分市场或一年中不同的时间段，这种情况并不少见。对于分析师和数据科学家来说，决定如何手动融合这些不同模型的预测是一项冗长乏味的任务。但是，如果收集了关于过去模型性能的数据，则可以通过允许

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image023.png)








https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               21/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 21 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

a gate model to decide which linear model an observation should be delegated to for a decision.

一个门模型来决定一个观察应该委托给哪个线性模型来做决定。

 

Predict linear model degradation: In most cases, models are trained on static snapshots of data and then validated on later snapshots of similar data. Though an accepted practice, this process leads to model degradation when the real-world phenomena represented in the training and validation data start to change. Such degradation could occur when competitors enter or leave a market, when macroeconomic factors change, or when consumer fads change, and for many other common reasons. If data is collected about market and economic factors and about past model performance, another model can be used to predict when traditional deployed models need to be retrained or replaced. Like changing an expensive mechanical component before it requires maintenance, models can be retrained or replaced before their predictive power lessens. (I’ve [written previously ](https://www.oreilly.com/ideas/the-preoccupation-with-test-error-in-applied-machine-learning)about the topic of test error in applied machine learning and ways that machine learning should be used in the real world.)

预测线性模型退化:在大多数情况下，模型是根据数据的静态快照进行训练，然后根据相似数据的后续快照进行验证。虽然这是一个公认的实践，但当培训和验证数据中表示的现实世界现象开始发生变化时，这个过程会导致模型退化。当竞争对手进入或离开市场、宏观经济因素发生变化、消费者潮流发生变化以及许多其他常见原因时，这种退化就可能发生。如果收集有关市场和经济因素以及过去模型表现的数据，则可以使用另一个模型来预测何时需要重新训练或更换传统的部署模型。就像更换昂贵的机械部件之前，它需要维修，模型可以重新训练或更换之前，他们的预测能力减少。(我讨论过应用机器学习中的测试错误，以及机器学习在现实世界中应用的方法。)

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image023.png)![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image045.png)

 

Of course, there are many other opportunities for incorporating machine learning into the lifecycle of a traditional model. You may have better ideas or implementations in place already!

当然，将机器学习纳入传统模型的生命周期还有许多其他的机会。您可能已经有了更好的想法或实现！

 

Does incorporation of machine learning into traditional analytical processes provide global or local interpretability?

将机器学习引入传统的分析过程是否能提供全局或局部解释性？

 

It generally attempts to retain the global interpretability of traditional linear models. However, adding features extracted by machine learning algorithms into a linear model can reduce global interpretability.

它通常试图保留传统线性模型的全局可解释性。然而，将机器学习算法提取的特征添加到线性模型中会降低全局解释性。

 

What complexity of function does incorporating machine learning into traditional analytical processes create?

将机器学习融入到传统的分析过程中会产生什么样的功能复杂性？

 

The goal is to continue using linear, monotonic response functions, but in more eﬃcient and automated ways.

我们的目标是继续使用线性的、单调的响应函数，但是是以更高效和自动化的方式。

 

How does the incorporation of machine learning into traditional analytical processes enhance understanding?

将机器学习引入传统的分析过程如何增进理解？

 

Incorporating machine learning models into traditional analytical processes aims to use linear, understandable models more eﬃciently and accurately. Understanding can be enhanced further if the process of adding nonlinear features to a linear model, using

将机器学习模型整合到传统的分析过程中，目的是更有效、更准确地使用线性、可理解的模型。如果将非线性特征添加到线性模型的过程中，使用








https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               22/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 22 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

gated models, or forecasting model degradation leads to deeper knowledge of driving phenomena that create nonlinearity, trends, or changes in your data.

门控模型，或预测模型退化导致更深入的知识驱动现象，创建非线性，趋势，或变化在您的数据。

 

How does the incorporation of machine learning into traditional analytical processes enhance trust?

将机器学习引入传统的分析过程如何增强信任？

 

It can help make our understandable models more accurate, and if augmentation does lead to increased accuracy, this is an indication that the pertinent phenomena in the data have been modeled in a more trustworthy, dependable fashion.

它可以帮助我们理解的模型更加准确，如果增强确实导致准确性的提高，这表明数据中的相关现象已经以一种更加可信、可靠的方式被建模。

 

 

Small, interpretable ensembles

小的，可解释的集合

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image046.jpg)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 12. A diagram of a small, stacked ensemble. Figure courtesy of Vinod Iyengar and the H2O.ai team.

图12。一个小的，堆叠的整体的图表。图片由VinodIyengar和h2oai团队提供。

 

 

Many organizations are so adept at traditional linear modeling techniques that they simply cannot squeeze much more accuracy out of any single model. One potential way to increase accuracy without losing too much interpretability is to combine the predictions of a small number of well-understood models. The predictions can simply be averaged, manually weighted, or combined in more mathematically sophisticated ways. For instance, predictions from the best overall model for a certain purpose can be combined with another model for the same purpose that excels at rare event detection. An analyst or data scientist could do experiments to determine the best weighting for the predictions of each model in a simple ensemble, and partial dependency plots could be used to ensure that the model inputs and predictions still behave monotonically with respect to one another.

许多组织对传统的线性建模技术如此熟练，以至于他们无法从任何单一模型中获得更多的精确度。在不损失太多可解释性的情况下提高准确性的一个潜在方法是将少数理解透彻的模型的预测结合起来。这些预测可以简单地用平均值，人工加权，或者用更复杂的数学方法组合起来。例如，某一目的的最佳整体模型的预测可以与另一个模型相结合，以达到在罕见事件检测方面出类拔萃的目的。分析师或数据科学家可以做实验，以确定在一个简单的集合中每个模型的预测的最佳权重，并且可以使用部分依赖图来确保模型输入和预测仍然相对于彼此单调地表现。

 

[If you prefer or require a more rigorous way to combine model predictions, then super](http://biostats.bepress.com/ucbbiostat/paper222/) [learners are a great option. Super learners are a specific implementation of stacked](http://dl.acm.org/citation.cfm?id=148453) [generalization introduced by Wolpert in the early 1990s. Stacked generalization ](http://dl.acm.org/citation.cfm?id=148453)uses a

使用

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image047.png)![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image048.png)![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image049.png)






https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               23/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 23 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

combiner model to decide the weighting for the constituent predictions in the ensemble. Overfitting is a serious concern when stacking models. Super learners prescribe an approach for cross-validation and add constraints on the prediction weights in the ensemble to limit overfitting and increase interpretability. Figure 12 is an illustration of cross-validated predictions from two decision trees and a linear regression being combined by another decision tree in a stacked ensemble.

组合模型来决定总体中组分预测的权重。当叠加模型时，过拟合是一个严重的问题。超级学习者规定一个交叉验证的方法，并在集合中增加预测权重的约束，以限制过度拟合和增加可解释性。图12是一个交叉验证的预测例子，这些预测来自于两个决策树和一个由另一个决策树组合而成的线性回归。

 

Do small, interpretable ensembles provide global or local interpretability?

小的、可解释的集合是否提供全局或局部解释？

 

 

They provide increased accuracy, but may decrease overall global interpretability. They do not aﬀect the interpretability of each individual constituent model, but the resulting ensemble model may be more diﬃcult to interpret.

它们提高了准确性，但可能会降低整体的可解释性。它们不会影响每个个体组成模型的可解释性，但是由此产生的集合模型可能更难解释。

 

What complexity of function do small, interpretable ensembles create?

小的、可解释的集合创造了什么样的功能的复杂性？

 

 

They can create very complex response functions. To ensure interpretability is preserved, use the lowest possible number of individual constituent models, use simple, linear combinations of constituent models, and use partial dependence plots to check that linear or monotonic relationships have been preserved.

它们可以创建非常复杂的响应函数。为了确保可解释性得到保留，使用尽可能少的单个组成模型，使用简单的组成模型的线性组合，并使用部分依赖图来检查线性或单调关系是否得到保留。

 

How do small, interpretable ensembles enhance understanding?

小的、可解释的集合如何增进理解？

 

 

They enhance understanding if the process of combining interpretable models leads to greater awareness and familiarity with phenomena in your data that positively impacts generalization and predictions on future data.

如果组合可解释模型的过程能够提高对数据中的现象的认识和熟悉程度，从而对未来数据的概括和预测产生积极影响，那么它们能够增强理解能力。

 

How do small, interpretable ensembles enhance trust?

小型的、可解释的集合如何增强信任？

 

 

They allow us to boost the accuracy of traditional trustworthy models without sacrificing too much interpretability. Increased accuracy is an indication that the pertinent phenomena in the data have been modeled in a more trustworthy, dependable fashion. Trust can be further enhanced by small, interpretable ensembles when models complement each other in ways that conform to human expectations and domain knowledge.

它们允许我们在不牺牲太多可解释性的情况下提高传统可信模型的准确性。准确性的提高表明数据中的相关现象已经以一种更值得信赖、更可靠的方式建模。当模型以符合人类期望和领域知识的方式相互补充时，通过小型的、可解释的集合可以进一步增强信任。

 

 

Monotonicity constraints

单调性约束






















https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               24/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 24 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image051.jpg)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 13. An illustration of monotonic data and model constraints for neural networks. Figure courtesy of Vinod Iyengar and the H2O.ai team.

图13。单调数据和神经网络模型约束的一个例子。图片由VinodIyengar和h2oai团队提供。

 

Monotonicity constraints can turn diﬃcult-to-interpret nonlinear, non-monotonic models into highly interpretable, and possibly regulator-approved, nonlinear, monotonic models. Monotonicity is very important for at least two reasons:

单调约束可以将难以解释的非线性、非单调模型转化为高度可解释的、可能由调节器批准的、非线性、单调的模型。单调性非常重要，至少有两个原因:

 

\1. Monotonicity is often expected by regulators: no matter what a training data sample says, regulators may still want to see monotonic behavior. Consider savings account balances in credit scoring. A high savings account balance should be an indication of creditworthiness, whereas a low savings account balance should be an indicator of potential default risk. If a certain batch of training data contains many examples of individuals with high savings account balances defaulting on loans or individuals with low savings account balances paying oﬀ loans, of course a machine-learned response function trained on this data would be non-monotonic with respect to savings account balance. This type of predictive function could be unsatisfactory to regulators because it defies decades of accumulated domain expertise and thus decreases trust in the model or sample data.

监管机构经常预期单调性:无论培训数据样本说什么，监管机构可能仍然希望看到单调行为。考虑信用评分中的储蓄账户余额。高储蓄账户余额应该表明信用可靠性，而低储蓄账户余额应该表明潜在的违约风险。如果某一批培训数据包含许多例子，说明储蓄账户余额高的个人拖欠贷款，或储蓄账户余额低的个人偿还贷款，当然，根据这些数据培训的机器学习反应函数将是关于储蓄账户余额的非单调函数。这种类型的预测功能可能不能令监管者满意，因为它违背了数十年积累的领域专业知识，因此降低了对模型或样本数据的信任。

 

\2.  Monotonicity enables consistent reason code generation: consistent reason code generation is generally considered a gold standard of model interpretability. If monotonicity is guaranteed by a credit scoring model, reasoning about credit applications is straightforward and automatic. If someone's savings account balance is low, their credit worthiness is also low. Once monotonicity is assured, reasons for credit decisions can then be reliably ranked using the max-points-lost method. The max-points-lost method places an individual on the monotonic, machine-learned response surface and measures their distance from the maximum point on the surface (i.e., the ideal, most creditworthy possible customer). The axis (e.g., independent variable) on which an individual is the farthest from the ideal

单调性支持一致原因代码生成:一致原因代码生成通常被认为是模型可解释性的黄金标准。如果信用评分模型保证了单调性，那么关于信用评分应用的推理就是直截了当和自动化的。如果某人的储蓄账户余额很低，他们的信用可靠性也很低。一旦单调性得到保证，信贷决策的原因就可以可靠地使用最大点损失法进行排序。Max-points-lost方法将个体放置在单调的、机器学习的响应曲面上，并测量他们与表面上最大点(即理想的、可能信誉最好的客户)之间的距离。一个人离理想最远的轴线(例如，自变量)






https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               25/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 25 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

customer is the most important negative reason code for a credit decision. The axis (e.g., independent variable) on which an individual is the closest to the ideal customer is the least important negative reason code for a credit decision, and other independent variables are ranked as reason codes between these two given the position of the individual in relation to the ideal customer. Monotonicity simply ensures clear, logical reasoning using the max-points-lost method: under a monotonic model, an individual who was granted a loan could **never** have a lower savings account balance than an individual who was denied a loan.

顾客是信用决策最重要的消极原因代码。个人最接近理想客户的轴(例如，独立变量)是信用决策中最不重要的消极原因代码，其他独立变量则作为这两者之间的原因代码，给定个人相对于理想客户的位置。单调性只是确保了逻辑推理清晰，使用了最大失分法:在单调模型下，一个被授予贷款的个人的储蓄账户余额永远不会低于一个被拒绝贷款的个人。

 

Monotonicity can arise from constraints on input data, constraints on generated models, or from both. Figure 13 represents a process where carefully chosen and processed non-negative, monotonic independent variables are used in conjunction with a single hidden layer neural network training algorithm that is constrained to produce only positive parameters. This training combination generates a nonlinear, monotonic response function from which reason codes can be calculated, and by analyzing model parameter values, high-degree interactions can be identified. Finding and creating such non-negative, monotonic independent variables can be a tedious, time-consuming, trial-and-error task. Luckily, neural network and tree-based response functions can usually be constrained to be monotonic with respect to any given independent variable without burdensome data preprocessing requirements. [Monotonic neural networks ](https://papers.nips.cc/paper/1358-monotonic-networks.pdf)often entail custom architecture and constraints on the values of the generated model parameters. For [tree-based models](https://github.com/dmlc/xgboost/issues/1514), monotonicity constraints are usually enforced by a uniform splitting strategy, where splits of a variable in one direction always increase the average value of the dependent variable in the resultant child node, and splits of the variable in the other direction always decrease the average value of the dependent variable in resultant child node. As implementations of monotonicity constraints vary for diﬀerent types of models in practice, they are a model-specific interpretation technique.

单调性可能来自输入数据的约束、生成模型的约束，或者两者都有。图13是一个精心挑选和处理的非负单调自变量与单一隐藏层神经网络训练算法结合使用的过程，该算法只能产生正参数。这种训练组合产生一个非线性、单调的响应函数，从中可以计算出原因码，并通过分析模型参数值，可以识别出高度的相互作用。寻找和创建这种非负的、单调的独立变量可能是一项乏味、耗时、反复试验的任务。幸运的是，神经网络和基于树的响应函数对于任何给定的自变量通常可以被约束为单调的，而不需要繁琐的数据预处理要求。通常需要自定义体系结构和对生成的模型参数值的约束。单调约束通常采用均匀分裂策略，其中单向分裂的变量总是增加结果子节点中因变量的平均值，反向分裂的变量总是减少结果子节点中因变量的平均值。由于单调性约束的实现在实践中因不同类型的模型而异，因此它们是一种特定于模型的解释技术。

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image052.png)![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image053.png)

 

 

 

Do monotonicity constraints provide global or local interpretability?

单调性约束是否提供全局或局部解释性？

 

 

Monotonicity constraints create globally interpretable response functions.

单调性约束创建全局可解释的响应函数。

 

 

What complexity of function do monotonicity constraints create?

单调性约束产生了什么样的函数复杂性？

 

 

They create nonlinear, monotonic response functions.

它们产生非线性、单调的响应函数。

 

 

How do monotonicity constraints enhance understanding?

单调性约束如何增强理解？

 

 

They enable automatic generation of reason codes and for certain cases (i.e., single hidden-layer neural networks and single decision trees) important, high-degree

它们能够自动生成原因代码，并且在某些情况下(例如，单一隐藏层神经网络和单一决策树)是重要的、高度的






https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               26/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 26 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

variable interactions can also be automatically determined.

变量间的相互作用也可以自动确定。

 

 

How do monotonicity constraints enhance trust?

单调性约束如何增强信任？

 

 

Trust is increased when monotonic relationships, reason codes, and detected interactions are parsimonious with domain expertise or reasonable expectations. Sensitivity analysis can also increase trust in the model if results remain stable when input data undergoes minor perturbations and if the model changes in dependable, predictable ways over time.

当单调的关系、原因代码和检测到的交互与领域专业知识或合理的期望相差甚远时，信任就会增加。如果输入数据经历了微小的扰动，而且随着时间的推移，模型以可靠的、可预测的方式发生了变化，如果结果保持稳定，敏感度分析还可以增加对模型的信任。

 

 

Part 3: Understanding complex machine learning models

第三部分:理解复杂机器学习模型

 

 

The techniques presented in this part can create interpretations for nonlinear, non-monotonic response functions. They can be used alongside techniques discussed in Parts 1 and 2 to increase the interpretability of all model types. Practitioners might need to use more than one of the interpretability techniques described below to derive satisfactory explanations for their most complex models.

这一部分介绍的技术可以创建非线性，非单调响应函数的解释。它们可以与第1部分和第2部分中讨论的技术一起使用，以增加所有模型类型的可解释性。从业人员可能需要使用下述一种以上的可解释性技术，以便对其最复杂的模型作出令人满意的解释。

 

 

Surrogate models

代理模型

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image054.jpg)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 14. An illustration of surrogate models for explaining a complex neural network. Figure courtesy of Patrick Hall and the H2O.ai team.

图14。解释复杂神经网络的代理模型图解。图片由PatrickHall和h2oai团队提供。

 

A surrogate model is a simple model that is used to explain a complex model. Surrogate models are usually created by training a linear regression or decision tree on

代理模型是一个用来解释复杂模型的简单模型。代理模型通常是通过训练一个线性回归或决策树来创建的






https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               27/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 27 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

the original inputs and predictions of a complex model. Coeﬃcients, variable importance, trends, and interactions displayed in the surrogate model are then assumed to be indicative of the internal mechanisms of the complex model. There are few, possibly no, theoretical guarantees that the simple surrogate model is highly representative of the more complex model.

复杂模型的原始输入和预测。系数，可变的重要性，趋势，和相互作用显示在代理模型，然后假定是指示的内部机制的复杂模型。简单的代理模型高度代表了更复杂的模型，这一点在理论上几乎没有保证。

 

What is the scope of interpretability for surrogate models?

代理模型的可解释范围是什么？

 

 

Generally, surrogate models are global. The globally interpretable attributes of a simple model are used to explain global attributes of a more complex model. However, there is nothing to preclude fitting surrogate models to more local regions of a complex model's conditional distribution, such as clusters of input records and their corresponding predictions, or deciles of predictions and their corresponding input rows. Because small sections of the conditional distribution are more likely to be linear, monotonic, or otherwise well-behaved, local surrogate models can be more accurate than global surrogate models. [Local Interpretable Model-agnostic Explanations](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime) (discussed in the next section) is a formalized approach for local surrogate models. Of course, global and local surrogate models can be used together to foster both global and local interpretability.

通常，代理模型是全局的。简单模型的全局可解释属性用于解释更复杂模型的全局属性。然而，并不排除将替代模型拟合到一个复杂模型的条件分布的更多局部区域，比如输入记录的集群及其对应的预测，或者预测的十分位数及其对应的输入行。因为条件分布的一小部分更有可能是线性的、单调的或其他良好的行为，局部代理模型可以比全局代理模型更准确。(在下一节中讨论)是局部代理模型的形式化方法。当然，全局和局部代理模型可以一起用来促进全局和局部的可解释性。

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image055.png)

 

 

What complexity of functions can surrogate models help explain?

代理模型能够帮助解释什么样的函数复杂性？

 

 

Surrogate models can help explain machine learning models of any complexity, but they are probably most helpful for nonlinear, non-monotonic models.

代理模型可以帮助解释任何复杂的机器学习模型，但它们可能对非线性、非单调模型最有帮助。

 

How do surrogate models enhance understanding?

代理模型如何增进理解？

 

 

Surrogate models enhance understanding because they provide insight into the internal mechanisms of complex models.

代理模型增强了理解，因为它们提供了对复杂模型内部机制的洞察力。

 

How do surrogate models enhance trust?

代理模型如何增强信任？

 

 

Surrogate models enhance trust when their coeﬃcients, variable importance, trends, and interactions are in line with human domain knowledge and reasonable expectations of modeled phenomena. Surrogate models can increase trust when used in conjunction with sensitivity analysis to test that explanations remain stable and in line with human domain knowledge, and reasonable expectations when data is lightly and purposefully perturbed, when interesting scenarios are simulated, or as data changes over time.

当代理模型的系数、变量重要性、趋势和交互符合人类领域知识和对模型现象的合理预期时，代理模型增强了信任。代理模型可以增加信任，当联合使用敏感度分析，以测试解释保持稳定，符合人类领域的知识，合理的预期时，数据是轻微和故意的扰动，当有趣的情景被模拟，或随着时间的数据变化。














https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               28/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 28 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

 

Local Interpretable Model-agnostic Explanations (LIME)

本地可解释的模型无关解释(LIME)

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image056.jpg)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 15. An illustration of the LIME process in which a weighted linear model is used to explain a single prediction from a complex neural network. Figure courtesy of [Marco Tulio Ribeiro](https://www.oreilly.com/people/marco-tulio-ribeiro); [image ](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime)used with permission.

图15。用加权线性模型解释来自复杂神经网络的单个预测的LIME过程的一个例子。经许可使用。

 

 

[LIME ](https://arxiv.org/pdf/1606.05386.pdf)is a prescribed method for building local surrogate models around single observations. It is meant to shed light on how decisions are made for specific observations. LIME requires that a set of explainable records be found, simulated, or created. This set of well-understood records is used to explain how machine learning algorithms make decisions for other, less well-understood records. An implementation of LIME may proceed as follows. First, the set of explainable records are scored using the complex model. Then, to interpret a decision about another record, the explanatory records are weighted by their closeness to that record, and an L1 regularized linear model is trained on this weighted explanatory set. The parameters of the linear model then help explain the prediction for the selected record.

是围绕单个观测建立局部代理模型的规定方法。它的目的是阐明如何作出决定的具体观察。Lime要求找到、模拟或创建一组可解释的记录。这组易于理解的记录用于解释机器学习算法是如何为其他不易理解的记录做出决策的。Lime的实施可按以下方式进行。首先，使用复杂模型对可解释的记录集进行计分。然后，为了解释关于另一个记录的决策，根据记录与该记录的接近程度对解释记录进行加权，并在此加权解释集上训练L1正则化线性模型。然后线性模型的参数帮助解释所选记录的预测。

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image057.png)

 

LIME was originally described in the context of explaining image or text classification decisions. It could certainly also be applied to business or customer data, for instance by explaining customers at every decile of predicted probabilities for default or churn, or by explaining representative customers from well-known market segments. Multiple [implementations of LIME are available. Two I see the most often are from the original](https://github.com/marcotcr/lime) [authors of LIME and from the ](https://github.com/marcotcr/lime)[eli5 package](http://eli5.readthedocs.io/en/latest/index.html)[, which implements several handy machine](https://github.com/marcotcr/lime) learning interpretability tools.

石灰最初是在解释图像或文本分类决定的上下文中描述的。它当然也可以应用于企业或客户数据，例如，在每十分位数的预测违约或流失概率中解释客户，或者解释来自知名细分市场的具有代表性的客户。多种学习解释工具。

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image048.png)![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image059.png)

 

What is the scope of interpretability for LIME?

莱姆可解释的范围是什么？






https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               29/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 29 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

LIME is a technique for local interpretability.

Lime是一种适合当地解释的技术。

 

 

What complexity of functions can LIME help explain?

Lime可以帮助解释哪些功能的复杂性？

 

 

LIME can help explain machine learning models of any complexity, but it is probably most appropriate for nonlinear, non-monotonic models.

Lime可以帮助解释任何复杂的机器学习模型，但它可能最适合非线性，非单调模型。

 

How does LIME enhance understanding?

石灰如何提高认识？

 

 

LIME provides insight into the important variables and their linear trends around specific, important observations, even for extremely complex response functions.

Lime提供了围绕特定重要观测的重要变量及其线性趋势的洞察力，即使对于极其复杂的响应函数也是如此。

 

How does LIME enhance trust?

Lime如何增强信任？

 

 

LIME increases trust when the important variables and their linear trends around specific records conform to human domain knowledge and reasonable expectations of modeled phenomena. LIME can also enhance trust when used in conjunction with maximum activation analysis to see that a model treats obviously diﬀerent records using diﬀerent internal mechanisms and obviously similar records using similar internal mechanisms. LIME can even be used as a type of sensitivity analysis to determine whether explanations remain stable and in line with human domain knowledge and expectations when data is intentionally and subtly perturbed, when pertinent scenarios are simulated, or as data changes over time.

当围绕特定记录的重要变量及其线性趋势符合人类领域知识和对模型现象的合理预期时，LIME会增加信任。当与最大激活分析结合使用时，LIME也可以增强信任，看到一个模型使用不同的内部机制处理明显不同的记录，使用类似的内部机制处理明显相似的记录。甚至可以用LIME作为一种敏感度分析，来判断当数据被有意和微妙地扰乱时，当相关的场景被模拟时，或者当数据随着时间的推移而变化时，解释是否保持稳定，是否符合人类领域的知识和期望。

 

 

Maximum activation analysis

最大激活分析
































































https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               30/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 30 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image060.jpg)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 16. Illustration of diﬀerent inputs activating diﬀerent neurons in a neural network. Image courtesy of Patrick Hall and the H2O.ai team.

图16。图示一个神经网络中激活不同神经元的不同输入。图片由PatrickHall和h2oai团队提供。

 

In maximum activation analysis, examples are found or simulated that maximally activate certain neurons, layers, or filters in a neural network or certain trees in decision tree ensembles. For the purposes of maximum activation analysis, low residuals for a certain tree are analogous to high-magnitude neuron output in a neural network. Like monotonicity constraints in Part 2, maximum activation analysis is a general idea that is likely a model-specific interpretation technique in-practice.

在最大激活分析，例子被发现或模拟，最大限度地激活某些神经元，层，或过滤器在一个神经网络或某些树的决策树集合。出于最大激活分析的目的，某棵树的低残差类似于神经网络中的高幅值神经元输出。与第2部分中的单调性约束一样，最大激活分析也是一个通用的概念，它可能是实践中特定于模型的解释技术。

 

Maximum activation analysis elucidates internal mechanisms of complex models by determining the parts of the response function that specific observations or groups of similar observations excite to the highest degree, either by high-magnitude output from neurons or by low residual output from trees. Maximum activation analysis can also find interactions when diﬀerent types of observations consistently activate the same internal structures of a model.

最大激活分析通过确定特定观测值或类似观测值组激发到最高程度的响应函数部分来阐明复杂模型的内部机制，这些响应函数可以通过神经元的高量级输出或树木的低剩余输出来激发。最大激活分析还可以找到相互作用时，不同类型的观察一致激活相同的内部结构的模型。

 

Figure 16 is an idealized illustration of a good customer causing high-magnitude outputs from a diﬀerent set of neurons than a fraudulent customer. Here the red highlighting indicates the three maximally activated neurons for each type of input. The maximally activated neurons are diﬀerent for these two very diﬀerent inputs, indicating that the internal structure of the model treats input classes diﬀerently. If this pattern were to be consistent for many diﬀerent examples of good customers and

图16是一个理想化的例子，说明一个好客户与一个欺诈客户相比，从另一组神经元中导致了高量级的输出。这里红色高亮显示每种输入类型最大激活的三个神经元。对于这两个非常不同的输入，最大激活的神经元是不同的，这表明模型的内部结构对待输入类是不同的。如果这个模式对于许多不同的好客户的例子是一致的






https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               31/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 31 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

fraudulent customers, it would also indicate that internal model mechanisms are stable and dependable.

欺诈客户，这也表明，内部模式机制是稳定和可靠的。

 

What is the scope of interpretability for maximum activation analysis?

最大激活分析的解释范围是什么？

 

 

Maximum activation analysis is local in scope because it illustrates how certain observations or groups of observations are treated by discernible aspects of a complex response function.

最大激活分析是局部范围，因为它说明了如何处理某些观察或观察组的可辨别方面的复杂响应函数。

 

What complexity of functions can maximum activation analysis help explain?

最大激活分析能帮助解释哪些功能的复杂性？

 

 

Maximum activation analysis can help explain machine learning models of any complexity, but it is probably best suited for nonlinear, non-monotonic models.

最大激活分析可以帮助解释任何复杂的机器学习模型，但它可能最适合非线性，非单调模型。

 

How does maximum activation analysis enhance understanding?

最大激活分析如何增进理解？

 

 

Maximum activation analysis increases understanding because it exposes the internal structures of complex models.

最大激活分析可以增加理解，因为它暴露了复杂模型的内部结构。

 

How does maximum activation analysis enhance trust?

最大激活分析如何增强信任？

 

 

LIME, discussed above, helps explain the predictions of a model in local regions of the conditional distribution. Maximum activation analysis helps enhance trust in localized, internal mechanisms of a model. The two make a great pair for creating detailed, local explanations of complex response functions. Maximum activation analysis enhances trust when a complex model handles obviously diﬀerent records using diﬀerent internal mechanisms and obviously similar records using similar internal mechanisms. It enhances trust when these internal mechanisms are consistently and repeatedly diﬀerent for many input examples. It enhances trust when found interactions match human domain knowledge or expectations, and maximum activation analysis also enhances trust when used as a type of sensitivity analysis. It can be used to investigate if internal treatment of observations remains stable when data is lightly perturbed, when interesting situations are simulated, or as data changes over time.

上面讨论的LIME有助于解释条件分布在局部地区的模型的预测。最大激活分析有助于增强对局部化的模型内部机制的信任。对于创建复杂响应函数的详细的、局部的解释，这两者是绝佳的组合。当一个复杂的模型使用不同的内部机制处理明显不同的记录，并使用相似的内部机制处理明显相似的记录时，最大激活分析增强了信任。当这些内部机制对于许多输入示例而言始终不同且反复不同时，它增强了信任。当发现互动符合人类领域的知识或期望时，它增强了信任，最大化激活分析作为一种敏感度分析时，也增强了信任。它可以用来研究，当数据受到轻微扰动时，当有趣的情况被模拟时，或者当数据随时间变化时，观测值的内部处理是否保持稳定。

 

 

Sensitivity analysis

敏感度分析


























https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               32/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 32 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia



 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 17. A representation of a variable’s distribution changing over time. Figure courtesy of Patrick Hall and the H2O.ai team.

图17。变量分布随时间变化的一种表示。图片由PatrickHall和h2oai团队提供。

 

Sensitivity analysis investigates whether model behavior and outputs remain stable when data is intentionally perturbed or other changes are simulated in data. Beyond traditional assessment practices, sensitivity analysis of machine learning model predictions is perhaps the most important validation technique for machine learning models. Machine learning models can make drastically diﬀering predictions from minor changes in input variable values. In practice, many linear model validation techniques focus on the numerical instability of regression parameters due to correlation between input variables or between input variables and the dependent variable. It may be prudent for those switching from linear modeling techniques to machine learning techniques to focus less on numerical instability of model parameters and to focus more on the potential instability of model predictions.

敏感度分析研究当数据被故意扰动或者在数据中模拟其他变化时，模型行为和输出是否保持稳定。超越传统的评估实践，机器学习模型预测的敏感度分析可能是机器学习模型最重要的验证技术。机器学习模型可以根据输入变量值的细微变化做出截然不同的预测。在实践中，许多线性模型验证技术侧重于由于输入变量之间或输入变量与因变量之间的相关性而导致的回归参数的数值不稳定性。对于那些从线性建模技术转向机器学习技术的人来说，谨慎的做法是减少对模型参数的数值不稳定性的关注，更多地关注模型预测的潜在不稳定性。

 

Sensitivity analysis can also test model behavior and outputs when interesting situations or known corner cases are simulated. Diﬀerent techniques, including those presented in this article and many others, can be used to conduct sensitivity analysis. Output distributions, error measurements, plots, and interpretation techniques can be used to explore the way models behave in important scenarios, how they change over time, or if models remain stable when data is subtly and intentionally corrupted.

敏感度分析也可以测试模型的行为和输出，当有趣的情况或已知的角落情况被模拟。不同的技术，包括本文以及其他许多技术，都可以用来进行敏感度分析。输出分布、误差测量、图表和解释技术可以用来探索模型在重要场景中的行为方式，它们随着时间的推移是如何变化的，或者当数据被微妙和故意破坏时，模型是否保持稳定。

 

What is the scope of interpretability for sensitivity analysis?

敏感度分析的解释范围是什么？

 

 

Sensitivity analysis is a global interpretation technique when global interpretation techniques are used, such as using a single, global surrogate model to ensure major interactions remain stable when data is lightly and purposely corrupted.

在使用全局解释技术时，敏感度分析是一种全局解释技术，比如使用单一的全局代理模型，以确保在数据轻度和故意损坏时，主要的交互仍然保持稳定。

 

Sensitivity analysis is a local interpretation technique when local interpretation techniques are used, for instance using LIME to determine if the important variables in

当使用局部解释技术时，敏感度分析是一种局部解释技术，例如使用LIME来确定是否存在重要的变量








https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               33/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 33 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

a credit allocation decision remain stable for a given customer segment under macroeconomic stress testing.

在宏观经济压力测试下，信贷分配决策对特定客户群保持稳定。

 

What complexity of functions can sensitivity analysis help explain?

什么功能的复杂性可以用敏感度分析来解释？

 

 

Sensitivity analysis can help explain the predictions of any type of response function, but is probably most appropriate for nonlinear response functions and response functions that model high degree variable interactions. For both cases, small changes in input variable values can result in large changes in a predicted response.

敏感度分析可以帮助解释任何类型的响应函数的预测，但可能是最适合的非线性响应函数和响应函数模型高度变量的相互作用。对于这两种情况，输入变量值的小变化都可能导致预测响应的大变化。

 

How does sensitivity analysis enhance understanding?

敏感度分析是如何增进理解的？

 

 

Sensitivity analysis enhances understanding because it shows a model's likely behavior and output in important situations, and how a model's behavior and output may change over time.

敏感度分析增强理解，因为它显示了模型在重要情况下的可能行为和输出，以及模型的行为和输出如何随着时间的推移而改变。

 

How does sensitivity analysis enhance trust?

敏感度分析如何增强信任？

 

 

Sensitivity analysis enhances trust when a model's behavior and outputs remain stable when data is subtly and intentionally corrupted. It also increases trust if models adhere to human domain knowledge and expectations when interesting situations are simulated, or as data changes over time.

当一个模型的行为和输出在数据被巧妙和故意破坏的情况下保持稳定时，敏感度分析增强了信任。当模拟有趣的情况时，或者当数据随时间变化时，如果模型坚持人类领域的知识和期望，那么它也会增加信任。

 

 

Variable importance measures

可变重要性度量

 

 

For nonlinear, non-monotonic response functions, variable importance measures are often the only commonly available quantitative measure of the machine-learned relationships between independent variables and the dependent variable in a model. Variable importance measures rarely give insight into even the average direction that a variable aﬀects a response function. They simply state the magnitude of a variable's relationship with the response as compared to other variables used in the model.

对于非线性、非单调的响应函数，变量重要性度量常常是唯一可用的定量度量模型中自变量和因变量之间的机器学习关系的方法。可变重要性度量甚至很少能洞察到变量影响响应函数的平均方向。与模型中使用的其他变量相比，它们只是简单地陈述了变量与响应的关系的大小。

 

 

Global variable importance measures

全局可变重要性度量






























https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               34/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 34 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia



 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 18. An illustration of variable importance in a decision tree ensemble model. Figure courtesy of Patrick Hall and the H2O.ai team.

图18。决策树集成模型中变重要性的一个例证。图片由PatrickHall和h2oai团队提供。

 

Variable importance measures are typically seen in tree-based models but are sometimes also reported for other models. As illustrated in Figure 18, a simple heuristic rule for variable importance in a decision tree is related to the depth and frequency at which a variable is split on in a tree, where variables used higher in the tree and more frequently in the tree are more important. For a single decision tree, a variable's importance is quantitatively determined by the cumulative change in the splitting criterion for every node in which that variable was chosen as the best splitting candidate. For a gradient boosted tree ensemble, variable importance is calculated as it is for a single tree but aggregated for the ensemble. For a random forest, variable importance is also calculated as it is for a single tree and aggregated, but an additional measure of variable importance is provided by the change in out-of-bag accuracy caused by shuﬄing the independent variable of interest, where larger decreases in accuracy are taken as larger indications of importance. (Shuﬄing is seen as zeroing-out the eﬀect of the given independent variable in the model, because other variables are not shuﬄed.) For neural networks, variable importance measures are typically associated with the aggregated, absolute magnitude of model parameters for a given variable of interest. Global variable importance techniques are typically model specific, and practitioners should be aware that unsophisticated measures of variable importance can be biased toward larger scale variables or variables with a high number of categories.

可变重要性度量通常出现在基于树的模型中，但有时也出现在其他模型中。如图18所示，关于决策树中变量重要性的简单启发式规则与变量在树中分裂的深度和频率有关，在树中使用的变量更高，在树中使用的频率更高。对于一个单一的决策树，一个变量的重要性是通过选择该变量作为最佳分裂候选变量的每个节点的分裂准则的累积变化来定量确定的。对于一个梯度增强的树集合，可变重要性被计算为一个单一的树，但聚合为集合。对于随机森林，可变重要性的计算方法也与对于单一树木的计算方法一样，而且是聚合的，但重要性可以通过改变感兴趣的独立变量引起的袋外精确度的变化来衡量，其中精确度下降幅度较大被视为重要性的较大迹象。(由于其他变量没有被洗牌，所以洗牌被认为是将模型中给定的自变量的影响归零。)对于神经网络，可变重要性度量通常与一个给定的感兴趣的变量的模型参数的聚合绝对星等有关。全局变量重要性技术通常是模型特定的，从业人员应该意识到，不成熟的变量重要性可能偏向于大规模的变量或具有大量类别的变量。

 

 

 

 

Leave-One-Covariate-Out (LOCO)

左一协变量输出(LOCO)




















https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               35/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 35 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia



 

 

 

 

 

 

 

 

 

 

 

 

Figure 19. An illustration of the LOCO approach. Figure courtesy of Patrick Hall and the H2O.ai team.

图19。Loco方法的一个示例。图片由PatrickHall和h2oai团队提供。

 

 

A recent [preprint article ](https://arxiv.org/pdf/1604.04173v1.pdf)put forward a local, model-agnostic generalization of mean accuracy decrease variable importance measures called Leave-One-Covariate-Out, or LOCO. LOCO was originally described in the context of regression models, but the general idea of LOCO is model agnostic and can be implemented in various ways. A general implementation of LOCO may proceed as follows. LOCO creates local interpretations for each row in a training or unlabeled score set by scoring the row of data once and then again for each input variable (e.g., covariate) in the row. In each additional scoring run, one input variable is set to missing, zero, its mean value, or another appropriate value for leaving it out of the prediction. The input variable with the largest absolute impact on the prediction for that row is taken to be the most important variable for that row's prediction. Variables can also be ranked by their impact on the prediction on a per-row basis. LOCO also creates global variable importance measures by estimating the mean change in accuracy for each variable over an entire data set and can even provide confidence intervals for these global estimates of variable importance.

最近有人提出了一种局部的、与模型无关的平均精度降低可变重要性度量的概括，称为leave-one-covariance-out，或LOCO。Loco最初是在回归模型的上下文中描述的，但是LOCO的一般思想是模型不可知的，并且可以以各种方式实现。本处办事处的一般实施可按以下方式进行。Loco通过对行中的每个输入变量(例如协变量)一次又一次地对数据行进行评分，为训练或未标记评分集中的每一行创建局部解释。在每次额外的评分运行中，一个输入变量的平均值被设置为缺失，零，或者另一个适当的值，以便将其置于预测之外。对该行的预测具有最大绝对影响的输入变量被认为是该行预测的最重要变量。变量也可以按照它们对每行预测的影响进行排序。Loco还通过估计整个数据集中每个变量精确度的平均变化来创建全局可变重要性度量，甚至可以为这些可变重要性的全局估计值提供置信区间。



 

 

 

What is the scope of interpretability for variable importance measures?

可解释的可变重要性措施的范围是什么？

 

 

Variable importance measures are usually global in scope; however, the LOCO approach oﬀers local variable importance measures for each row in a training set or in new data.

可变重要性度量通常是全局的;然而，LOCO方法为训练集或新数据中的每一行提供了局部可变重要性度量。

 

What complexity of functions can variable importance measures help explain?

可变重要性度量可以帮助解释哪些功能的复杂性？

 

 

Variable importance measures are most useful for nonlinear, non-monotonic response functions but can be applied to many types of machine-learned response functions.

变量重要性措施是最有用的非线性，非单调响应函数，但可以应用于许多类型的机器学习响应函数。

 

How do variable importance measures enhance understanding?

可变重要性度量如何增进理解？

 

 

Variable importance measures increase understanding because they tell us the most influential variables in a model and their relative rank.

可变重要性度量可以增加理解，因为它们告诉我们模型中最有影响力的变量及其相对排名。








https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               36/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 36 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

How do variable importance measures enhance trust?

可变重要性度量如何增强信任？

 

 

Variable importance measures increase trust if they are in line with human domain knowledge and reasonable expectations. They also increase trust if they remain stable when data is lightly and intentionally perturbed, and if they change in acceptable ways as data changes over time or when pertinent scenarios are simulated.

变量重要性度量增加信任，如果他们符合人类领域的知识和合理的期望。当数据受到轻微和有意的扰动时，当数据随时间变化或模拟了相关场景时，如果数据以可接受的方式变化时，它们保持稳定，那么它们也会增加信任。

 

 

Treeinterpreter

树语翻译



 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 20. A single decision tree with a highlighted decision path. Figure courtesy of [@crossentropy](https://twitter.com/crossentropy); [image](http://blog.datadive.net/interpreting-random-forests/) used with permission.

图20。带有突出显示的决策路径的单个决策树。经许可使用。

 

Several [average tree ](http://link.springer.com/article/10.1134/S0032946011030069)interpretation approaches have been proposed over the years, but the simple, open source package known as [treeinterpreter ](https://github.com/andosa/treeinterpreter)has become popular in recent months. Treeinterpreter decomposes decision tree and random forest predictions into bias (overall training data average) and component terms from each independent variable used in the model. Tree interpreter is model specific to algorithms based on decision trees. Figure 20 portrays the decomposition of the decision path into bias and individual contributions for a decision tree. The red

在过去的几年里，人们提出了几种解释方法，但是最近几个月，简单的、开放源码的软件包变得非常流行。Treeinterpreter将决策树和随机森林预测分解为偏差(总体训练数据平均值)和模型中使用的每个独立变量的组成项。树解释器是针对基于决策树算法的特定模型。图20描绘了决策路径分解为偏差和个人对决策树的贡献。红色








https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               37/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 37 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

branches in Figure 20 illustrate the decision path for a certain record of data through this single decision tree created from the results of treeinterpreter. (Treeinterpreter simply outputs a list of the bias and individual contributions for a variable in a given model or the contributions the input variables in a single record make to a single prediction.) The [eli5 ](http://eli5.readthedocs.io/en/latest/index.html)package also has an implementation of treeinterpreter.

图20中的分支说明了通过这个单一决策树根据树解释器的结果创建的某个数据记录的决策路径。(Treeinterpreter只是输出给定模型中某个变量的偏差和个体贡献的列表，或者输入变量在单个记录中对单个预测的贡献。)该包还有一个树解释器的实现。



 

What is the scope of interpretability for treeinterpreter?

树木解释器的可解释范围是什么？

 

 

Treeinterpreter is global in scope when it represents average contributions of independent variables to an overall decision tree or random forest model prediction. It is local in scope when used to explain certain predictions.

当它表示独立变量对总体决策树或随机森林模型预测的平均贡献时，Treeinterpreter是全局范围的。当用于解释某些预测时，它是局部范围的。

 

What complexity of functions can treeinterpreter help explain?

树解释器能够帮助解释哪些复杂的功能？

 

 

Treeinterpreter is meant to explain the usually nonlinear, non-monotonic response functions created by decision tree and random forest algorithms.

Treeinterpreter用于解释由决策树和随机森林算法创建的非线性、非单调响应函数。

 

How does treeinterpreter enhance understanding?

树语翻译者如何增进理解？

 

 

Treeinterpreter increases understanding by displaying average, ranked contributions of independent variables to the predictions of decision tree and random forest models.

Treeinterpreter通过显示独立变量对决策树和随机森林模型预测的平均、排序贡献来提高理解能力。

 

How does treeinterpreter enhance trust?

树语解释器如何增强信任？

 

 

Treeinterpreter enhances trust when displayed variable contributions conform to human domain knowledge or reasonable expectations. Treeinterpreter also enhances trust if displayed explanations remain stable when data is subtly and intentionally corrupted, and if explanations change in appropriate ways as data changes over time or when interesting scenarios are simulated.

当显示出符合人类领域知识或合理期望的可变贡献时，Treeinterpreter可以增强信任。如果显示的解释在数据被巧妙和故意破坏时保持稳定，如果解释在数据随时间变化或有趣的场景被模拟时以适当的方式变化，Treeinterpreter也能增强信任。

 

 

Conclusion

总结

 

 

Over the last few months, as my friends and colleagues heard that I was compiling this article, the pace at which they emailed, texted, tweeted, and slacked me regarding new work in this field has only accelerated. Currently, I probably see two new libraries, algorithms, or papers a day, and I can’t possibly keep up with adding them into this overview article. The fact is, this article will always be a finite document and has to stop somewhere. So, it stops here for now. I believe the article has put forward a useful ontology for understanding machine learning interpretability techniques moving into the future by categorizing them based on four criteria: their scope (local

在过去的几个月里，当我的朋友和同事听说我正在编写这篇文章时，他们发邮件、发短信、发推特、打击我在这个领域的新工作的速度只是加快了。目前，我每天可能会看到两个新的库、算法或论文，我不可能跟得上将它们添加到这篇概述文章中的步伐。事实上，这篇文章将永远是一个有限的文档，必须在某个地方停止。所以，现在到此为止。我相信这篇文章已经提出了一个有用的本体论来理解机器学习可解释性技术的未来，通过分类它们基于四个标准:它们的范围(本地)






https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               38/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 38 / 44




2019/1/18                                                            Ideas on interpreting machine learning - O'Reilly Media

2019/1/18口译机器学习的想法-O'ReillyMedia

 

versus global), the complexity of response function they can explain, their application domain (model specific versus model agnostic), and how they can enhance trust and understanding. I also believe the article has covered the main categories of techniques, especially from an applied and commercial usage perspective. If I had more time and could keep adding to the article indefinitely, I think two topics I would prioritize for readers today would be [RuleFit ](http://statweb.stanford.edu/~jhf/R_RuleFit.html)and the multiple advances in making deep learning [more interpretable, one example of many being learning deep k-Nearest Neighbor](https://arxiv.org/pdf/1702.08833.pdf) [representations. These are two things I’m certainly looking forward to learning ](https://arxiv.org/pdf/1702.08833.pdf)more about myself.

它们可以解释的响应函数的复杂性、它们的应用领域(特定于模型的对模型不可知的)，以及它们如何增强信任和理解。我也相信这篇文章已经涵盖了技术的主要类别，特别是从应用和商业应用的角度。如果我有更多的时间并且能够无限期地增加这篇文章，我想我今天会为读者优先考虑的两个主题是和使深度学习更加了解我自己的多重进步。



 

With so many new developments occurring so quickly, it’s truly an exciting time to be working on this subject. I hope you find the information in this article helpful. For those of us working on the subject, I hope you are as inspired as I am about the future of making machine learning and artificial intelligence more useful and transparent to users and consumers of these revolutionary technologies.

随着如此多的新发展如此迅速地出现，这确实是一个令人兴奋的时间在这个主题上工作。我希望你能从这篇文章中找到有用的信息。对于我们这些研究这个课题的人来说，我希望你们和我一样受到启发，认为未来将使机器学习和人工智能对这些革命性技术的用户和消费者更加有用和透明。

 

Related resources:

相关资源:

 

 

[Learning Path: Machine Learning](https://www.safaribooksonline.com/library/view/learning-path-machine/9781491958155/?utm_source=newsite&utm_medium=content&utm_campaign=lgen&utm_content=patrick-hall-post-interpreting-ml-related-resources-link)



 

**Practical Machine Learning with H2O**



 

**Hands-on Machine Learning with Scikit-Learn and TensorFlow**



 

**Thoughtful Machine Learning with Python**



 

 

**Machine learning: A quick and simple definition****.** **Get a basic overview of machine learning**

.了解机器学习的基本概况

**and then go deeper with recommended resources.**

然后利用推荐的资源深入调查。

 

 

 

Article image: Inputs activating diﬀerent neurons in a neural network. (source: Image courtesy of Patrick Hall and the h2o.ai team, used with permission).

文章图片:输入激活神经网络中不同的神经元。(来源:图片由PatrickHall和h2o.ai团队提供，经许可使用)。

 

Share        Tweet    ![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image076.png) **Share 1.2K**    **![img](file:////Users/stellazhao/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image077.png)** **Share**

分享Tweet










































https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning                                                                                               39/44

Https: / / www.oreilly. com / ideas / ideas-on-interpreting-machine-learning 39 / 44