 

[TOC]



# 0.摘要 

主要的序列转换模型(sequence transduction models)都是基于复杂的递归（recurrent）或卷积神经网络（convolutional neural networks ），包括一个编码器和一个解码器。性能最好的模型还通过注意机制（attention mechanism）连接编码器和解码器。我们提出了一个新的简单的网络结构--Transformer，完全基于注意力机制，完全不需要递归和卷积。在两个机器翻译任务上的实验表明，这些模型在质量上是优越的，同时更具有并行性，并且需要的训练时间明显减少。我们的模型在WMT2014年的英德翻译任务中达到了28.4BLEU，超过了现有的最佳效果，包括集成（ensembles）的效果，超过了2BLEU。在WMT2014英法翻译任务中，我们的模型在8个gpu上训练3.5天后，创造了单一模型的新的最高BLEU评分--41.0，训练开销远少于参考文献中最好模型的训练成本。



# 1.引言 

RNN，特别是LSTM和GRN，已经被广泛应用于序列建模和序列转换问题，如语言建模和机器翻译。对于探索循环语言模型和encoder-decoder 结构的边界也有大量的探索。


Jakob提议用自我注意力取代RNNs，并开始努力评估这个想法。Ashish和Illia，设计和实施了第一个Transformer模型，并参与了这项工作的每一个重要方面。Noam提出了scaled dot-product attention、multi-head attention和无参数位置表征，成为几乎所有细节的参与者。在我们最初的代码库和tensor2tensor中，Niki设计、实现、调优和评估了无数的模型变体（variants）。Llion还尝试了新的模型变体，负责我们最初的代码库，以及高效的推理和可视化。Lukasz和Aidan花了无数的时间设计和实现tensor2tensor的各个部分，取代了我们早期的代码库，极大地提高了我们的研究成果，并大大加快了我们的研究。


递归模型通常沿着输入和输出序列的符号位置进行因子计算。将位置对齐到计算时间中的步骤，它们生成一系列隐藏状态$$h_t$$，作为先前隐藏状态$$h_{t-1}$$和位置t输入的函数。这种内在的序列特性阻碍了训练示例中的并行化，因为内存约束限制了示例之间的批处理，这在较长的序列长度时变得至关重要。最近的工作通过因子分解技巧[18]和条件计算（ conditional computation ）[26]在计算效率方面取得了显著的改进，同时也提高了后者的模型性能。然而，序列计算的基本约束仍然存在。

注意力机制(Attention mechanisms)已经成为各种任务中,序列模型和转换模型的一个不可分割的部分，注意力机制允许对输入和输出序列中的依赖关系进行建模，而不考虑它们之间的距离。然而大部分情况下，注意力机制都与recurrent网络结合使用。

在这项工作中，我们提出了Transformer，一个避免循环（recurrence）的模型架构，而是完全依赖于注意力机制来刻画输入和输出之间的全局依赖性。Transformer允许非常多的并行化，只需要在8个 P100的GPU上训练12个小时，即可使得翻译质量达到最先进的状态（a new state of the art）。


# 2.背景

 

减少序列计算量的目标也构成了扩展神经GPU[20]、ByteNet[15]和ConvS2S[8]的基础，它们都使用卷积神经网络作为基本构件，并行计算所有输入和输出位置的隐含表示。在这些模型中，连接来自两个任意输入或输出位置的信号所需的操作数随着位置之间的距离增长，对于ConvS2S是线性增长，对于ByteNet则是对数增长。这使得学习不同位置之间的依赖关系变得更加困难。在Transformer中，这被减少到一个固定的操作数，尽管代价是由于注意力加权位置的平均而降低了有效分辨率，我们用第3.2节所述的Multi-Head Attention来抵消这种影响。


自我注意（Self-attention），有时称为内部注意（intra-attention），是一个注意机制，它将单一序列的不同位置关联起来，以计算序列的一个表示（representation）。自我注意已经成功地应用于各种各样的任务，包括阅读理解、抽象概括、文字蕴涵和学习任务无关的句子表征[4,22,23,19]。

端到端记忆网络是基于循环注意机制，而不是序列对齐递归，并已被证明在简单语言问答（simple-language question answering）和语言建模任务上表现良好[28]。


然而，据我们所知，Transformer是第一个完全依靠自我注意来计算其输入和输出表示的转换模型，没有使用序列对齐的RNNs或卷积。在接下来的章节中，我们将描述Transformer，引出自我注意力（ self-attention ），并讨论它相对于其他模型的优势（如[14,15]和[8]）。


# 3. 模型架构

最有竞争力的神经序列转导模型有一个编码器-解码器结构[5,2,29]。在这里，编码器将输入序列的符号表示$$(x_1,\dots,x_n)$$映射到连续表示的序列$$(z_1,\dots,z_n)$$。给定z后，解码器生成符号的输出序列$$(y_1,\dots,y_m)$$，每次生成一个元素。在每个步骤中，模型是自回归的[9]，在生成下一个符号时，会接受上一步生成的符号作为附加输入。

Transformer遵循这种总体架构，使用层叠式（stacked）自注意（self-attention）和point-wise的全连接层,用于编码器和解码器，如图1的左右两部分所示。

![image-20190128213849886](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190128213849886.png)

 

## 3.1编码器和解码器堆栈

- 编码器:编码器是由N=6个identical层堆栈组成。每一层有两个子层sublayer：
  - 第一种是一个multi-head自我注意机制，
  - 第二种是一个简单的全连接前馈网络(FFN）。我们分别在两个sublayer的旁边都使用一个残差连接[10]，然后进行层标准化[1]。也就是说，每个子层的输出是$$LayerNorm(x+Sublayer(x))$$。为了方便残差连接，模型中的所有sublayer层以及embedding层，输出的维数$$d_{model} = 512$$。

- Decoder:decoder由N=6个相同的layer组成。
  - 除了每个encoder层中的两个sublayer外，还插入了第三个sublayer（右边第二个），该sublayer对encoder的输出执行多头注意。与encoder类似，我们在每个sublayer周围使用残差连接，然后使用layer标准化。
  - 我们还修改了decoder stack中的自注意sublayer，以防止positions注意到subsequent位置。，基于常识--输出embedding存在一个位置的偏移的，所以这种屏蔽可以确保对位置$$i$$的预测只能依赖于位置小于$$i$$的已知输出。

 

## 3.2 注意力

注意力函数可以描述为将查询（query）和一组键值对（key-value）映射到输出，其中query, keys, values和 output 都是向量。output就是对values进行加权求和，而分配给每个value的权重由query与value对应的key的compatibility函数计算出来的。

### 3.2.1 Scaled Dot-Product Attention

输入包括:query, $$d_k$$, $$d_v$$

处理：

1. 计算query和所有keys的点乘，然后用$$\sqrt{d_k}$$做scale。
2. 对1的结果使用softmax，获取values上的权重。

![image-20190127161619981](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/006tNc79gy1fzmlvh2an2j317g0omq87.png)



Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.

 在实践中，我们并行地计算一组查询的注意力函数，并将它们打包成一个矩阵Q。键和值也被打包成矩阵K和V。我们计算输出矩阵如下:

$$\text { Attention } ( Q , K , V ) = \operatorname { softmax } \left( \frac { Q K ^ { T } } { \sqrt { d _ { k } } } \right) V$$

 

两个最常用的注意力函数是加注意力[2]和点积(乘)注意力。除了 scaling factor不一样，dot-produc注意函数和我们的算法是一致的。additive注意力使用FFN计算compatibility函数一个隐藏层。虽然两者在理论上的复杂性是相似的，但是点积注意力在实践中要快得多，空间效率也更高，因为它可以使用高度优化的矩阵乘法代码来实现。

 对于$$d_k$$较小时，这两种机制表现相似，较大时[3]，additive注意力表现优于点积注意力。我们猜测，对于大的$$d_k$$值，点积增长的幅度很大，把softmax推到梯度极小的区域。为了抵消这种影响，我们用$$\sqrt{d_k}$$做归一化。



### 3.2.2 多头注意力

我们发现，与使用$$d_{model}$$维度键、值和查询执行单个注意函数不同，使用不同的、学习过的dk、dk和dv维度分别线性投影查询、键和值h次是有益的。然后在每个预计版本的查询、键和值上并行执行注意函数，产生dv维输出值。这些值被连接起来并再次投影，最终生成最终的值，如图2所示。



多头注意允许模型联合注意来自不同位置的不同表示子空间的信息。只有一个注意力头，平均就会抑制这个。



在这项工作中，我们使用了8个平行的注意力层，或者叫做磁头。对于这些，我们使用dkdvdmodelh64。由于减少了每个头的维数，总的计算成本与单头注意力的全维数相似。

 

### 3.2.3 注意力在我们的模型中的应用

Transformer以三种不同的方式使用多头注意力:

- 在"编码器-解码器 注意"层中，查询来自前一个decoder层，记忆的key和value来自encoder的输出。这允许decoder中的每个位置参与输入序列中的所有位置。这模仿了序列到序列模型中典型的编码器-解码器注意机制，如[31,2,8]。

- encoder包含自我注意层。在自注意层中，所有的键、值和查询都来自同一个位置，在本例中是encoder中前一层的输出。encoder中的每个位置都可以被encoder前一层中的所有位置影响。

- 类似地，decoder中的自注意层，允许decoder中的每个位置关注decoder中直到的所有位置，直到并包括该位置。我们需要防止解码器中的左向信息流，以保持自回归性。我们通过屏蔽(设置为)所有的输入的softmax值，这对应的非法连接的缩放点产品的注意内实现这一点。见图2。

 

## 3.3  Position-wise FFN

除了注意子层之外，我们的编码器和解码器中的每一层都包含一个FFN，它被单独和完全一样地应用于每个位置。包括两个线性转换($$(w_1, b_1), (w_2, b2)$$)，中间有一个ReLU。

$$\mathrm { FFN } ( x ) = \max \left( 0 , x W _ { 1 } + b _ { 1 } \right) W _ { 2 } + b _ { 2 }​$$

 虽然线性变换在不同的位置是相同的，他们使用不同的参数从一层到一层。另一种描述这种情况的方法是用核大小为1的两个卷积。

输入和输出的维度是$$d_{model} = 512$$，中间层维度为$$d_{ff} = 2048$$。


## 3.4  嵌入和 Softmax

与其他序列转换模型类似，我们使用learned embedding将输入单词和输出单词转换为$$d_{model} = 512$$的向量。我们也使用线性变换和softmax将decoder的输出 变换为 下一个单词的概率。在该模型中，共享两个嵌入层之间的权重矩阵以及softmax前一步的线性变换。



## 3.5 位置编码

由于我们的模型不包含循环和卷积，为了使模型利用序列的顺序，我们必须注入一些关于单词在序列中的相对或绝对位置的信息。为此，我们将"位置编码"添加到编码器和解码器的底部。位置编码与embedding具有相同的维度$$d_{model}$$dmodel，因此可以将两者相加。有许多选择的位置编码，学习和固定[8]。

这里，我们使用不同频率的正弦和余弦函数:

$$P E _ { ( p o s , 2 i ) } = \sin \left( \operatorname { pos } / 10000 ^ { 2 i / d _ { \operatorname { model } } } \right) ​$$

$$ P E _ { ( p o s , 2 i + 1 ) } = \cos \left( p o s / 10000 ^ { 2 i / d _ { \text { model } }  }\right.)​$$



其中pos是位置，i是维度。也就是说，位置编码的每个维都对应一个正弦曲线。波长从$$2\pi$$到$$10000*2\pi$$形成一个等比数列。我们之所以选择这个函数，是因为我们假设它可以让模型很容易地通过相对位置来学习参与，因为对于任何固定的偏移量k，$$P E_{ p o s  + k}$$可以表示为$$P E _ { p o s }$$一个线性函数

我们还用学到的位置嵌入[8]进行了实验，发现两个版本产生的结果几乎相同(参见表3行(e))。我们之所以选择正弦波形式，是因为它允许模型推断到比训练期间所遇到的序列长度更长的序列长度。

 

# 4.为什么要self关注

在本节中，我们将自我注意层的各个方面与循环和卷积层进行比较，后者通常用于将一个可变长度的符号表示序列$$\left( x _ { 1 } , \dots , x _ { n } \right)$$映射到另一个等长序列$$\left(z _ { 1 } , \dots , z_ { n } \right)$$，以及$$x _ { i } , z _ { i } \in \mathbb { R } ^ { d }$$，例如一个典型的序列转换编码器或解码器中的隐藏层,这激发了我们使用自我注意力的三个期望。

 

- 一个是每层的总计算复杂度。
- 另一个是可以并行化的计算量，通过所需的最少顺序操作次数来衡量。

- 第三个是网络中长程依赖关系之间的路径长度。在许多序列转换任务中，学习长期的依赖是一个关键的挑战。影响学习这种依赖关系能力的一个关键因素是，网络中前向和后向信号遍历的路径的长度。输入和输出序列中任何位置组合之间的这些路径越短，就越容易学习长程依赖关系[11]。因此，我们还比较了不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。

如表1所示，自注意层用固定数量的顺序执行操作连接所有位置，而循环层需要o(n)顺序操作。就计算复杂度而言，当序列长度n小于表示维数d时，自注意层比循环层更快，最常见的情况是最先进的模型在机器翻译中使用的句子表示，如单词片段和字节对[25]表示。为了提高涉及很长序列的任务的计算性能，自注意可以限制为只考虑一个大小为r的邻域

![image-20190128212500273](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190128212500273.png)






输入序列以各自的输出位置为中心。这将把最大路径长度增加到o(nr)。我们计划在今后的工作中进一步研究这种方法。

一个核宽度$$k < n$$的单一的卷积层，不连接所有输入和输出位置。如果是相邻的内核，则需要一个o(n/k)卷积层堆栈;如果是扩展的卷积[15]，则需要o(logk(n))堆栈，增加网络中任意两个位置之间的最长路径的长度。卷积层一般比递归层昂贵，因为k的系数可分离卷积[6]，但是，大大降低了复杂性，以o(knd+nd2)。然而，即使使用kn，一个可分离卷积的复杂度也等于一个自注意层和一个逐点前馈层的组合，这就是我们在模型中采用的方法。

作为附带利益，自我注意可以产生更多可解释的模型。我们从模型中检查注意力分布，并在附录中提出和讨论例子。个人注意力不仅能清楚地学会执行不同的任务，许多还表现出与句子的句法和语义结构相关的行为。

 

# 5  训练

 

这一部分描述了我们的模型的训练框架

 

## 5.1训练数据及分批处理

我们对标准的WMT2014英德数据集进行了训练，该数据集包含约450万个句子对。句子使用字节对编码[3]进行编码，这种编码具有约37000个标记的共享源-目标词汇表。对于英语-法语，我们使用了更大的WMT2014英法数据集，其中包括36M个句子，并将标记分割成32000个单词词汇表[31]。句子对通过近似序列长度进行批处理。每批培训包含一组句子对，其中包含约25000个源单词和25000个目标单词。

 

## 5.2 硬件和调度

我们在一台有8个NVIDIAP100gpu的机器上训练我们的模型。对于我们使用整篇文章中描述的超参数的base模型，每个训练步骤大约需要0.4秒。我们训练基础模型总共100,000步或12小时。对于我们的大型模型(见表3的底线)，步骤时间为1.0秒。这些大模型被训练了30万步(3.5天)。

 

## 5.3 优化器

 

我们使用了Adam优化器,$$ \beta _ { 1 } = 0.9 , \beta _ { 2 } = 0.98$$,  $$ \epsilon = 10 ^ { - 9 }$$。在整个训练过程中，我们根据这个公式改变了学习速度:

 ![image-20190128213141002](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190128213141002.png)

 

这相当于在第一个warmup_steps训练步骤中线性地增加学习速率，然后按照steps的逆平方根成比例地减少学习速率。我们用了warmup_steps=4000。

 

## 5.4 正则化



我们在训练期间采用三种正则化方法:

- Residual Dropout：我们对每个子层的输出应用dropout，然后将其添加到子层输入并进行归一化处理。此外，我们对编码器和解码器堆栈中的嵌入和位置编码的和应用dropout。对于base模型，我们使用$$P _ { d r o p } = 0.1$$

  



![image-20190128213657460](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190128213657460.png)

表2:在2014年英语-德语和英语-法语新闻测试中，Transformer获得了比以前最先进的模型更好的BLEU分数，



 在训练期平滑标签，我们使用了价值ls0:1[30]的标签平滑。这伤害了困惑，因为模型学会了更多的不确定，但提高了准确性和BLEU评分。

 

# 6 结果

 

## 6.1机器翻译

在WMT2014英德翻译任务中，transformer模型(表2中的transformer模型)的性能比之前报道的最佳模型(包括集成模型)高出2:0以上，建立了一个新的最先进的BLEU得分28:4。这个模型的配置列在表3的底部。8P100gpu的训练时间为3:5天。即使我们的base模型超过所有以前发布的模型和集成，以一小部分的训练成本的任何竞争模型。

 在WMT2014英法翻译任务中，我们的大模型达到了BLEU分41:0，优于所有之前发布的单一模型，训练成本低于先前最先进模型的1/4。transformer(大)模型训练的英语对法语使用drop0:1，而不是0:3。

 

For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty = 0:6 [31]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [31].

对于基本模型，我们使用了一个单一的模型，这个模型是通过对最后5个检查点的平均值得到的，这些检查点每隔10分钟写一次。对于大型模型，我们取最后20个检查点的平均值。我们使用光束搜索，光束大小为4，长度惩罚为0:6[31]。这些超参数是在开发集上进行实验后选择的。在推理过程中，我们将最大输出长度设置为输入长度+50，但是如果可能的话，可以提前终止[31]。

Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5.

表2总结了我们的结果，并将我们的翻译质量和培训成本与文献中的其他模型架构进行了比较。通过对训练时间、GPU使用数量的乘积，以及对每个GPU5的单精度浮点容量的估计，来估计用于训练模型的浮点操作数量。

## 6.2 模型的变体

为了评估Transformer不同部件的重要性，我们以不同的方式改变了我们的基本模型，在开发集newstest2013上测试英语到德语翻译的性能。我们使用了前面部分所描述的束搜索（beam search），但没有使用检查点平均法。我们在表3中展示了这些结果。



![image-20190127153219192](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190127153219192.png)

表3:Transformer架构的变化，未列出的值与base模型相同，所有的指标都是在开发集newstest2013上测试的。根据我们的字节对编码，上面列出的perplexities是按字计算的，不应该与每个词的困惑（perplexities）相比较。

 

- (a)中可以看到head太大太少都不好：改变了注意力heads的数量$$h$$以及注意力key的维度$$d_k$$和value维度$$d_v$$，保持计算量不变，如第3.2.2节所述。虽然head=1比最佳设置差0.9BLEU，但是h太多也会导致翻译质量快速下降下。
- (b)中，我们观察到减少注意力键大小$$d_k$$,会损害模型质量。

- (c)和(d)，正如预期的那样，模型越大效果越好，并且dropout是非常有助于避免过度拟合。
- (E)中，我们将正弦位置编码替换为学到的位置嵌入(positional embeddings)，能看到结果与base模型几乎一样。

 

# 7   结论

 

在这项工作中，我们提出了Transformer，第一个完全基于注意力的序列转换模型，**用多头自注意代替了编解码器结构中最常用的递归层（ recurrent layers**）。

对于翻译任务，Transformer的训练速度比基于循环层或卷积层的架构快得多。无论是WMT2014英德翻译还是WMT2014英法翻译，Transformer都能达到了一个新的水平。在WMT2014英语-德语，我们的最佳模型甚至优于之前报道的集成模型。

我们对基于注意力模型的未来感到兴奋，并计划将其应用于其他任务。我们计划将Transformer扩展到包括文本以外的输入和输出模式的问题，并研究局部的、有限的注意力机制，以有效地处理大的输入和输出，如图像、音频和视频。减少产生的连续性是我们的另一个研究目标。






# 参考文献

1.  https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf

2. [https://github.com/](https://github.com/tensorflow/tensor2tensor) [tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor).

 

 
