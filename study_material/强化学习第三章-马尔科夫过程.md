[TOC]

# 有限马尔可夫决策过程——强化学习第三章



# 引言

继续《强化学习》一书的笔记总结，这一章聚焦的是**Finite** **Markov Decision Process有限马尔可夫决策过程**, 简称Finite MDP。与前一章multi-armed bandit问题相比，相同点是都要评估系统反馈，但MDP还需要在不同场景下选择不同的行动。在bandit问题中，我们要估计的是每一个行动a的value $$q_*(a)$$ ，在MDP中我们需要估计每一个行动a及状态s的value $$q_*(s,a)$$或者在optimal的行动选择下每一个状态s的value $$v_*(s)$$,这些依赖于状态的值对于衡量行动选择的**长期效应**非常重要。

另一方面，MDP也是强化学习问题一种数学上理想化的形式，方便我们有一些理论上的探讨，借此引入强化学习常用的概念与方法，涉及到value function以及Bellman equation。而且MDP对于许多问题都有一定的适用性，这一章概念及定义也较多，是比较重要的内容。

MDP是通过与环境的互动来学习一些经验从而达到某种设定的目标的问题。这里进行学习和决策的部分叫做**agen**t代理人。与agent互动的所有除agent之外的部分叫做**environment**环境。agent与environment是不断地交互的，agent选择了某些action，environment对这些action做出响应反馈一些reward信息，并将新的场景**situation**提供给agent，agent的目标就是通过合适的action的选择使得积累的reward总和最大化。他们之间的相互作用如下图所示

![image-20190131194501452](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131194501452.png)



再具体一点地说，agent和environment在一系列离散的时间点 $$t=0,1,2,3,...$$进行交互，在每一个时间点 $$t$$ ，agent得到environment的状态**state**的某种表示 $$S_t \in S$$，据此agent选择一个action $$A_t \in A(s)$$，这一步之后，受到这一action的影响，agent从environment得到一个**reward** $$R_{t+1} \in R$$，然后进入到下一个状态 $$S_{t+1}$$ ，这一系列的步骤如下所示：

$$S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,...$$

对于有限MDP，state, action及reward的集合 $$S,A,R$$都有有限个元素，且 $$R_t,S_t$$ 仅依赖于前一时刻的state $$S_{t-1}$$和action $$A_{t-1}$$，而不依赖于更早之前的state或action，即假设t时刻的state和reward分别用 $$s' \in S$$和 $$r \in R$$表示，则条件概率可以表示为

$$p(s',r|s,a) \doteq Pr\{S_t=s', R_t=r |S_{t-1} =s, A_{t-1}=a\}$$

这一假设要求state包含可能对将来造成影响的所有之前的agent与environment的互动信息，这一性质也被称为**Markov property**。

这里我们提到了reward，如何定义reward要依赖于具体问题。比如我们想让一个机器人尽快的从某个迷宫中逃出去，我们可以定义每经历一个时间点得到的reward是-1，直到最终逃出迷宫，为了最大化reward则agent需要学习尽快的从迷宫中逃出，再比如下棋，可以将胜利设为reward为1，失败设为reward为-1，而平局或者中间过程设为reward为零。如何设置合适的reward也是一门学问，而且我们设置的reward应该传达的是我们想要达到什么目的，而不是如何达到这些目的，例如下棋，我们设置的reward应该是赢得最终胜利，而不是要立即吃掉面前的棋子。

假设t时刻后的reward的序列表示为 $$R_{t+1},R_{t+2},R_{t+3},...$$，则**return**收益即累计的reward可如下定义 $$G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + ... + R_T $$，其中T是最后terminal state的时间点，这适用于某些有结局的场景，例如游戏结束或者逃出迷宫，这一类问题被称作**episodic task**, 每一个episode到达最终态后，我们可以将系统重置到初始状态再重头开始下一轮试验。当然，有些问题可能没有终止状态，比如自动驾驶我们希望车可以一直安全的行驶而不出现终止状态，我们称这一类问题为**continuing task**。通常对于这类问题我们会定义一个折扣率，而return也变为**discounted return**： $$G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma ^2R_{t+3} + ... =\sum_{k=0}^\infty \gamma ^k R_{t+k+1}$$ ，其中 $$0 \leq \gamma < 1]$$，叫做discount rate。折扣率 $$\gamma$$ 决定了将来的reward对于现在的value，即将来k时刻后reward与现在即时得到的reward要打一个大小为 $$\gamma ^{k-1}$$的折扣，这就有点”人生得意须尽欢，莫使金樽空对月“的感觉，假设 $$\gamma = 0$$，则return中仅有即时的reward一项，则agent可以说是myopic，即只追求眼前利益，对于 $$\gamma < 1$$ ，只要$${R_k}$$是有限的，则 $$G_t$$ 也是有限的，且随着 $$\gamma$$越来越接近1，将来的reward所占比重越来越大，可以说agent是farsighted，即有远见的。

这里discounted return也可以写成其递归形式：

$$G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma ^2R_{t+3} + ...  \\= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + ... ) \\= R_{t+1} + \gamma G_{t+1}$$

当然，我们也可以将episodic task和continuing task的return合并为同一形式：

$$G_t \doteq \sum_{k=t+1}^T \gamma ^{k-t-1} R_k$$

对于episodic task $$\gamma = 1$$，对于continuing task $$T=\infty$$且 $$\gamma < 1$$。

几乎所有的强化学习问题都需要定义value function，对于state value function来说就是衡量某个state的好坏，而action value function就是衡量对于某个state采取某种action的好坏。其中，好坏的定义就依赖于上面的return，更准确的说是对return的期望**expected return**。当然将来的reward依赖于agent所采取的行动，这种从state到采取每种行动的概率的映射就是**policy**。通常我们用 $$\pi$$来表示t时刻的policy， $$\pi(a|s)$$则代表了 $$S_t=s$$时，采取 $$A_t=a$$s的概率。

如此，我们可以定义在policy$$\pi$$的state-value function:

$$v_{\pi}(s) \doteq E_{\pi}[G_t | S_t=s] = E_{\pi}[\sum_{k=0}^\infty \gamma ^k R_{t+k+1}|S_t=s]$$

同样的，我们可以定义在policy $$\pi$$ 以及state $$s$$下采取action$$a$$的action-value function:

$$q_{\pi}(s) \doteq E_{\pi}[G_t | S_t=s,A_t=a] = E_{\pi}[\sum_{k=0}^\infty \gamma ^k R_{t+k+1}|S_t=s,A_t=a]$$

$$v_{\pi}$$ 可以通过不断的从真实的return中取样来得到，即我们对于每一个state保存一个平均值变量，随着该state越来越多的被访问到，该平均值越来越接近于该state的真实的value值，对于action-value同样的，不过这些平均值变量是对于每个state的每个action定义的，这种估值方法被称作**Monte Carlo**方法，之前在深度学习笔记中总结过Monte Carlo方法（[川陀学者：蒙特卡罗方法——深度学习第十七章](https://zhuanlan.zhihu.com/p/48481980)），之后在强化学习第五章还会再次讲到这一方法。当然对于state或者action的空间非常大的情况，对于每一种组合保存一个平均值变量是不实际的，这时候我们就需要用一些带有参数的函数来近似这些值了，这些近似方法在以后的章节里也会总结。

对于value function，我们也可以得到其递归形式：

![image-20190131194751855](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131194751855.png)

这一方程就是非常重要的**Bellman equation**了，它代表了当前state的value与下一时刻的state的value的关系，即对下一时刻每种可能达到的状态所得的reward与discounted value的期望。

我们也可以用**backup diagram**来表示这一关系

![企业微信截图_3ca36f75-1849-4262-9638-8a0b402c09eb](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/lala.png)

其中空心节点代表了state，实心点代表了state-action pair，从根节点s出发，它在policy ![\pi](https://www.zhihu.com/equation?tex=%5Cpi) 下以一定概率选择图中action ![a](https://www.zhihu.com/equation?tex=a) ，然后environment会做出反馈以概率p达到下一个state ![s'](https://www.zhihu.com/equation?tex=s%27) 以及对应的reward，再对每一种可能求和，就得到了Bellman equation。

当然，我们最终的目的是得到最优的policy，即积累的reward最大的policy，所以我们说如果对于所有state的expected return，policy ![\pi](https://www.zhihu.com/equation?tex=%5Cpi) 都要不亚于 ![\pi'](https://www.zhihu.com/equation?tex=%5Cpi%27) ，则 ![\pi](https://www.zhihu.com/equation?tex=%5Cpi) 不亚于 ![\pi'](https://www.zhihu.com/equation?tex=%5Cpi%27) ，即 ![\forall s \in S, v_\pi(s) \geq v_{\pi'}(s) \Rightarrow \pi \geq \pi'](https://www.zhihu.com/equation?tex=%5Cforall+s+%5Cin+S%2C+v_%5Cpi%28s%29+%5Cgeq+v_%7B%5Cpi%27%7D%28s%29+%5CRightarrow+%5Cpi+%5Cgeq+%5Cpi%27)

而至少存在这样的一个policy，它不亚于其他的所有policy，这就是**optimal policy**，对于optimal state-value function有： ![v_*(s) \doteq \underset{x}{\mathrm{max}}v_\pi(s) ](https://www.zhihu.com/equation?tex=v_%2A%28s%29+%5Cdoteq+%5Cunderset%7Bx%7D%7B%5Cmathrm%7Bmax%7D%7Dv_%5Cpi%28s%29+) ，同理对于optimal action-value function 有![q_*(s,a) \doteq \underset{x}{\mathrm{max}}q_\pi(s,a) ](https://www.zhihu.com/equation?tex=q_%2A%28s%2Ca%29+%5Cdoteq+%5Cunderset%7Bx%7D%7B%5Cmathrm%7Bmax%7D%7Dq_%5Cpi%28s%2Ca%29+)

根据这一定义，我们也可以导出**Bellman optimality equation**:

![image-20190131194848070](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131194848070.png)

它代表了当前state的optimal value与下一时刻的state的optimal value的关系。

同样的，对于optimal action-value function，其Bellman optimality equation为

![image-20190131194910457](/Users/stellazhao/statistics_studyplace/EasyML_BOOK/study_material_0227/_image/image-20190131194910457.png)

当我们得到了Bellman optimality equation后，我们就可以求解每个state的 ![v_*(s)](https://www.zhihu.com/equation?tex=v_%2A%28s%29) ，而optimal policy的选择也容易了，对于每个state都会有一个或多个action可以得到最优值，最优的policy只需要将非零的概率给这些action就可以了。我们可以说这是一种greedy的算法，但是由于 ![v_*](https://www.zhihu.com/equation?tex=v_%2A) 不仅考虑了即时的reward，还已经考虑了将来的可能的reward序列，所以这种只考虑即时的value function的greedy算法实际上会得到最优的长程的reward。

实际问题中，Bellman optimality equation可能不易求解，很多强化学习方法就是围绕如何近似求解Bellman optimality equation而进行的。



# 总结

总结一下，这一章围绕有限MDP问题，引入了深度学习的几个基本要素agent, environment, action, state, reward, value function以及policy。并且推导了十分重要的Bellman equation和Bellman optimality equation。下一章重点讨论dynamic programming，to be continued。

# 参考资料

1.Sutton和Barto合著强化学习[Reinforcement Learning: An Introduction](http://link.zhihu.com/?target=http%3A//incompleteideas.net/book/the-book-2nd.html)第三章，推荐阅读原文。