

[TOC]

# 对价格序列进行异常检测



> 异常检测：检测出数据中与其他数据不一致的数据点



异常检测( outlier detection or  anomaly detection)，是一个数据挖掘过程，用于确定数据集中的异常类型，并确定其发生的详细情况。 在当今世界，数据量巨大，无法手动标记异常值，因此自动的异常检测方法至关重要。 自动异常检测检测有广泛的应用，如欺诈检测，系统健康监测，故障检测，在传感器网络中的事件检测系统（这个是什么）等。

但是我想把异常检测应用到酒店价格上，处于私人原因。

你有过这样的经历吗，比如说，你经常去某个地方出差，而且总是住在同一家酒店。 虽然大多数时候，房价差不多，但偶尔同一家酒店、同一类型的房间，房价高得让人无法接受，而且你不得不换到另一家酒店，因为你的旅行津贴不能cover这个房价。 我已经经历过好几次了，这让我想到，如果我们可以创建一个模型来自动检测这种价格异常会怎样呢？

当然，有些异常情况一生只发生一次，而且我们事先已经知道，在未来几年可能不会在同一时间发生，比如2019年2月2日至2月4日亚特兰大荒谬的酒店价格。



![img](https://cdn-images-1.medium.com/max/1600/1*hGWm-K7FMcyXEA2j4i5weg.png)

 图1

在这篇文章中，我将探索不同的异常检测 技术， 我们的目标是用非监督学习的方法搜索酒店价格序列中的异常点。 我们开始吧！

### 数据

获取数据很难，我设法拿到了一些，但数据不是完美的。


我们将要使用的数据是 [Expedia 酒店个性化搜索数据集](https://www.kaggle.com/c/expedia-personalized-sort/data)的一个子集，可以在[这里]((https://www.kaggle.com/c/expedia-personalized-sort/data).)找到。

We are going to slice a subset of the training.csv set like so:

我们将切分training.csv中的一部分作为子集：

- Select one single hotel which has the most data point 选择一家数据点最多的酒店`property_id = 104517` .
-  选择`visitor_location_country_id = 219` , as we know from the another analysis that country id 219 is the Unites States. The reason we do that is to unify the 正如我们从另一份分析中知道的那样，country  id 219表示 Unites States。 我们这样做的原因是为了统一`price_usd` 列. Since different countries have different conventions regarding displaying taxes and fees and the value may be per night or for the whole stay. And we know that price displayed to US visitors is always per night and without taxes. 柱。 由于不同的国家有不同的公约，显示税收和费用和价值可能是每晚或整个逗留。 我们知道，向美国游客展示的价格总是每晚美元，而且不用交税
- 选择`search_room_count = 1`.
- 选择我们需要的特征:`date_time`, `price_usd`, `srch_booking_window`, `srch_saturday_night_bool`.

```
expedia = pd.read_csv('expedia_train.csv')
df = expedia.loc[expedia['prop_id'] == 104517]
df = df.loc[df['srch_room_count'] == 1]
df = df.loc[df['visitor_location_country_id'] == 219]
df = df[['date_time', 'price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]
```

在切片之后，下面我们将要使用的数据:

```
df.info()
```



![img](https://cdn-images-1.medium.com/max/1600/1*qDPjZZFs375IpJiYXQWBpA.png)

 图2

```
df['price_usd'].describe()
```



![img](https://cdn-images-1.medium.com/max/1600/1*WbcFNTxZ63e4vpF-52ZuUw.png)

注意，我们发现了一个极端异常值 -最高价格5584美元。

如果一个单独的数据实例相对于其余的数据是异常的，我们称之为“点异常”（point anomalies）(例如，具有大交易价值的购买)。  经过一点点调查，我猜想这要么是一个错误的数据，要么是用户不小心搜索了一个总统套房，没有预订或查看的意图。 为了发现更多不极端的异常，我决定移除这一个（异常检测中，需不需要移除极端异常值）。

```
expedia.loc[(expedia['price_usd'] == 5584) & (expedia['visitor_location_country_id'] == 219)]
```



![img](https://cdn-images-1.medium.com/max/2000/1*ABbgFa6gLhUvC0WM3DV6VQ.png)

Figure 3 图3

```
df = df.loc[df['price_usd'] < 5584]
```

提醒一点：我相信你已经发现我们遗漏了一些东西，也就是说，我们不知道用户搜索的房间类型，一个标准房间的价格可能跟一个海景房差异很大。 记住这一点，为了演示的目的，我们必须继续。

### 时间序列可视化

```
df.plot(x='date_time', y='price_usd', figsize=(12,6))
plt.xlabel('Date time')
plt.ylabel('Price in USD')
plt.title('Time Series of room price by date time of search');
```



![img](https://cdn-images-1.medium.com/max/1600/1*ESU3OuX2zT5L01iAlPEK5Q.png)

 图4

```
a = df.loc[df['srch_saturday_night_bool'] == 0, 'price_usd']
b = df.loc[df['srch_saturday_night_bool'] == 1, 'price_usd']
plt.figure(figsize=(10, 6))
plt.hist(a, bins = 50, alpha=0.5, label='Search Non-Sat Night')
plt.hist(b, bins = 50, alpha=0.5, label='Search Sat Night')
plt.legend(loc='upper right')
plt.xlabel('Price')
plt.ylabel('Count')
plt.show();
```



![img](https://cdn-images-1.medium.com/max/1600/1*kN38184_RxkgANP4uiov1w.png)

 图5

一般来说，

- 在非周六晚上搜索时，价格更稳定、更低。

- 星期六晚上的搜索时，价格就会上涨。

看来这个地方在周末很受欢迎。

### **Clustering-Based Anomaly Detection**

### 基于聚类的异常检测

#### **k-means algorithm**

#### K 均值算法

k-means is a widely used clustering algorithm. It creates ‘k’ similar clusters of data points. Data instances that fall outside of these groups could potentially be marked as anomalies. Before we start k-means clustering, we use elbow method to determine the optimal number of clusters.

K 均值算法是一种应用广泛的聚类算法。 它创建了 k 类相似的数据点集合。 不属于这些组的数据实例可能会被标记为异常。 在我们开始 K平均算法之前，我们使用肘部方法来确定最佳的类个数。



<iframe width="700" height="250" data-src="/media/7176354c825eb6a71e1fac12f6a6e006?postId=13586cd5ff46" data-media-id="7176354c825eb6a71e1fac12f6a6e006" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F24217243%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="https://towardsdatascience.com/media/7176354c825eb6a71e1fac12f6a6e006?postId=13586cd5ff46" style="display: block; position: absolute; margin: auto; max-width: 100%; box-sizing: border-box; transform: translateZ(0px); top: 0px; left: 0px; width: 700px; height: 284.984px;"></iframe>

elbow_curve.py



![img](https://cdn-images-1.medium.com/max/1600/1*sbYunUvghD_r721IR5E2RA.png)

Figure 6 图6

From the above elbow curve, we see that the graph levels off after 10 clusters, implying that addition of more clusters do not explain much more of the variance in our relevant variable; in this case `price_usd`.

从上面的肘形曲线，我们可以看到图形在10个聚类之后趋于平稳，这意味着更多的聚类并不能解释更多的相关变量的差异; 在这种情况下，价格为 usd。

we set `n_clusters=10`, and upon generating the k-means output use the data to plot the 3D clusters.

我们设置 n 个集群10，生成 k 均值输出后，使用数据绘制3D 集群。



<iframe width="700" height="250" data-src="/media/05c5134811786d11460ed9657b78c165?postId=13586cd5ff46" data-media-id="05c5134811786d11460ed9657b78c165" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F24217243%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="https://towardsdatascience.com/media/05c5134811786d11460ed9657b78c165?postId=13586cd5ff46" style="display: block; position: absolute; margin: auto; max-width: 100%; box-sizing: border-box; transform: translateZ(0px); top: 0px; left: 0px; width: 700px; height: 373px;"></iframe>

k-means_3D.py



![img](https://cdn-images-1.medium.com/max/1600/1*HoU7DGQx8UgHBJSXLuq1bQ.png)

Figure 7 图7

Now we need to find out the number of components (features) to keep.

现在我们需要找出要保留的组件(特性)的数量。(2)



<iframe width="700" height="250" data-src="/media/bec9e30929ed17818aa194a98f85f887?postId=13586cd5ff46" data-media-id="bec9e30929ed17818aa194a98f85f887" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F24217243%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="https://towardsdatascience.com/media/bec9e30929ed17818aa194a98f85f887?postId=13586cd5ff46" style="display: block; position: absolute; margin: auto; max-width: 100%; box-sizing: border-box; transform: translateZ(0px); top: 0px; left: 0px; width: 700px; height: 476px;"></iframe>

PCA.py



![img](https://cdn-images-1.medium.com/max/1600/1*_ncv1D_uD2wWmigdRvZbsA.png)

Figure 8 图8

We see that the first component explains almost 50% of the variance. The second component explains over 30%. However, we’ve got to notice that almost none of the components are really negligible. The first 2 components contain over 80% of the information. So, we will set `n_components=2`.

我们看到第一个分量几乎解释了50% 的方差。 第二个分量解释了超过30% 。 然而，我们必须注意到，几乎没有一个分量是可以忽略不计的。 前两个组件包含超过80% 的信息。 所以，我们将设置 n 个分量2。

The underline assumption in the clustering based anomaly detection is that if we cluster the data, normal data will belong to clusters while anomalies will not belong to any clusters or belong to small clusters. We use the following steps to find and visualize anomalies.

在基于聚类的异常检测数据库中，隐藏的假设是，如果我们对数据进行聚类，正常数据将被聚到不同的类中，而异常数据不属于任何类或者只是属于小型的类簇。 我们使用以下步骤来发现和可视化异常。

- Calculate the distance between each point and its nearest centroid. The biggest distances are considered as anomaly. 计算每个点和最近的类中心的距离。 最大的距离值被认为是异常的
- We use 我们使用`outliers_fraction` to provide information to the algorithm about the proportion of the outliers present in our data set. Situations may vary from data set to data set. However, as a starting figure, I estimate 为算法提供关于数据集中异常比例的信息。 不同的数据集的情况可能有所不同。 然而，作为一个起始数字，我估计`outliers_fraction=0.01`, since it is the percentage of observations that should fall over the absolute value 3 in the Z score distance from the mean in a standardized normal distribution. 因为它是在标准正态分布中，从均值到 z 得分距离中应落在绝对值3之上的观察值的百分比
- Calculate 计算`number_of_outliers` using 使用`outliers_fraction`.
- Set 预备`threshold` as the minimum distance of these outliers. 作为这些离群值的最小距离
- The anomaly result of 异常的结果`anomaly1` contains the above method Cluster (0:normal, 1:anomaly). 包含上述方法簇(0: normal，1: regular)
- Visualize anomalies with cluster view. 使用集群视图可视化异常
- Visualize anomalies with Time Series view. 使用时间序列视图可视化异常



<iframe width="700" height="250" data-src="/media/69bb6ec809f35df3f820e852ef809dc8?postId=13586cd5ff46" data-media-id="69bb6ec809f35df3f820e852ef809dc8" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F24217243%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="https://towardsdatascience.com/media/69bb6ec809f35df3f820e852ef809dc8?postId=13586cd5ff46" style="display: block; position: absolute; margin: auto; max-width: 100%; box-sizing: border-box; transform: translateZ(0px); top: 0px; left: 0px; width: 700px; height: 563.984px;"></iframe>

viz_cluster_view.py



![img](https://cdn-images-1.medium.com/max/1600/1*JG_xuw8E14iEkxLBuBF4fg.png)

Figure 9 图9



<iframe width="700" height="250" data-src="/media/ccdcc98f153045f0d5d80984d3ad371a?postId=13586cd5ff46" data-media-id="ccdcc98f153045f0d5d80984d3ad371a" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F24217243%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="https://towardsdatascience.com/media/ccdcc98f153045f0d5d80984d3ad371a?postId=13586cd5ff46" style="display: block; position: absolute; margin: auto; max-width: 100%; box-sizing: border-box; transform: translateZ(0px); top: 0px; left: 0px; width: 700px; height: 284.984px;"></iframe>

viz_time_series_view.py



![img](https://cdn-images-1.medium.com/max/1600/1*B85xLfKeg4n4NqFx4H1Cow.png)

Figure 10 图10

It seems that the anomalies detected by k-means clustering were either some of very high rates or some of very low rates.

看起来，K平均算法的异常检测要么是一些非常高的比率，要么是一些非常低的比率。



### 孤立森林的异常检测

[Isolation Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html) detects anomalies purely based on the fact that anomalies are data points that are few and different. The anomalies isolation is implemented without employing any distance or density measure. This method is fundamentally different from clustering based or distance based algorithms.

孤立森林检测异常完全基于这样一个事实，即异常是数据点很少和不同。 异常隔离是实现没有使用任何距离或密度测量。 这种方法与基于聚类或基于距离的算法有着根本的不同。

- When applying an 当应用一个[IsolationForest 隔离森林](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html) model, we set 模特，准备好了`contamination = outliers_fraction`, that is telling the model that the proportion of outliers in the data set is 0.01. 数据集中异常值的比例为0.01，这表明数据集中异常值的比例为0.01
- `fit` and 及`predict(data)` performs outlier detection on data, and returns 1 for normal, -1 for anomaly. 对数据执行异常检测，正常返回1，异常返回1
- Finally, we visualize anomalies with Time Series view. 最后，我们使用时间序列视图可视化异常



<iframe width="700" height="250" data-src="/media/c6c92c26a88924d5f0b778a25d5899c0?postId=13586cd5ff46" data-media-id="c6c92c26a88924d5f0b778a25d5899c0" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F24217243%3Fs%3D400%26v%3D4&amp;key=4fce0568f2ce49e8b54624ef71a8a5bd" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="https://towardsdatascience.com/media/c6c92c26a88924d5f0b778a25d5899c0?postId=13586cd5ff46" style="display: block; position: absolute; margin: auto; max-width: 100%; box-sizing: border-box; transform: translateZ(0px); top: 0px; left: 0px; width: 700px; height: 438.984px;"></iframe>

IsolationForest.py



![img](https://cdn-images-1.medium.com/max/1600/1*qddrIOLJSd2-iMpj7qbjiQ.png)

Figure 11 图11

### **Support Vector Machine-Based Anomaly Detection**

### 基于支持向量机的异常检测

A [SVM](https://en.wikipedia.org/wiki/Support-vector_machine) is typically associated with supervised learning, but [OneClassSVM](https://en.wikipedia.org/wiki/Support-vector_machine) can be used to identify anomalies as an unsupervised problems that learns a decision function for anomaly detection: classifying new data as similar or different to the training set.

支持向量机通常与监督式学习分类相关联，但是 OneClassSVM 可以用来识别异常作为一个无监督的问题，学习异常检测分类的决策函数: 将新数据分类为与训练集相似或不同的数据。

#### OneClassSVM

According to the paper: [Support Vector Method for Novelty Detection](http://users.cecs.anu.edu.au/~williams/papers/P126.pdf). SVMs are max-margin methods, i.e. they do not model a probability distribution. The idea of SVM for anomaly detection is to find a function that is positive for regions with high density of points, and negative for small densities.

根据论文: 支持向量方法的异常检测。 支持向量机是最大间隔方法，也就是说它们不对概率分布建模。 支持向量机用作异常检测的原理是，找到一个函数，这个函数对于点密度高的区域是正数，对于点密度小的区域是负数。

- When fitting 装配的时候[OneClassSVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM) model, we set 模特，准备好了`nu=outliers_fraction`, which is an upper bound on the fraction of training errors and a lower bound of the fraction of support vectors, and must be between 0 and 1. Basically this means the proportion of outliers we expect in our data. 本文提出了一种新的支持向量分数估计方法，它是训练误差分数的上界，支持向量分数的下界，并且必须在0和1之间。 基本上，这意味着在我们的数据中我们期望的离群值的比例
- Specifies the kernel type to be used in the algorithm: 指定要在算法中使用的核函数:`rbf`. This will enable SVM to use a non-linear function to project the hyperspace to higher dimension. . 这将使支持向量机能够使用非线性函数将超空间投影到更高的维数
- `gamma` is a parameter of the RBF kernel type and controls the influence of individual training samples - this effects the "smoothness" of the model. Through experimentation, I did not find any significant difference. 是 RBF 核函数的一个参数，控制个体训练样本的影响——这影响了模型的"平滑性"。 通过实验，我没有发现任何显著的差异
- `predict(data)` perform classification on data, and because our model is an one-class model, +1 or -1 is returned, and -1 is anomaly, 1 is normal. 对数据执行分类，因为我们的模型是单类模型，所以返回 + 1或-1,-1是异常，1是正常的



<iframe width="700" height="250" data-src="/media/7cc8e61cb8d578758377b9119528fd8a?postId=13586cd5ff46" data-media-id="7cc8e61cb8d578758377b9119528fd8a" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F24217243%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="https://towardsdatascience.com/media/7cc8e61cb8d578758377b9119528fd8a?postId=13586cd5ff46" style="display: block; position: absolute; margin: auto; max-width: 100%; box-sizing: border-box; transform: translateZ(0px); top: 0px; left: 0px; width: 700px; height: 373px;"></iframe>

OneClassSVM.py



![img](https://cdn-images-1.medium.com/max/1600/1*4CBpGg6xTabEf_K1yWbteQ.png)

Figure 12 图12

### Anomaly Detection using Gaussian Distribution

### 使用高斯分布检测异常

Gaussian distribution is also called normal distribution. We will be using the Gaussian distribution to develop an anomaly detection algorithm, that is, we’ll assume that our data are normally distributed. This’s an assumption that cannot hold true for all data sets, yet when it does, it proves an effective method for spotting outliers.

高斯分布也称为正态分布。 我们将使用高斯分布来开发一个异常检测算法，也就是说，我们将假设我们的数据是正态分布的。 这个假设并不适用于所有的数据集，但是当它适用时，它证明了一种发现异常值的有效方法。

Scikit-Learn’s `**covariance.EllipticEnvelope**` is a function that tries to figure out the key parameters of our data’s general distribution by assuming that our entire data is an expression of an underlying multivariate Gaussian distribution. The process like so:

sklearn的椭圆包络函数（EllipticEnvelope），试图通过假设我们的整个数据集合符合多元高斯分布的表达式，来计算数据分布的关键参数。 过程是这样的:

- Create two different data sets based on categories defined earlier, — search_Sat_night, Search_Non_Sat_night. 根据前面定义的类别创建两个不同的数据集---- search_Sat_night，Search_Non_Sat_night
- Apply 申请`EllipticEnvelope`(gaussian distribution) at each categories. (正态分布)每个类别
- We set 我们准备好`contamination` parameter which is the proportion of the outliers present in our data set. 参数是我们的数据集中存在的离群值的比例
- We use 我们使用`decision_function` to compute the decision function of the given observations. It is equal to the shifted Mahalanobis distances. The threshold for being an outlier is 0, which ensures a compatibility with other outlier detection algorithms. 计算给定观测值的决策函数。 它等于移动的马氏距离。 离群值的阈值是0，这确保了与其他异常检测算法的兼容性
- The 这个`predict(X_train)` predict the labels (1 normal, -1 anomaly) of X_train according to the fitted model. 根据拟合模型预测 x_train的标签(1代表正常值,-1代表异常值)



<iframe width="700" height="250" data-src="/media/75d2d69eb279bbd7fd02d2375502f42b?postId=13586cd5ff46" data-media-id="75d2d69eb279bbd7fd02d2375502f42b" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F24217243%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="https://towardsdatascience.com/media/75d2d69eb279bbd7fd02d2375502f42b?postId=13586cd5ff46" style="display: block; position: absolute; margin: auto; max-width: 100%; box-sizing: border-box; transform: translateZ(0px); top: 0px; left: 0px; width: 700px; height: 570.984px;"></iframe>

EllipticEnvelope.py



![img](https://cdn-images-1.medium.com/max/1600/1*YMF_eAI6ofVzwKc0Ncsz8g.png)

图13


有趣的是，用这种方法检测到的异常现象只是观察到了异常的高价，而没有观察到异常的低价。


到目前为止，我们已经用4种不同的方法对价格需求进行了异常检测进行了定价。 因为我们的异常检测是非监督式学习。 在建立模型之后，我们不知道它做得有多好，因为我们没有任何东西可以对它进行测试。 因此，在将这些方法置于关键场景之前，需要先测试效果。

[Jupyter notebook](https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Time%20Series%20of%20Price%20Anomaly%20Detection%20Expedia.ipynb) can be found on [Github](https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Time%20Series%20of%20Price%20Anomaly%20Detection%20Expedia.ipynb). Enjoy the rest of the week!

可以在 Github 上找到 Jupyter 笔记本。 好好享受这周剩下的时光吧！



# 参考文献:

[**Introduction to Anomaly Detection 异常检测入门**
*Experience with the specific topic: Novice Professional experience: No industry experience This overview is intended… 经验与具体主题: 新手专业经验: 没有行业经验这个概述的目的是..*www.datascience.com](https://www.datascience.com/blog/python-anomaly-detection)

[**sklearn.ensemble.IsolationForest - scikit-learn 0.20.2 documentation Sklearn.ensemble.isolationforest-scikit-learn 0.20.2文档**
*Behaviour of the decision_function which can be either 'old' or 'new'. Passing behaviour='new' makes the… 决策函数的行为可以是"旧的"或"新的"。 传球行为"新"使得..*scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)

[**sklearn.svm.OneClassSVM - scikit-learn 0.20.2 documentation**
*Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid'… 指定要在算法中使用的内核类型。 它必须是一个'线性','聚','径向','乙状结肠'..*scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html)

[**sklearn.covariance.EllipticEnvelope - scikit-learn 0.20.2 documentation 椭圆包络-scikit-learn 0.20.2文档**
*If True, the support of robust location and covariance estimates is computed, and a covariance estimate is recomputed… 如果为真，则计算鲁棒位置和协方差估计的支持度，并重新计算协方差估计*scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html)

[**Unsupervised Anomaly Detection | Kaggle 无人监督的异常检测 | Kaggle**
*Edit description 编辑描述*www.kaggle.com](https://www.kaggle.com/victorambonati/unsupervised-anomaly-detection)

