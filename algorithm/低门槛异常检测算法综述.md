[TOC]

标题《时间序列数据的常用异常检测算法》

# 背景

为了降低运维工程师使用modelflow建模的成本，modelflow会针对细分场景对算法进行优化：在原有算法的基础上进行封装，实现算法的超参数的抽象，使得算法变得更加易用。
监控告警是运维场景中最常用的场景之一，第一步就是做异常检测，下面针对时间序列的异常检测这个场景的常用算法进行抽象。

# 异常检测的定义

>"An outlier is an observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism." — D. M. Hawkins, Identification of Outliers, Chapman and Hall, 1980.

异常检测是数据挖掘中一个重要方面，被用来发现小的模式(相对于聚类)，即数据集中间显著不同于其它数据的对 象，异常检测应用在电信和信用卡欺骗、贷款审批、气象预报、金融领域和客户分类等领域中。

上面，Hawkins[^3]给出了异常的本质定义:异常是在数据集中表现得与众不同的数据，使人怀疑这些数据并非随机偏差，而 是产生于完全不同的机制。后来研究者们根据对异常存在的 不同假设，发展了很多异常检测算法，大体可以分为基于统 计的算法、基于深度的算法、基于距离的算法、基于密度的 算法，以及面向高维数据的算法等。

总结一下，异常检测本质是从历史数据学到正常模式，并将检测数据跟正常模式进行比较，进而判断得出是否异常

在数据库中包含着少数的数据对象，它们与数据的一般行为或特征不一致，这些数据对象叫做异常点 (Outlier)  ，也叫做孤立点。异常点的检测和分析是一种十分重要的数据挖掘类型，被称之为异常点挖掘 [28  ]  。 

对于异常数据的挖掘主要是使用偏差检测，在数学意义上，偏差是指分类中的反常实例、不满足规则的特例，或者观测结果与模型预测值不一致并随时间的变化的值等等。偏差检测的基本目标是寻找观测结果与参照值之间有意义的差别，主要的偏差技术有聚类、序列异常、最近邻居法、多维数据分析等。除了识别异常数据外，异常数据挖掘还致力于寻找异常数据间隐含模型，用于智能化的分析预测。对于异常数据分析方法的研究是论文的重要内容之一，通过研究异常数据，找到适合出口企业产品质量深入分析和有效监管的方法和策略。

# 异常检测算法的历史演进

根据《数据挖掘导论》[^7]   中对异常检测算法的分类：
- 统计方法Statistical Approaches
  - 一元正态分布（ Univariate Normal Distribution）
  - 多元正态分布（Outliers in a Multivariate Normal Distribution）
  - 混合模型（A Mixture Model Approach for Anomaly Detection）
- 基于邻近度（Proximity-Based Outlier Detection）
- 基于密度（Density-Based Outlier Detection）
- 基于聚类

Statistical Approaches　496
10.2.1　Detecting Outliers in a Univariate Normal Distribution　497
10.2.2　Outliers in a Multivariate Normal Distribution　499
10.2.3　A Mixture Model Approach for Anomaly Detection　500
10.2.4　Strengths and Weaknesses　502
10.3　Proximity-Based Outlier Detection　502
10.3.1　Strengths and Weaknesses　503
10.4　Density-Based Outlier Detection　504
10.4.1　Detection of Outliers Using Relative Density　505
10.4.2　Strengths and Weaknesses　506
10.5　Clustering-Based Techniques　506

## 统计方法

从20世纪80年代起，异常检测问题就在**统计学领域**里得到广泛研究，通常用户用某个统计分布对数据点进行建模， 再以假定的模型，根据点的分布来确定是否异常。许许多多针对不同分布的异常测试(Discordancy Test)方法发展起来，。这方面比较有代表性的有 1967  年 Mikey ， Dunn & Clark  提出的基于“均值漂移”模型的单点诊断量， 1970  年 Gentleman &Wilk  提出的群组诊断量， 1972  年 Tietjen &Moore  提出的单样本 k  个离群点的统计量 E k   ， 1985  年 Marasinghe  提出的改进的 E k   统计量 F k   ， 1989  年 Rosner  提出的单样本多个离群检测算法 ESD(Generalized Extreme Studentized Deviate)  方法， 1991  年 Paul & Fung  改进了 ESD  方法参数 k  选择的主观性，提出了回归分析的 GESR (Generalized Extreme Studentized DeviateResi2dual)  方法。近年来，多样本的离群检测方法也得到了一定的发展，总的思路是先尽量得到一个不含离群点的“干净集”，然后在此基础上对剩余的其他数据点进行逐步离群检测  [29  ] 

这些方法的最大缺陷是:在许多情况 下，用户并不知道这个数据分布。而且现实数据也往往不符 合任何一种理想状态的数学分布。 

目前利用统计学研究异常点数据有了一些新的方法，如通过分析统计数据的散度情况，即数据变异指标，来对数据的总体特征有更进一步的了解，对数据的分布情况有所了解，进而通过数据变异指标来发现数据中的异常点数据。常用的数据变异指标有极差、四分位数间距、均差、标准差、变异系数等等，变异指标的值大表示变异大、散度广；值小表示离差小，较密集。 

基于统计的方法检测出来的离群点很可能被不同的分布模型检测出来，可以说产生这些离群点的机制可能不唯一，解释离群点的意义时经常发生多义性，这是基于统计方法的一个缺陷。其次，基于统计的方法在很大程度上依赖于待挖掘的数据集是否满足某种概率分布模型，模型的参数、离群点的数目等对基于统计的方法都有非常重要的意义，而确定这些参数通常都比较困难。为克服这一问题，一些人提出对数据集进行分布拟合，但分布拟合存在两个问题：①给出的分布可能不适合任一标准分布。②即使存在一个标准分布，分布拟合的过程耗时太长。此外，基于统计的离群检测算法大多只适合于挖掘单变量的数值型数据，目前几乎没有多元的不一致检验，对于大多数的应用来说，例如图像和地理数据，数据集的维数却可能是高维的。实际生活中，以上缺陷都大大限制了基于统计的方法的应用，使得它主要局限于科研计算，算法的可移植性较差。 

参考数据挖掘导论和其他文章[^17], 总结出该分类下有如下的检测算法：

- 一元正态分布（ Univariate Normal Distribution）
- grubb test
- 多元正态分布（Outliers in a Multivariate Normal Distribution）
- ESD
- 混合模型（A Mixture Model Approach for Anomaly Detection）



## 基于邻近度或相似性(proximity/similarity-based)

用什么标准判定一个数据对象是孤立点呢？即便是对给定的距离量度函数，对孤立点也有不同的定义，以下是使用较多的几个：  

l基于距离的离群点最早是由 Knorr  和 Ng  提出的，他们把记录看作高维空间中的点，离群点被定义为数据集中与大多数点之间的距离都大于某个阈值的点，通常被描述为 DB ( *pct   ， d  min*   )   ，数据集 T  中一个记录 *O*  称为离群点，当且仅当数据集 T  中至少有 *pct*  部分的数据与 *O*  的距离大于 *d  min*   。换一种角度考虑，记 *M =N    × (1 - pct)*   ，离群检测即判断与点 *O*  距离小于 *d  min*   的点是否多于 M  。若是，  则 *O*  不是离群点，否则 *O*  是离群点 [4 ][36 ]  。

### 基于距离(distance-based)

基于距离的离群检测方法中，算法需要事先确定参数 *pct*  和 *d  min*   ，对于不同的数据集这往往是一件比较困难的事情，特别是 *d  min*   ，不同聚类密度的数据集 *d  min*   会有很大的差异，而这一般没有规律可循，因此，对于给定的不同 *d  min*   ，异常检测结果通常具有很大的不稳定性 [43  ]  。另一方面，基于距离的方法理论上能处理任意维任意类型的数据，当属性数据为区间标度等非数值属性时，记录之间的距离不能直接确定，通常需要把属性转换为数值型 [37 ][44 ]  ，再按定义计算记录之间的距离。当空间的维数大于三维时，由于空间的稀疏性，距离不再具有常规意义，因此很难为异常给出合理的解释。针对这个问题，一些人通过将高维空间映射转换到子空间的办法来解决数据稀疏的问题，此方法在聚类算法中用得比较多 [45  ][46 ]  ， Agarwal R. [45  ]   等人曾试着用这种投影变换的方法来挖掘离群。总的来说，基于距离的离群检测方法具有比较直观的意义，算法比较容易理解，因此在实际中应用得比较多。 

目前比较成熟的基于距离的异常点检测的算法有： 

1  ．基于索引的算法 (Index-based)  ：给定一个数据集合，基于索引的算法采用多维索引结构 R-  树， *k-d*  树等，来查找每个对象在半径 d  范围内的邻居。假设 M  为异常点数据的 *d*  领域内的最大对象数目。如果对象 *O*  的 M+l  个邻居被发现，则对象 *O*  就不是异常点。这个算法在最坏情况下的复杂度为 *O(k\*n  2 )*   ， *k*  为维数， *n*  为数据集合中对象的数目。当 *k*  增加时，基于索引的算法具有良好的扩展性 [44  ]  。 

2  ．嵌套循环算法 (Nested-loop)  ：嵌套一循环算法和基于索引的算法有相同的计算复杂度，但是它避免了索引结构的构建，试图最小化 I/O  的次数。它把内存的缓冲空间分为两半，把数据集合分为若干个逻辑块。通过精心选择逻辑块装入每个缓冲区域的顺序， I/O  效率能够改善 [44 ]  。 

3  ．基于单元的算法 (cell-based)[47  ]   ：在该方法中，数据空间被划为边长等于 *d* /(2**k*  1/2 )  的单元。每个单元有两个层围绕着它。第一层的厚度是一个单元，而第二层的厚度是 [2**k*  1/2 -1]  。该算法逐个单元地对异常点计数，而不是逐个对象地进行计数。对于一个给定的单元，它累计三个计数：单元中对象的数目 (cell_count)  、单元和第一层中对象的数目 (cell_+_1_layer_count)  单元和两个层次中的对象的数目 (cell_+_2_layers_count)  。该算法将对数据集的每一个元素进行异常点数据的检测改为对每一个单元进行异常点数据的检测，它提高了算法的效率。它的算法复杂度是 *O(c  k   +n )*   ，这里的 *c*  是依赖于单元数目的常数， *k*  是维数。它是这样进行异常检测的：若 cell_+_1_layer_count>M  ，单元中的所有对象都不是异常；若 cell_+_2_layers_count<=M  ，单元中的所有对象都是异常；否则，单元中的某一些数据可能是异常。为了检测这些异常点，需要逐个对象加入处理。基于距离的异常点检测方法要求用户设置参数 *P*  和 *d*  ，而寻找这些参数的合适设置可能涉及多次试探和错误。 

基于距离的方法与基于统计的方法相比，不需要用户拥有任何领域知识，与序列异常相比，在概念上更加直观。更重要的是，距离异常接近 Hawkins  的异常本质定义。然而，三种类型的基于距离的离群检测算法中，基于索引的算法和循环——嵌套算法需要 *O* (*k* **n  2* )  的时间开销，因此在大数据集中还有待于改进；而基于单元的算法，虽然与 *n*  具有线性的时间关系，但是它与 *k*  成指数关系，这限制了它在高维空间中的应用，此外，基于单元的算法还需要事先确定参数 *pct*  ， *d  min*   以及单元的大小，这使得算法的可行性比较差；高维空间中，基于索引的方法由于需要事先建立数据集的索引，建立与维护索引也要花大量的时间。因此三种方法对于高维空间中的大数据集，算法的效率都不高 [44  ]  。 

基于距离的方法与基于统计的方法相比，不需要用户拥有任何领域知识，与序列异常相比，在概念上更加直观。更重要的是，距离异常接近 Hawkins  的异常本质定义。然而，三种类型的基于距离的离群检测算法中，基于索引的算法和循环——嵌套算法需要 *O* (*k* **n  2* )  的时间开销，因此在大数据集中还有待于改进；而基于单元的算法，虽然与 *n*  具有线性的时间关系，但是它与 *k*  成指数关系，这限制了它在高维空间中的应用，此外，基于单元的算法还需要事先确定参数 *pct*  ， *d  min*   以及单元的大小，这使得算法的可行性比较差；高维空间中，基于索引的方法由于需要事先建立数据集的索引，建立与维护索引也要花大量的时间。因此三种方法对于高维空间中的大数据集，算法的效率都不高 [44  ]  。

### 基于密度(density-based)

基于密度的离群检测算法一般都建立在距离的基础上，某种意义上可以说基于密度的方法是基于距离的方法中的一种，但基于密度的异常观点比基于距离的异常观点更贴近 Hawkins  的异常定义，因此能够检测出基于距离的异常算法所不能识别的一类异常数据——局部异常。基于密度的方法主要思想是将记录之间的距离和某一给定范围内记录数这两个参数结合起来，从而得到“密度”的概念，然后根据密度判定记录是否为离群点。 

Breunig  等人提出的基于局部离群因子的异常检测算法 LOF  是基于密度方法的一个典型例子。它首先产生所有点的 MinPts  邻域及 MinPts  距离，并计算到其中每个点的距离；对低维数据，利用网格进行 *k   －* NN  查询，计算时间为 *O*   (*n* )   ；对中维或中高维数据，采用如 X2  树等索引结构，使得进行 k2NN  查询的时间为 *O* (*logn* )   ，整个计算时间为 *O* (*nlogn* )  ；对特高维数据，索引结构不再有效，时间复杂度提高到 *O* ( *n  2*   )  。然后计算每个点的局部异常因子，最后根据局部异常因子来挖掘离群。 LOF  算法中，离群点被定义为相对于全局的局部离群点，这与传统离群的定义不同，离群不再是一个二值属性 (  要么是离群点，要么是正常点 )   ，它摈弃了以前所有的异常定义中非此即彼的绝对异常观念，更加符合现实生活中的应用。 

LOF  算法中充分体现了“局部”的概念，每个点都给出了一个离群程度，离群程度最强的那几个点被标记为离群点。此外， Aggarwal  也提出了一个结合子空间投影变换的基于密度的高维离群检测算法。 



## 基于深度(depth-based)

基于深度的离群点检测算法的主要思想是先把每个记录标记为 k  维空间里的一个点，然后根据深度的定义 (  常用 Peeling Depth Contours  定义 )  给每个点赋予一个深度值；再根据深度值按层组织数据集，深度值较小的记录是离群点的可能性比深度值较大的记录大得多，因此算法只需要在深度值较小的层上进行离群检测，不需要在深度值大的记录层进行离群检测。基于深度的方法比较有代表性的有 Struyf  和 Rousseeuw  提出的 DEEPLOC  算法。虽然，理论上基于深度的识别算法可以处理高维数据，然而实际计算时， k  维数据的多层操作中，若数据集记录数为 N  ，则操作的时间复杂度为Ω (N   [k/2 ] )   。因此，当维数 k  ≤ 3  时处理大数据集时还有可能是高效的，而当 k  ≥ 4  时，算法的效率就非常低。也就是说，已有的基于深度的离群点检测算法无法挖掘高维数据，只有当 k  ≤ 3  时计算效率才是可接受的。 

##  基于偏差（Deviation-based)

基于偏差的离群检测算法 (Deviation-based Outlier Detection)  通过对测试数据集主要特征的检验来发现离群点。目前，基于偏移的检测算法大多都停留在理论研究上，实际应用比较少。以下三种是比较有代表性的 :   ① Arning  采用了系列化技术的方法来挖掘离群，由于算法对异常存在的假设太过理想化，因此并没有得到普遍的认同，对于现实复杂数据，其效果不太好，经常遗漏了不少的异常数据 ;   ② Sarawagi  应用 OLAP  数据立方体引进了发现驱动的基于偏移的异常检测算法 ;   ③ Jagadish  给出了一个高效的挖掘时间序列中异常的基于偏移的检测算法。虽然，基于偏移的离群检测算法理论上可以挖掘各种类型的数据，但是由于要事先知道数据的主要特征，而现实世界中的数据集一方面由于数据量比较大，另一方面由于属性比较多，因此这方面的特征往往不容易发现，当确定记录之间的相异度函数时，如果选择不合适，则得到的离群挖掘结果很可能不尽人意，所以本方法在实际问题中应用得比较少。 

基于偏移的异常点检测不采用统计检验或者基于距离的度量值来确定异常对象，它是模仿人类的思维方式，通过观察一个连续序列后，迅速地发现其中某些数据与其它数据明显的不同来确定异常点对象，即使不清楚数据的规则。基于偏移的异常点检测常用两种技术：序列异常技术和 OLAP  数据立方体技术。我们简单介绍序列异常的异常点检测技术。序列异常技术模仿了人类从一系列推测类似的对象中识别异常对象的方式。它利用隐含的数据冗余。给定 n  个对象的集合 S  ，它建立一个子集合的序列， {S 1   ， S 2   ， …  ， Sm}  ，这里 2<= m< =n   ，由此，求出子集间的偏离程度，即“相异度”。该算法从集合中选择一个子集合的序列来分析。对于每个子集合，它确定其与序列中前一个子集合的相异度差异。光滑因子最大的子集就是异常数据集。这里对几个相关概念进行解释： 

1  ．异常集：它是偏离或异常点的集合，被定义为某类对象的最小子集，这些对象的去除会产生剩余集合的相异度的最大减少。 

2  ．相异度函数：已知一个数据集，如果两个对象相似，相异函数返回值较小，反之，相异函数返回值较大；一个数据子集的计算依赖于前个子集的计算。 

3  ．基数函数：数据集、数据子集中数据对象的个数。 

4  ．光滑因子：从原始数据集中去除子集，相异度减小的两度，光滑因子最大的子集就是异常点数据集。 

基于偏差的异常点数据的检测方法的时间复杂度通常为 O(n )  ， n  为对象个数。基于偏差的异常点检测方法计算性能优异，但由于事先并不知道数据的特性，异常存在的假设太过理想化，因而相异函数的定义较为复杂，对现实复杂数据的效果不太理想。




Argrawal和Ragaran在1996年提出过"序列异常"的概念。他们采用这样一个机制:扫描数据集并观测到一系列相似数据，当发现一个数据点明显不同于前面的序列，这样的点就被认为是异常数据。这个算法复杂度与数据集大小呈线性关系，有优异的计算性能。但是序列异常在对异常存在的假设太理想化，对现实复杂数据效果不太好。

Knorr和Ng在1998年提出了基于距离的异常检测算法， Rastogi 和Ramaswam y改进了他们的异常定义。

1. 最简单的基于统计的方法：

   ​	ESD

   ​	Grubb test

   ​	3-sigma

2. 对于时间的序列，它表现出序列特有的规律，所以在1之前要经过特殊处理

   1. 时序的趋势性: 通过观察历史数据，发现如果时间序列表现出时间上具有一定的延续性，这时我们可以假设每一个时间点数据的分布都离邻近窗口相差比较近。具体的做法是，将滑动的窗口作为（sliding window）作为参考的集合（reference set），使用方法1比较待检测数据和reference set的差异。
   2. 光滑或者较光滑：通过观察历史数据，发现如果时间序列在历史上大部分时候都表现的比较光滑，那么通过检测突变点(change point)可以找到异常值。具体的方法是对原始数据构造差分序列，检测差分序列的zero-crossing点。
   3. 时间序列有周期性：通过观察历史数据，发现数据的模式周期性的重现，这时可以假定每个时间点数据跟前面几个周期同时刻的数据分布类似。具体的做法
      1. 构造季节差分序列：然后应用1的方法。








# 异常检测方法分类

根据对近期和历史的异常检测算法研究进展调研，结合时间序列数据的应用场景，总结得到如下的算法分类：

## 基于统计的方法

### 一元正态分布
一元正态分布/高斯分布检测器（ Univariate Normal Distribution）


###  grubb test

统计中的一种假设检验的方法, 备择假设是样本中有1个异常值

###  ESD
统计中的一种假设检验的方法, 允许样本中有多个异常值，是grubb test的泛化版

###  多元正态分布

多元正态分布（Outliers in a Multivariate Normal Distribution)/EllipticEnvelope:minimum covariance determinant estimator”,使用马氏距离当成异常得分。

###  混合模型

混合模型（A Mixture Model Approach for Anomaly Detection）

- GMM
总结：需要先假定原始数据服从某个分布(对于真实数据来说条件太苛刻)，可以输出异常得分。

## 控制图（contorl chart）
### 指数加权滑动平均EWMA
### 滑动平均MA
##  angle-based outlier factor (ABOF) 

## 基于模型

### 神经网络
#### replicator NN
#### deep auto_encoder
#### variational auto encoder

### 线性模型
#### pca
#### pca + nonlinear-kernel
#### one-class SVM

## 基于邻近度

基于对邻近度的算法检测异常的原理为，当数据点离其他样本距离较远时或相似度较低时，该数据点被定义为异常的。根据“相似性”的不同定义，检测方法又以分为三类： 基于距离，基于密度和基于聚类的。

### 基于距离（distcance-based）

邻近度用数据点的k邻近距离表示，k邻近距离较大的样本点是异常点。具体来说，对异常得分的计算有一下几种方法：k邻近距离(KNN)，average KNN 和median。KNN是最naive的计算方法，但是该方法受k取值的影响很大，即不同k值，可能得完全不一样的检测效果（如下图）。举个例子，当该样本点的第k个邻居刚好是极端值时，计算出来的距离非常大，导致被误检为异常值点。所以有了改进版average KNN/median KNN，考虑附近k个邻居的平均距离和距离的中位数。

![image-20181107210102912](/Users/stellazhao/tencent_workplace/gitlab/dataming/EasyML/doc/algorithm/_image/knn_k=1or5.png)

#### knn

      - average KNN
      - median KNN
### 基于密度(density-based)：

数据点在给定领域内，邻居点的个数被称为局部密度（local density）。局部密度越稀疏，说明该样本点越有可能是异常点。
####  lof

###  基于聚类 

一般异常检测中问题中，对异常点的定义是 不同于其它样本点的 “某个”数据点，这没有考虑到一种特殊情况：异常点会 以”小簇(small cluster)“的形式出现，基于聚类的异常检测方法就是专门考虑这类异常点的。个人认为，基于聚类的异常检测算法可以单独归为一类

#### DBSCAN

```
Density-Based Spatial Clustering of Applications with Noise
```

相对于统计方法，基于相似度的方法需要人工确定的 

三种方法的比较，聚类算法最快，但是检测的粒度最粗糙，对下图中局部的异常点通常无能为例，根本原因在于虽然下面的数据点A离 1类的中心点(centroid of cluster1)， 只能用density-based方法检测。
  ![image-20181107200825025](/Users/stellazhao/tencent_workplace/gitlab/dataming/EasyML/doc/algorithm/_image/聚类算法无能为力的局部异常点.png)

## 集成的方法
  - 有序
  - 相互独立
    	- 孤立森林

按照易用程度，异常检测方法，可以分为两类：
【没想好...】

1. 门槛低，易于调参，适用范围很少：这类算法针对单条曲线训练，算法的大部分参数都有明确的意义，调参过程简单有效【对标机器学习】, 并且可以可视化。
2. 高门槛，不好调参，覆盖大部分曲线：比如多曲线样本的异常检测，难以训练，可解释性差【对标深度学习】

# 总结和展望

异常检测是数据挖掘的一个重要方面，近年来受到越来越多的重视，研究人员先后提出了许多的算法，总的趋势是:
1. 不断拓展异常的定义，以期更加接近Hawkins的异常本质性定义，涵盖更多类型的异常
2. 算法运行上更加自动化 (尽量减少对用户的领域知识要求和人工干预程度)
3. 探索新的异常视点以适用于更高维度的数据集。 

我们认为，在未来若干年内这些仍将是异常检测研究的 主要方向。与异常检测方法的不断改进相比，对于异常数据的解释尤其是可视化描述相对滞后。对异常的解释有助于用户评估这些筛选出来的异常数据的合理性，并增加对异常数据的理解。



# 参考资料
1. [paper][LOF]M. M. Breunig, H. P. Kriegel, R. T. Ng, J. Sander. LOF: Identifying Density-based Local Outliers. SIGMOD, 2000.
2. [paper][EM][FastLOF]M. Goldstein. FastLOF: An Expectation-Maximization based Local Outlier detection algorithm. ICPR, 2012
3. [wiki][lof]https://en.wikipedia.org/wiki/Local_outlier_factor
4. [code][sklearn]http://scikit-learn.org/stable/modules/outlier_detection.html
5. [book]Aggarwal, C.C., 2015. Outlier analysis. In Data mining (pp. 237-263). Springer, Cham.
6. [data]http://odds.cs.stonybrook.edu/#table1, 异常检测数据集
7. [code][lstm]https://github.com/aurotripathy/lstm-anomaly-detect
8. [code][keras][rnn]https://github.com/Vict0rSch/deep_learning
9. [code][sklearn][MinCovDet]https://github.com/scikit-learn/scikit-learn/blob/f0ab589f/sklearn/covariance/outlier_detection.py#L22
10. [zhihu][pyod]https://www.zhihu.com/question/280696035
11. [code][pyod]https://github.com/yzhao062/Pyod.git
13 . https://en.wikipedia.org/wiki/Anomaly_detection
[^2]: https://datascience.stackexchange.com/questions/6547/open-source-anomaly-detection-in-python

[^3]: 4 Ruts I , Rousseeuw P.Computing Depth Contours of Bivariate Point
[^7 ]: 《数据挖掘导论》由[人民邮电出版社](https://baike.baidu.com/item/%E4%BA%BA%E6%B0%91%E9%82%AE%E7%94%B5%E5%87%BA%E7%89%88%E7%A4%BE/905716)出版，[美]作者Pang-Ning Tan，Michael Steinbach，Vipin Kumar 合著
[^17]: https://www.cnblogs.com/en-heng/p/9202654.html

[1] Hochenbaum, Jordan, Owen S. Vallis, and Arun Kejariwal. "Automatic Anomaly Detection in the Cloud Via Statistical Learning." arXiv preprint arXiv:1704.07706 (2017).