[TOC]

# 问题描述

<http://iops.ai/competition_detail/?competition_id=8&flag=1>


在运维场景中对监控指标的多个维度进行根因定位。具体来说，当某个总指标（如总流量）发生异常时，需要快速准确地定位到是哪个交叉维度的细粒度指标（如“省份=北京 & 运营商=联通”的流量）的异常导致的，以便尽快做进一步的修复止损操作。图示如下：

# 算法框架



调研hotspot，整体算法框架如下：

![image-20190322210515877](/Users/stellazhao/EasyML_BOOK/_image/image-20190322210515877.png)

- 对KPI使用N-sigma进行实时异常检测，如果异常进入2
- 开始定位：
  for  layer=1到layer= L:
	- for  cuboid in  layer=l 的cuboids:
  	1. 剪枝：cuboid l中如果的element的属性值，没有出现在l-1(上一层的Bset中)，那就把elements去掉，剪掉之后生成备选的elements给2用。
  	2. mcts：使用蒙特卡洛搜索树寻找最有可能是根因的维度组合$B_{set}$，这里的reward function = potential score（potential score表示该维度为故障根因的概率)：
  		for 迭代次数(即蒙特卡洛数的深度) 从1 到 100：
  			1.select;
  			....
  		返回ps最大的维度
  	3. 如果potential score超过指定PS或者迭代时长超过给定值或者所有维度都遍历过了，迭代终止，返回ps最大的维度值。




# 符号说明

![image-20190322211056826](/Users/stellazhao/EasyML_BOOK/_image/image-20190322211056826.png)



$e_i$: $e=(p, i, d, c) ​$，表示多个属性值的组合，是有具体的取值的.![image-20190322212044105](/Users/stellazhao/Library/Application Support/typora-user-images/image-20190322212044105.png)

$v(e_i)​$:某个维度上指标的实际值。

$f(e_i)​$:某个维度上指标的预测值。

cuboids:粒度可以任意细，任意粗  $ \left\{B_{P}, B_{P, I}, B_{P, I, D}, \ldots\right\}​$
layer1，layer2，。。。layer3的解释如下
![image-20190322220603157](/Users/stellazhao/EasyML_BOOK/_image/image-20190322220603157.png)



data cube: 多维的时序数据

ps: 衡量一组$S=\{e_i\}$是根因的概率。

e的后继结点：$\operatorname{Desc}(e)=\left\{e^{\prime} | e^{\prime} \text { is a descendant of } e\right\}$

e的后继LEAF结点：$\operatorname{Desc}^{\prime} (e)=\left\{e^{\prime} | e^{\prime}  \left(p^{\prime},i^{\prime}, d^{\prime}, c^{\prime}\right) \in L E A F, e^{\prime} \in \operatorname{Desc}(e) \right\}$

为什么要单独定义LEAF维度？因为它具有可加性，加起来等于它的父节点,如下

$\begin{array}{c}{v(e)=\sum\limits_{e^{\prime} \in D e s c^{\prime}(e)} v\left(e^{\prime}\right)} \\ {\text { e.g. }} \\ {v(\text { Bei jing }, *, *, *)=\sum\limits_{j, k, h} v\left(\text {Beijing}, i_{j}, d_{k}, c_{h}\right),} \\ {\text { Total } P V=v(*, *, *, *)=\sum\limits_{i, j, k, h} v\left(p_{i}, i_{j}, d_{k}, c_{h}\right)}\end{array}​$



# ripple effect

# potential score 

##  potential score的计算公式

- ps的计算公式如下

  $\text { Potential Score }=\max \left(1-\frac{d(\vec{v}, \vec{a})}{d(\vec{v}, \vec{f})}, 0\right)​$

  - a表示某维度确为根因的假设下，LEAF的预期值
  - v表示LEAF的真实值
  - f表示LEAF的预期值，跟疑似维度无关

  原理就是：比较在疑似故障这个假设下，LEAF相对真实值的区别，跟没有这个假设的前提下，LEAF相对真实值的区别。这两个区别越大，越说明这个假设是显著的。

  强行令他大于0，是因为想取到0~1 之间的值，当成概率。

- potential score指标计算公式基于的假设是：如果某个维度是故障的根因，那么在这个维度上的kpi的变化率 跟 它 的所有后代维度（descendant）的实际变化率应该是一致的。所以可以使用某个维度的kpi变化率减去后代的kpi变化率，基于这个difference的值算根因概率potential score，这个差值越大，根因概率越小，反之亦然。

- 这个绝对变化是 预测值（历史均值f）和实际值(v）的差(delta)。

## potential score计算步骤示例



假设现在最细粒度的LEAF维度有如下6个取值

$\vec{y}​$=[(Beijing;Mobile);(Shanghai;Mobile);

(Guangdong;Mobile); (Bei jing;Unicom);

 (Shanghai;Unicom); (Guangdong;Unicom)].

那么

 $\vec f = (20;15;10;10;25;20)$, 

$\vec v= (14;9;10;7;)​$

对于cuboid--$B_p​$,它包含3个属性值（elements）:

[(Beijing;\*);(Shanghai; \*);(Guangdong;*)]

所以$B_p​$的所有子集有7个，上面3个ele的排列组合：

$S_{p 1}=\{(B e i j i n g, *)\}​$,

$ S_{p 2}=\{(\text {Shanghai}, *)\}​$

....

$S_{p 7}=\{(\text {Beijing}, *), \quad \text { (Shanghai,*) },(\text {Guangdong,*}) \}$

![image-20190322212356628](/Users/stellazhao/EasyML_BOOK/_image/image-20190322212356628.png)

- 注：上图中箭头左边是预测值($\vec{f}$)，右边的是真实值($\vec v$)

  现在以$S_{p1}=\{(B e i j i n g, *)\}$,$为例，算它的根因概率（ps），步骤如下

（1）如果$S_{p1}$是故障根因，受其影响，最细维度（LEAF）的预测值为：

- (a)$S$不是LEAF

  - $y_i$ 不属于$D e s c^{\prime}(S)$（非leaf后继）

    $a\left(y_{i}\right)=f\left(y_{i}\right)$

  - $y_i​$ 属于$D e s c^{\prime}(S)​$(leaf后继)

    $a\left(y_{i}^{\prime}\right)=f\left(y_{i}^{\prime}\right)-h(x) \times \frac{f\left(y_{i}^{\prime}\right)}{f(x)},(f(x) \neq 0)$

- (b)$S​$是LEAF:

  - $y_i​$ 不属于S， $a(y_i)= f(y_i)​$

  - $y_i​$ 属于S，$ a(y_i)= v(y_i)​$

$\vec{a}\left(S_{p 1}\right)=(14,15,10,7,25,20)​$

   (2) 最细维度（LEAF）对应的真实值为:

$\vec{v}=\left[v\left(y_{1}\right), v\left(y_{2}\right), v\left(y_{3}\right), \ldots, v\left(y_{n}\right)\right]​$

  (3) 不管$S_{p1}​$是不是故障根因，最细维度（LEAF）上面的预测值为:

$\vec{f}=\left[f\left(y_{1}\right), f\left(y_{2}\right), f\left(y_{3}\right), \ldots, f\left(y_{n}\right)\right]​$



# 蒙特卡洛搜索树

## 符号说明

s： 状态，表示每个cubic中的elemments的集合,如$s\{e_1, e_2, ...\}$

$A(s)​$: 表示状态=s时的所有action集合，值就是其中1个element。表示在s选定之后，还可以往里面加入的elements，这个直接用该cuboid的所有terminal-node\s中的elements。

$N(s)​$: 表示s被访问的次数，s就是维度集合

$N(s;a)​$: 表示边(s;a) 被访问的次数

$Q(s, a)​$: 表示action function，取$s^{\prime}​$的ps和$s^{\prime}​$后继节点的ps的较大值。

$Q(s, a)=\max _{u \in\left\{s^{\prime}\right\} \cup \operatorname{descendent}\left(s^{\prime}\right)} p s(S(u))​$

蒙特卡洛搜索树是强化学习框架下的一种数值算法。



##执行步骤

1）选择（selection）：player根据当前的s和Q选取最优的action

$a=\underset{a \in A(s)}{\arg \max }\left\{Q(s, a)+C \sqrt{\frac{\ln N(s)}{N(s, a)}}\right\}​$

上式中C(*)表示exploration， 左边的Q表示。

Upper Confidence Bounds（UCB）

初始状态N(S, A)=0，怎么办？对这些未被访问的(s,a),赋予一定的访问概率R

$$R=1-Q\left(s, a_{\max }\right)$$, ---（没懂，为什么Q越大，概率越小）

$a_{max} = \arg\max \limits_{a \in A(s) \cap N(s, a)=0} Q(s, a)​$

selection起始于根节点，终于LEAF节点,这个值得是维度组合的叶子节点，如$\{e_1, \dots, e_{12}\}$

（错误：注意，终止于LEAF的维度值，而不是树的叶子节点）。

![image-20190325133227795](/Users/stellazhao/EasyML_BOOK/_image/image-20190325133227795.png)

2) expand：起于上一部选定的分支，终于其子节点。在里面加一个ele。

![image-20190325133205483](/Users/stellazhao/EasyML_BOOK/_image/image-20190325133205483.png)

3）evaluation：计算2)$s'$的ps，Q，和N

![image-20190325133147984](/Users/stellazhao/EasyML_BOOK/_image/image-20190325133147984.png)

4) 反向传播（Backpropagation）：更新

从树的根节点到$s'$的路径中，更新经过节点的Q和N。

注意：只有当子节点的Q > 父节点的Q时，我们采取更新父节点的Q

![image-20190325133126318](/Users/stellazhao/EasyML_BOOK/_image/image-20190325133126318.png)

MCTS 将在每个迭代过程（也就是1到4）中增加一个子节点。不过，要注意其实根据不同的应用这里也可以在每个迭代过程中增加超过一个子节点。

对每个cubic使用MTCS，迭代终止，如果以下条件之一满足：

1）$$B S e t=S$$ if $$ p s(S) \geqslant P T$$

2)   集合中所有可能的节点（维度值）都已经expanded完了

3）迭代时长超过给定值（根据经验人工确定）



# 层次剪枝

为了进一步减少搜索空间 ，HotSpot使用了层次剪枝的策略。

这里剪枝不是剪的蒙特卡洛搜索树，是剪的原始的action空间。

基本原理是：

由于MTCS是一层一层搜索的cuboids的，在搜索lay较小的cubics，例如（layer=1的$B_p$）

就可以剪掉一些ps较小的elements，因为这些elements的后继节点也不太可能是根因。

## 符号说明

$$B S e t_{l, B}​$$: 使用MCTS对layer=l的cubic B, 算出来的ps最大的Set。

hotspot剪枝的策略：对每层l中的elements不在$$B S e t_{l, B}$$, 剪掉。这个策略跟关联规则挖掘中的Apriori Principle很像，叫层次剪枝这个名字是因为利用到了layer的信息。



## 示例

![image-20190325135442670](/Users/stellazhao/EasyML_BOOK/_image/image-20190325135442670.png)



假设第一层的维度MCTS搜索出来的结果：
$$B S e t_{1, B_{P}}=\{(F u j i a n, *),(\text {Jiangsu}, *)\}$$并且$$ps(B S e t_{l, B_P})=0.5$$

$$B S e t_{1, B_{I}}=\{(*,Mobile),(*, \text {Unicom})\}$$并且$$ps(B S e t_{l, B_I})=0.32$$

现在MCTS要去搜索第二层了

就把 (Zhe jiang; Unicom) 和 (Zhe jiang;Unicom)这两个elements给剪掉了，因为他们的父节点 (Zhe jiang;)不在第一层维度的BSets里面，因此，第二层维度，我们只搜索剩下的4个elements，看哪种组合最优可能是根因的维度集合。

上面的剪枝将蒙特卡洛搜索树的action空间从63减少到了15 ($2^6- 1$ 到 $2^4 -  1$). 

然后使用蒙特卡洛搜索树搜到得分最高的set，即根因维度

$$R S e t=B \operatorname{Set}_{2, B_{P, I}}=\{(\text { Fujian, Mobile), (Jiangsu, Unicom) }\}$$且$$ps(B \operatorname{Set}_{2, B_{P, I}})=1​$$

# 数据分析

比赛提供了两份数据集
Anomalytime_data_test1.csv: 异常时刻

2019AIOps_data_test1/*.csv：多维度指标

- 异常时刻：

  有200个异常时刻，即16个小时的异常。

- 多维指标数据：

测试数据集从2018-09-15 00:00:00,到 2018-09-28 23:55:00，频率为5分钟一条数据，供14天
多维时序：有5个维度
dim1 147
dim2 13
dim3 9
dim4 35
dim5 5

LEAF的个数，即layer 5的elements个数：

-  理论上：按照所有维度值的遍历，理论上应该有212 5760条曲线

- 实际上：每天最细粒度有3 0011条曲线

layer1($C^1_5 = 5​$)

layer2($C^2_5 = 20​$)

layer3($C^3_5 = 20​$)

layer4(($C^4_5 =5​$个cubics) $B_{H,I,J,K}​$, ...

layer5 $B_{H,I,J,K,L}​$

----

没想明白 要训练集干嘛？为什么要注入异常？



# 技术方案

~~1.在训练集上使用ripple effect 注入异常，训练模型（用hotspot跑一下）。~~

（问题：维度里面的‘unknown’是代表缺失还是汇总‘*’,先默认是缺失）

1. 数据清洗，把维度值里面的‘unknown’清除。
2. 在测试集上，直接使用算法进行根因定位,
   1. 样本量太大了，先使用两天的数据跑一下，2018-09-15 00:00:00 到 2018-09-16 23:55:00
   2. 异常检测就使用3-sigma策略，

- ~~对KPI使用N-sigma进行实时异常检测，如果异常进入2~~（读取给定的label列）

- 开始定位：
  for  layer=1到layer= L:

  ​	for  cuboid in  layer=l 的cuboids:

  如果l>=2, 剪枝：cuboid l中如果的element的属性值，没有出现在l-1(上一层的Bset中)，那就把elements去掉，剪掉之后生成备选的elements给2用。

  否则，直接进入下面：

  1. mcts：使用蒙特卡洛树搜索寻找最有可能是根因的维度组合$B_{set}$，这里的reward function = potential score（potential score表示该维度为故障根因的概率)：
  2. $Q(s, a)=\max _{u \in\left\{s^{\prime}\right\} \cup \operatorname{descendent}\left(s^{\prime}\right)} p s(S(u))​$，初始值就是ps(S(s))
  3. $a=\underset{a \in A(s)}{\arg \max }\left\{Q(s, a)+C \sqrt{\frac{\ln N(s)}{N(s, a)}}\right\}​$
  4. $a_{max} = \arg\max \limits_{a \in A(s) \cap N(s, a)=0} Q(s, a)$
  5. for 迭代次数(即蒙特卡洛树的深度) 从1 到 100：
     	1.select;
        	     ...
     返回ps最大的维度
  6. 如果potential score超过指定PS或者迭代时长超过给定值或者所有维度都遍历过了，迭代终止，返回ps最大的维度值。



## 实验记录

![image-20190330182941694](/Users/stellazhao/EasyML_BOOK/image-20190330182941694.png)



## 一些细节问题



1.关于多维时序存储的问题，需要每一层的维度值对应的时序值都存一遍吗？

存最细粒度的，需要的时候再算？

答案：可以，异常时刻少，提前算好各种维度值对应的时序数据不现实（维度组合很大很大），并且大部分有可能白算了 （被剪枝掉了）。

2.selection里面具体的逻辑是怎样的？

![image-20190401212315297](/Users/stellazhao/EasyML_BOOK/_image/image-20190401212315297.png)

3. mcts多轮迭代是不是都是从根节点的初始状态开始的？

   不是的，是从根节点的最新状态开始的，通过保存node，而node会指向parent和children，实际上是保存下来了整棵树的，所以每次迭代，都是基于之前访问记录和Q_value的计算结果上进行的。

   

4. node在什么时候会更新visits和Q_value？unvisits nodes为什么会有Q_value？

   1. 单个element组成的set的Q_value是有初始值的
   2. 多个elemnts组成的set的Q_value在evaluation处才会被计算？（不确定）
   3. selection的第一层循环完成（找到一条直达叶子节点的路径），才更新这条路径上的node的visits。

5. 每次遍历的leaf node指的是什么

## 遍历layer1 到layer5的cuboids

### layer1
#### cuboids-$B_{DIM3}$
$B_{DIM3}​$有 9个elements，
$B_{DIM3}​$的集合有$2^9 - 1​$个：
$$S_{dim3,1}=\{(C1, *)\}​$$
$$S_{dim3,2}=\{(C2, *)\}​$$
$$S_{dim3,3}=\{(C3, *)\}​$$
$$...​$$
$$S_{dim3,8}=\{(C8, *)\}​$$
$$S_{dim3,9}=\{(C1, *), (C2, *)\}​$$
$$...​$$

而LEAF的elements个数为2125760个（实际远小于这个数）
LEAF的elements长这样
$$y_{1}=\{(i51, e01, c1,p01, l1)\}​$$
$$y_{2}=\{(i49, e01, c1,p01, l1)\}​$$
$$...​$$

先计算单个elemets组成的集合, 也就是($S_{dim3,1}$~ $S_{dim3,8}$)的ps得分：

根据hotspot给的计算公式

- (a)$S​$不是LEAF

  - $y_i​$ 不属于$D e s c^{\prime}(S)​$（非leaf后继）

    $a\left(y_{i}\right)=f\left(y_{i}\right)$

  - $y_i$ 属于$D e s c^{\prime}(S)$(leaf后继)

    $a\left(y_{i}^{\prime}\right)=f\left(y_{i}^{\prime}\right)-h(x) \times \frac{f\left(y_{i}^{\prime}\right)}{f(x)},(f(x) \neq 0)​$

- (b)$S$是LEAF:

  - $y_i$ 不属于S， $a(y_i)= f(y_i)$
  - $y_i​$ 属于S，$ a(y_i)= v(y_i)​$

可以算出来a

接下来算ps-score

$\text { Potential Score }=\max \left(1-\frac{d(\vec{v}, \vec{a})}{d(\vec{v}, \vec{f})}, 0\right)$

每一个


# 参考资料

[1] Yongqian Sun, Youjian Zhao, Ya su, et al., “HotSpot:Anomaly Localization for Additive KPIs withMulti-Dimensional Attributes”, IEEE Access, 2018.<https://netman.aiops.org/wp-content/uploads/2018/12/sunyq_IEEEAccess2018_HotSpot.pdf>

[2]https://mp.weixin.qq.com/s/Kj309bzifIv4j80nZbGVZw













 






















