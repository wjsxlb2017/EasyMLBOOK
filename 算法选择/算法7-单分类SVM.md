
# 算法原理-文档
单分类SVM的原理是将所有的样本与零点在特征空间F中分离开，并且分的越开越好。
具体来说
训练过程（training）：构建特征空间的概率密度函数并估计参数，模型假设所有训练样本都是位于概率密度的中心处，除了原点。
推断过程（serving）：
对于新来的样本点，使用训练得到的概率密度函数计算它的概率，低于一定阈值返回1，即异常点，否则返回正常。


这个方法创建了一个参数为$$(\omega,\rho)$$的超平面，该超平面与特征空间$$F$$中的零点距离最大，并且将零点与所有的数据点分隔开, 可以表述为优化问题:

$$\min\limits_{\omega,\zeta_{i},\rho}\frac{1}{2}||w||^{2}+\frac{1}{\nu n}\sum\limits_{i=1}^{n}{\zeta_{i}}-\rho $$,
$$s.t. \omega^{T}\phi(x_{i})>\rho-\zeta_{i} , i=1,...,n$$,
$$ \zeta_{i}>0 ,i=1,...,n $$

符号说明:
- 超参:
  - $$\nu$$ 类似二分类SVM中的C, 它是间隔误差（margin error）的上界, 
  是训练集中做为支持向量的样例占比的下界。
  e.g., 如果$$\nu = 0.05$$, 至多有5%的样本被错误分类（在当前训练出来的决策超平面），
  至少有5%的训练实例被当成了（当前训练出来的决策超平面）支撑向量。
  
- 待估参数
  - $$\phi$$表示在一个映射函数，将特征从原始的空间映射到新的特征空间$$F$$.
  - $$\zeta_{i}$$ 是松弛变量。
  - $$\rho$$ 表示超平面距离原点的距离。
  - $$\omega$$ 表示超平面的法向量。

因为这个参数的重要性，这种方法也被称为 $$\nu-SVM$$ 。采用Lagrange技术并且采用dot-product calculation，预测某个样本是否为异常的函数变为：

$$f(x)=sgn(\omega ^{T}\phi(x_{i})-\rho) =sgn(\sum\limits_{i=1}^{n}{\alpha_{i}K(x,x_{i})}-\rho) $$



## 算法原理-参数
- nu：对于OCSVM, 关键的参数为$$\nu$$, $$\nu$$ 类似二分类SVM中的C, 它是间隔误差（margin error）的上界, 
是训练集中做为支持向量的样例占比的下界。$$\nu = A + B/C$$
e.g., 如果$$\nu = 0.05$$, 至多有5%的样本被错误分类（在当前训练出来的决策超平面），
至少有5%的训练实例被当成了（当前训练出来的决策超平面）支撑向量。
由于待估计的超平面一侧是原点 + 少数越界的点，另一侧是大部分正常的点。
  - 当$$\nu$$取值越小，对松弛变量的惩罚就越大，这就会使得超平面把更多的训练样本划分到了正常的一测.
  - 当$$\nu$$取值越大，对松弛变量的惩罚就越小, 这就会使得超平面把更多的训练样本划分到了异常的一测.
- 
  默认的判别函数
  $$f(x)=sgn(\omega ^{T}\phi(x_{i})-\rho) =sgn(\sum\limits_{i=1}^{n}{\alpha_{i}K(x,x_{i})}-\rho) $$
  模型的使用者可以通过修改判断的阈值$$\alpha$$（类似N sigma算法的N），来调成模型的敏感度，具体操作为:
  $$f(x)=sgn(\omega ^{T}\phi(x_{i})-\rho) =sgn(\sum\limits_{i=1}^{n}{\alpha_{i}K(x,x_{i})}- \alpha * \rho) $$

